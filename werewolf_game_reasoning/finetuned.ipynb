{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:10:43.805598Z",
     "iopub.status.busy": "2025-05-03T12:10:43.805303Z",
     "iopub.status.idle": "2025-05-03T12:10:47.374046Z",
     "shell.execute_reply": "2025-05-03T12:10:47.373611Z",
     "shell.execute_reply.started": "2025-05-03T12:10:43.805581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 20:10:47,362 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# #æ¨¡å‹ä¸‹è½½\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # æ‹†åˆ†æˆ 3 ä»½\n",
    "\n",
    "# # ä¿å­˜æ‹†åˆ†åçš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5595\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š7482\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # ä½¿ç”¨å®˜æ–¹PyPIæº\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:05:47.107787Z",
     "iopub.status.busy": "2025-05-04T07:05:47.107505Z",
     "iopub.status.idle": "2025-05-04T07:05:51.821400Z",
     "shell.execute_reply": "2025-05-04T07:05:51.820872Z",
     "shell.execute_reply.started": "2025-05-04T07:05:47.107764Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 15:05:51,029] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # æ­£ç¡®çš„å°å†™å¯¼å…¥\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" #å‚æ•°é‡å’Œ gpt2 ä¸€æ ·.\n",
    "basic_model_path = \"./lora_basic\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "sft_model_path = \"./lora_sft\"        # SFTå¾®è°ƒç»“æœ\n",
    "# merged_model_path = \"./lora_basic_sft\"  # åˆå¹¶æ¨¡å‹\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # ç¦ç”¨æˆªæ–­\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # å¢å¤§ LoRA çŸ©é˜µç§© -------------------> next time to 8 #é™ä½ç§©å€¼\n",
    "    lora_alpha=16,  # è°ƒæ•´ alpha å€¼ ------------------> next time to 32 #ä¿æŒalpha/r=4çš„æ¯”ä¾‹\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # æ‰©å±•ç›®æ ‡æ¨¡å—\n",
    "    target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, å¢å¼ºç‰¹å¾æå–èƒ½åŠ›\n",
    "    lora_dropout=0.1,  # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ ---------------> next time to 0.2 #å¢åŠ Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # ç¡®ä¿å¤„äºè®­ç»ƒæ¨¡å¼[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # å…è®¸åç»­å¾®è°ƒæ—¶æ›´æ–°å¤´éƒ¨[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:16.790096Z",
     "iopub.status.busy": "2025-05-03T14:20:16.789706Z",
     "iopub.status.idle": "2025-05-03T14:20:17.690541Z",
     "shell.execute_reply": "2025-05-03T14:20:17.690103Z",
     "shell.execute_reply.started": "2025-05-03T14:20:16.790075Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==================== åŸºç¡€çŸ¥è¯†å¾®è°ƒéƒ¨åˆ† ====================\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ç¡®ä¿åªæœ‰promptå’Œresponseåˆ—\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# åŸºç¡€çŸ¥è¯†æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # åŠ è½½å‰10è¡Œæ•°æ®\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # åŠ è½½è¿·ä½ æ•°æ®é›†\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # åŠ è½½æ•°æ®é›†\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬ [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:17.691412Z",
     "iopub.status.busy": "2025-05-03T14:20:17.691220Z",
     "iopub.status.idle": "2025-05-03T14:20:17.700613Z",
     "shell.execute_reply": "2025-05-03T14:20:17.700107Z",
     "shell.execute_reply.started": "2025-05-03T14:20:17.691399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>ç‹¼ç¾äººåœ¨ç‹¼äººæ€æ¸¸æˆä¸­åº”è¯¥å¦‚ä½•å‘æŒ¥å…¶ç‰¹æ®ŠæŠ€èƒ½â€œé­…æƒ‘æ®‰æƒ…â€ä»¥è·å¾—èƒœåˆ©ï¼Ÿ</Prompt>\\n<Response>åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œç‹¼ç¾äººæ˜¯ä¸€ä¸ªå…·æœ‰ç‰¹æ®ŠæŠ€èƒ½çš„è§’è‰²ã€‚å¥¹çš„æŠ€èƒ½æ˜¯â€œé­…æƒ‘æ®‰æƒ…â€ï¼Œå¯ä»¥ä¸å¦ä¸€åç©å®¶ç»‘å®šï¼Œè‹¥ç‹¼ç¾äººåœ¨æ¸¸æˆä¸­å‡ºå±€ï¼Œè¯¥ç©å®¶ä¹Ÿä¼šéšä¹‹å‡ºå±€ã€‚ä¸ºäº†å¸®åŠ©ç‹¼äººæ€æ–°æ‰‹ç©å®¶æ›´å¥½åœ°äº†è§£ç‹¼ç¾äººçš„ç©æ³•ï¼Œæœ¬æ–‡å°†è¯¦ç»†ä»‹ç»ç‹¼ç¾äººçš„ç©æ³•æ”»ç•¥ã€‚\\n1ã€äº†è§£ç‹¼ç¾äººçš„æŠ€èƒ½å’Œèƒœåˆ©æ¡ä»¶\\nç‹¼ç¾äººçš„æŠ€èƒ½æ˜¯â€œé­…æƒ‘æ®‰æƒ…â€ï¼Œå¯ä»¥å°†è¯¥æŠ€èƒ½ä½¿ç”¨åœ¨ç¥èŒç©å®¶èº«ä¸Šï¼Œä»¥å‘æŒ¥å…¶æœ€å¤§æ•ˆæœã€‚å¦åˆ™ï¼Œå¦‚æœåªæ˜¯æ€æ­»ä¸€ä¸ªå¹³æ°‘ï¼Œå¯¹äºç‹¼ç¾äººæ¥è¯´å°±æ²¡æœ‰å­˜åœ¨çš„å¿…è¦äº†ã€‚ç‹¼ç¾äººçš„èƒœåˆ©æ¡ä»¶ä¸ç‹¼äººä¸€è‡´ï¼Œéœ€è¦å°†æ‰€æœ‰ç¥èŒç©å®¶æ·˜æ±°å‡ºå±€ã€‚\\n2ã€ç»‘å®šå¼ºç¥\\nç‹¼ç¾äººæƒ³è¦å‘æŒ¥å‡ºæœ€å¤§çš„æ•ˆæœï¼Œå°±éœ€è¦ç»‘å®šå¼ºç¥ã€‚åœ¨æ¸¸æˆåˆæœŸï¼Œå¼ºç¥ä¸­çš„é¢„è¨€å®¶ä¼šèµ·è·³ï¼Œå› æ­¤ç‹¼é˜Ÿå¯ä»¥é€‰æ‹©æŠ—æ¨æˆ–è€…å¤œæ™šåˆ€æ‰é¢„è¨€å®¶ã€‚è€Œå¥³å·«ã€å®ˆå«å’ŒçŒäººæ˜¯å¼ºç¥ä¸­çš„é‡è¦è§’è‰²ï¼Œç‹¼ç¾äººåº”è¯¥ä¼˜å…ˆå°†ä»–ä»¬ä½œä¸ºé­…æƒ‘å¯¹è±¡ã€‚ä¸ºäº†æˆåŠŸç»‘å®šå¼ºç¥ï¼Œç‹¼ç¾äººéœ€è¦å…·å¤‡ä¸€å®šçš„æŠ¿ç¥èƒ½åŠ›ã€‚\\n3ã€ä¼˜å…ˆé€‰æ‹©é­…æƒ‘å¯¹è±¡\\nç‹¼ç¾äººéœ€è¦å­¦ä¼šéšè—è‡ªå·±çš„èº«ä»½ï¼Œå¹¶å°†è‡ªå·±çš„é­…åŠ›å¯¹è±¡é€‰æ‹©ä¸ºå¼ºå¤§çš„ç¥èŒè§’è‰²ï¼Œå¦‚å¥³å·«å’Œå®ˆå«ã€‚åœ¨æ¸¸æˆåˆæœŸï¼Œç‹¼ç¾äººä¸éœ€è¦æ‚è·³ç¥ç‰Œï¼Œåªéœ€æš—ä¸­è§‚å¯Ÿæ‰¾åˆ°é¢„è¨€å®¶ã€å¥³å·«å’Œå®ˆå«ä¸­çš„ä¸€ä¸ªï¼Œé­…åŠ›ä¼˜å…ˆçº§ï¼šå¥³å·«>å®ˆå«>é¢„è¨€å®¶ã€‚ä¸‹ä¸€æ­¥å°±æ˜¯æƒ³æ–¹è®¾æ³•è®©è‡ªå·±å‡ºå±€æŠŠè¿·äººçš„å¯¹è±¡å¸¦èµ°ï¼Œè¿™æ ·å¯ä»¥ç»™ç‹¼é˜Ÿå¢åŠ ä¸€ä¸ªè½®æ¬¡ã€‚\\n4ã€ä¸ç‹¼é˜Ÿå‹å…±äº«ä¿¡æ¯\\nåœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç‹¼ç¾äººéœ€è¦ä¸ç‹¼é˜Ÿå‹å…±äº«è¿·æƒ‘çš„ä¿¡æ¯ã€‚å¦‚æœç‹¼ç¾äººæ²¡æœ‰è¿ä¸­ç¥ç‰Œï¼Œç‹¼é˜Ÿå‹å¯ä»¥é€‰æ‹©è‡ªçˆ†ï¼Œåˆ›é€ æ–°çš„æœºä¼šè®©ç‹¼ç¾äººå†æ¬¡è¿·æƒ‘ï¼Œé—´æ¥ç»™ç‹¼é˜Ÿå¢åŠ æ–°çš„è½®æ¬¡ã€‚\\n5ã€å­¦ä¼šä¼ªè£…\\nåœ¨å‘è¨€æ—¶ï¼Œç‹¼ç¾äººéœ€è¦å°†è‡ªå·±æè¿°ä¸ºæ™®é€šç‹¼äººçš„æ ·å­ã€‚åŒæ—¶ï¼Œéœ€è¦å‘æŒ¥è‡ªå·±æ˜¯ç‹¼äººçš„è§†è§’ä¼˜åŠ¿ï¼ŒæŠŠè‡ªå·±æ‰“æˆç„¦ç‚¹ç‰Œï¼Œä½†èƒ½æ•…æ„èŠå¾—å¾ˆå·®ï¼Œè®©å¥½äººæŠŠä½ æ ‡ä¸ºæ™®é€šç‹¼äººç¥¨å‡ºå±€ï¼Œè¿™æ ·ä½ å°±èƒ½é¡ºåˆ©å¸¦èµ°å¼ºç¥ï¼\\næ€»ä¹‹ï¼Œåœ¨ç‹¼äººæ€æ¸¸æˆä¸­æ‰®æ¼”ç‹¼ç¾äººæ—¶ï¼Œä½ éœ€è¦æŒæ¡ä»¥ä¸Šæ”»ç•¥ï¼Œä»¥ä¾¿æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„ç‰¹æ®ŠæŠ€èƒ½ã€‚é€šè¿‡è§‚å¯Ÿå’ŒæŠ¿ç¥çš„èƒ½åŠ›ï¼Œé€‰æ‹©æ­£ç¡®çš„é­…æƒ‘å¯¹è±¡å’Œæ—¶æœºï¼Œä¸ç‹¼é˜Ÿå‹ä¿æŒæ²Ÿé€šå¹¶å­¦ä¼šä¼ªè£…è‡ªå·±ã€‚åªæœ‰è¿™æ ·ï¼Œæ‰èƒ½åœ¨æ¸¸æˆä¸­è·å¾—èƒœåˆ©ï¼</Response>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:19.753901Z",
     "iopub.status.busy": "2025-05-03T14:20:19.753629Z",
     "iopub.status.idle": "2025-05-03T14:20:22.775573Z",
     "shell.execute_reply": "2025-05-03T14:20:22.775026Z",
     "shell.execute_reply.started": "2025-05-03T14:20:19.753883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,404,416 || all params: 1,553,118,720 || trainable%: 0.6055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "# print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:55:40.202425Z",
     "iopub.status.busy": "2025-05-03T12:55:40.202120Z",
     "iopub.status.idle": "2025-05-03T12:55:40.205091Z",
     "shell.execute_reply": "2025-05-03T12:55:40.204511Z",
     "shell.execute_reply.started": "2025-05-03T12:55:40.202408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:22.776607Z",
     "iopub.status.busy": "2025-05-03T14:20:22.776296Z",
     "iopub.status.idle": "2025-05-03T14:53:42.231949Z",
     "shell.execute_reply": "2025-05-03T14:53:42.231421Z",
     "shell.execute_reply.started": "2025-05-03T14:20:22.776592Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676/676 [00:00<00:00, 1026.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 2062.30 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 33:10, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.983100</td>\n",
       "      <td>3.026573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.997700</td>\n",
       "      <td>2.866308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.927200</td>\n",
       "      <td>2.788158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.430700</td>\n",
       "      <td>2.732572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.526800</td>\n",
       "      <td>2.689067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.046700</td>\n",
       "      <td>2.664482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.816600</td>\n",
       "      <td>2.631630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.570300</td>\n",
       "      <td>2.604075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.321900</td>\n",
       "      <td>2.590086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.316100</td>\n",
       "      <td>2.572315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.695200</td>\n",
       "      <td>2.561522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.108000</td>\n",
       "      <td>2.547211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.373900</td>\n",
       "      <td>2.539508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.726000</td>\n",
       "      <td>2.528158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.810800</td>\n",
       "      <td>2.523940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.253500</td>\n",
       "      <td>2.518830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.384800</td>\n",
       "      <td>2.511735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.755600</td>\n",
       "      <td>2.514043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.733700</td>\n",
       "      <td>2.508328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.289600</td>\n",
       "      <td>2.508166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.522100</td>\n",
       "      <td>2.500364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>2.497128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.181400</td>\n",
       "      <td>2.496612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.952900</td>\n",
       "      <td>2.494183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.210500</td>\n",
       "      <td>2.492880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic/tokenizer_config.json',\n",
       " './lora_basic/special_tokens_map.json',\n",
       " './lora_basic/vocab.json',\n",
       " './lora_basic/merges.txt',\n",
       " './lora_basic/added_tokens.json',\n",
       " './lora_basic/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„epoch\n",
    "    per_device_train_batch_size=2,     # -> next time to 4\n",
    "    gradient_accumulation_steps=4,      # -> next time to 8   # ç­‰æ•ˆæ‰¹é‡=32\n",
    "    learning_rate=2e-4,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\")\n",
    "\n",
    "# æ–°å¢ä¿å­˜é€»è¾‘\n",
    "# basic_model_path = \"./lora_basic\"  # æŒ‡å®šåŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„\n",
    "model.save_pretrained(basic_model_path)  # ä¿å­˜LoRAé€‚é…å™¨å‚æ•°[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:05:58.782658Z",
     "iopub.status.busy": "2025-05-04T07:05:58.782350Z",
     "iopub.status.idle": "2025-05-04T07:06:01.808851Z",
     "shell.execute_reply": "2025-05-04T07:06:01.808412Z",
     "shell.execute_reply.started": "2025-05-04T07:05:58.782642Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,553,118,720 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# åœ¨SFTè®­ç»ƒä»£ç å‰é‡æ–°åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½åŸºç¡€å¾®è°ƒç»“æœï¼š\n",
    "# é‡æ–°åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆé‡è¦ï¼ï¼‰\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # å¼ºåˆ¶ä½¿ç”¨ FP16 è®¡ç®—\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # ä»åŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µLoRAå‚æ•°\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # éªŒè¯å‚æ•°åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:06:01.809804Z",
     "iopub.status.busy": "2025-05-04T07:06:01.809624Z",
     "iopub.status.idle": "2025-05-04T07:06:02.782030Z",
     "shell.execute_reply": "2025-05-04T07:06:02.781605Z",
     "shell.execute_reply.started": "2025-05-04T07:06:01.809790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== SFTå¾®è°ƒéƒ¨åˆ† ====================\n",
    "# SFTæ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# åŠ è½½SFTæ•°æ®é›†\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "sft_dataset = sft_dataset.select(range(5))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:06:02.782760Z",
     "iopub.status.busy": "2025-05-04T07:06:02.782581Z",
     "iopub.status.idle": "2025-05-04T07:06:02.787855Z",
     "shell.execute_reply": "2025-05-04T07:06:02.787424Z",
     "shell.execute_reply.started": "2025-05-04T07:06:02.782747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Instruction>\\nä½ ç°åœ¨æ­£åœ¨ç©ä¸€ç§å«åšâ€œç‹¼äººæ€â€çš„æ¸¸æˆã€‚\\nåœ¨è¿™æ¬¾æ¸¸æˆä¸­ï¼Œç©å®¶é€šå¸¸è¢«åˆ†ä¸ºä¸¤ä¸ªé˜µè¥ï¼šç‹¼äººå’Œæ‘æ°‘ã€‚\\nç‹¼äººæ€æ¸¸æˆä¸­ä¸åŒè§’è‰²çš„ç©å®¶æœ‰ä¸åŒçš„ç›®æ ‡ï¼š\\n- æ‘æ°‘çš„ç›®çš„æ˜¯è¯†åˆ«å‡ºç‹¼äººï¼Œå¹¶é€šè¿‡æŠ•ç¥¨ä½¿ä»–ä»¬å‡ºå±€ã€‚\\n- å¯¹äºç‹¼äººæ¥è¯´ï¼Œä»–ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯éšè—ä»–ä»¬çš„çœŸå®èº«ä»½ï¼Œåœ¨è®¨è®ºä¸­è¯¯å¯¼ä»–äººï¼Œä»¥å…è¢«æŠ•ç¥¨å‡ºå±€å¹¶å°½å¯èƒ½çš„çŒæ€æ‘æ°‘ã€‚\\nä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬è§„åˆ™ï¼š\\n- èº«ä»½ï¼šç©å®¶çš„èº«ä»½æ˜¯ç§˜å¯†åˆ†é…çš„ã€‚ç‹¼äººå½¼æ­¤çŸ¥é“å¯¹æ–¹çš„èº«ä»½ï¼Œè€Œæ‘æ°‘åªçŸ¥é“è‡ªå·±çš„èº«ä»½ã€‚\\n- æ˜¼å¤œæ›´æ›¿ï¼šæ¸¸æˆæœ‰äº¤æ›¿çš„ç™½å¤©å’Œé»‘å¤œé˜¶æ®µã€‚å¤œé‡Œï¼Œç‹¼äººç§˜å¯†é€‰æ‹©ä¸€åæ‘æ°‘çŒæ€ã€‚ç™½å¤©ï¼Œæ‰€æœ‰ç©å®¶è®¨è®ºå¹¶æŠ•ç¥¨å†³å®šä»–ä»¬è®¤ä¸ºæ˜¯ç‹¼äººçš„ç©å®¶ï¼Œç¥¨æ•°æœ€å¤šçš„ç©å®¶è¢«æ·˜æ±°ã€‚\\n- ç‰¹æ®Šè§’è‰²ï¼šæ¸¸æˆä¸­æœ‰å­˜åœ¨ä¸€äº›æœ‰ç‰¹æ®Šèƒ½åŠ›çš„è§’è‰²ï¼Œæ¯”å¦‚èƒ½å¾—çŸ¥ç©å®¶èº«ä»½çš„â€œé¢„è¨€å®¶â€ç­‰ã€‚\\n- è·èƒœæ¡ä»¶ï¼šå½“æ¸¸æˆä¸­æœ‰ä¸€ä¸ªç¾¤ä½“å®ç°å®ƒä»¬çš„è·èƒœæ¡ä»¶æ—¶æ¸¸æˆç»“æŸã€‚å¦‚æœæ‰€æœ‰ç‹¼äººè¢«æ·˜æ±°ï¼Œæ‘æ°‘å°±è·èƒœã€‚å¦‚æœç‹¼äººæ€æ­»äº†æ‰€æœ‰æ™®é€šæ‘æ°‘æˆ–æ‰€æœ‰ç‰¹æ®Šè§’è‰²ï¼Œç‹¼äººå°±è·èƒœã€‚\\n\\nåœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œæˆ‘ä»¬æœ‰ä»1åˆ°7å·å…±7åç©å®¶ â€”â€” 5åæ‘æ°‘å’Œ2åç‹¼äººã€‚æ‘æ°‘ä¸­æœ‰ç‰¹æ®Šè§’è‰²ï¼ŒåŒ…æ‹¬ï¼š\\n- 1ä½é¢„è¨€å®¶ï¼š\\n    - ç›®æ ‡ï¼šé¢„è¨€å®¶çš„ç›®çš„æ˜¯å¸®åŠ©æ‘æ°‘è¯†åˆ«ç‹¼äººã€‚\\n    - èƒ½åŠ›ï¼šåœ¨å¤œæ™šé˜¶æ®µï¼Œé¢„è¨€å®¶å¯ä»¥ç§˜å¯†é€‰æ‹©ä¸€åç©å®¶ï¼Œæ¯æ™šäº†è§£ä»–çš„çœŸå®èº«ä»½ï¼ˆæ˜¯å¦ä¸ºç‹¼äººï¼‰ã€‚\\n- 1ä½å®ˆå«ï¼š\\n    - ç›®æ ‡ï¼šå®ˆå«çš„ç›®çš„æ˜¯ç­–ç•¥æ€§åœ°ä½¿ç”¨ä»–çš„ç‰¹æ®Šèƒ½åŠ›æ¥å¸®åŠ©æ‘æ°‘ã€‚\\n    - èƒ½åŠ›ï¼šå®ˆå«æ¯æ™šå¯ä»¥ä¿æŠ¤ä¸€åç©å®¶ï¼Œé˜²æ­¢ä»–ä»¬å—åˆ°ç‹¼äººçš„æ”»å‡»ã€‚å®ˆå«å¯ä»¥é€‰æ‹©ä¿æŠ¤è‡ªå·±ï¼Œæˆ–è€…é€‰æ‹©ä¸ä¿æŠ¤ä»»ä½•äººï¼Œä½†ä»–ä¸èƒ½åœ¨è¿ç»­ä¸¤ä¸ªå¤œæ™šä¿æŠ¤åŒä¸€ä¸ªç©å®¶ã€‚\\nå…¶ä»–çš„éƒ½æ˜¯æ™®é€šæ‘æ°‘ã€‚\\n</Instruction>\\n<Prompt>åœ¨æœ¬åœºæ¸¸æˆä¸­ï¼Œä½ ç›®å‰å·²çŸ¥ä»¥ä¸‹ä¿¡æ¯ï¼š\\n1. è§’è‰²è®¾å®šï¼š\\nä½ æ˜¯3å·ç©å®¶ã€‚\\nä½ çš„èº«ä»½æ˜¯ï¼šé¢„è¨€å®¶ã€‚\\nä½ æ¯æ™šå¯ä»¥æŸ¥çœ‹ä¸€åç©å®¶æ˜¯å¦ä¸ºç‹¼äººï¼Œä½ çš„ç›®æ ‡æ˜¯åˆ©ç”¨è¿™äº›ä¿¡æ¯å¸®åŠ©å…¶ä»–äººæ‰¾å‡ºå¹¶æ·˜æ±°æ‰€æœ‰ç‹¼äººã€‚\\n2. å®¢è§‚ä¿¡æ¯ï¼š\\n- æ¸¸æˆè¿›ç¨‹ï¼šç›®å‰æ¸¸æˆè¿›è¡Œåˆ°ç¬¬2è½®ã€‚\\n- å½“å‰å­˜æ´»çš„ç©å®¶æœ‰ï¼š1å·ï¼Œ3å·ï¼Œ4å·ï¼Œ5å·ï¼Œ6å·ï¼Œåªèƒ½åœ¨ä»¥ä¸Šç©å®¶ä¸­é€‰æ‹©è¿›è¡ŒæŸ¥éªŒ\\n- è¡ŒåŠ¨è®°å½•ï¼šç¬¬1è½®é¢„è¨€å®¶æŸ¥éªŒ2å·ç©å®¶ï¼Œ2å·ç©å®¶æ˜¯ç‹¼äººã€‚\\n- æŠ•ç¥¨æƒ…å†µï¼šç¬¬1è½®æŠ•ç¥¨è®°å½•ï¼š1å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n2å·ç©å®¶æŠ•ç»™ï¼š3å·ç©å®¶ï¼›\\n3å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n4å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n5å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n6å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\nç»“æœï¼š2å·ç©å®¶è¢«æŠ•ç¥¨å‡ºå±€ã€‚\\n\\n3. ä¸»è§‚ä¿¡æ¯ï¼š\\n\\n- ç¬¬1è½®æ‰€æœ‰ç©å®¶å‘è¨€ï¼š\\n**5å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œæ²¡æœ‰ä¿¡æ¯ï¼Œå¬åç½®ä½é¢„è¨€å®¶æŠ¥æŸ¥éªŒã€‚\\n**6å·ç©å®¶**ï¼š6å·æ‘æ°‘ç‰Œã€‚æ˜¨æ™š7å·æ­»äº¡ã€‚1å·æœ‰å¯èƒ½æ˜¯ç‹¼äººã€‚å¬1å·å‘è¨€ã€‚\\n**1å·ç©å®¶**ï¼š5å·ç¬¬ä¸€ä¸ªå‘è¨€ï¼Œæˆ‘è®¤ä¸ºç®€çŸ­ä¸€ç‚¹ä¹Ÿåˆç†ï¼Œä½†æ˜¯æˆ‘è®¤ä¸º6å·æ˜¯ç‹¼äººï¼Œè¸©æˆ‘1å·æ˜¯ä¸ºäº†æ‰¾äººæŠ—æ¨ã€‚å¦‚æœåé¢é¢„è¨€å®¶æ²¡å½’ç¥¨ï¼Œæˆ‘ä¼šå»æŠ•6å·ã€‚\\n**2å·ç©å®¶**ï¼šæˆ‘æ˜¯ä¸€å¼ å¥½äººç‰Œï¼Œå‰ç½®ä½6å·è·Ÿ1å·é€‰æ‹©äº’ç›¸æ¶æ‰“ï¼Œæˆ‘è‚¯å®šæ˜¯äº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·ã€‚ä¸¤å¼ ç‰Œéƒ½æ˜¯ç›¸å½“äºåˆ’æ°´çš„çŠ¶æ€ã€‚å¬åé¢ç©å®¶çš„å‘è¨€å§ï¼\\n**3å·ç©å®¶**ï¼š2å·æŸ¥æ€ã€‚2å·æœ¬è½®çš„â€œäº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·â€è¿™æ®µå‘è¨€è·Ÿè®¤ç‹¼æ²¡æœ‰ä»»ä½•åŒºåˆ«ï¼Œæˆ‘ç›¸ä¿¡ä»Šå¤©2å·è‚¯å®šä¼šå‡ºå±€çš„ã€‚ä»Šå¤©å®ˆå«å®ˆæˆ‘ä¸€è½®ï¼Œç¡®ä¿æˆ‘æ˜å¤©çš„æŸ¥éªŒä¿¡æ¯èƒ½å¦‚å®æŠ¥å‡ºæ¥ã€‚å…¶æ¬¡ï¼Œä»2å·çš„å‘è¨€ä¸­å¯ä»¥æ¨ç†å‡º1å·å’Œ6å·é‡Œé¢åº”è¯¥æ²¡æœ‰2å·çš„ç‹¼åŒä¼´ã€‚å¦‚æœ2å·çš„ç‹¼åŒä¼´è¢«æ”»å‡»ï¼Œé‚£ä¹ˆ2å·ä¸å¤ªä¼šåªæ˜¯ä¸€ä¸ªçœ‹æˆçš„å§¿æ€ï¼Œè¯´è·Ÿæˆ‘æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¦‚æœ1å·å’Œ6å·é‡Œé¢æœ‰ç‹¼äººæˆ‘è®¤ä¸º2å·ç‹¼äººåº”è¯¥å»åšç‚¹äº‹æƒ…ä»è€Œä¸ºè‡ªå·±çš„ç‹¼é˜Ÿåšè´¡çŒ®ï¼Œä½†æ˜¯2å·å¹¶æ²¡æœ‰è¿™ä¹ˆåšï¼Œæ‰€ä»¥ä»2å·çš„è§†è§’ï¼Œæˆ‘å¯ä»¥åˆ¤æ–­å‡º1å·å’Œ6å·æ˜¯ä¸¤ä¸ªå¥½äººã€‚ä»Šå¤©å…¨ç¥¨æ‰“é£2å·ã€‚\\n**4å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œ3å·æ˜¯å•è¾¹é¢„è¨€å®¶ï¼Œä»Šå¤©æŠ•2å·æŸ¥æ€ã€‚å¦‚æœçœŸçš„åƒ3å·ç©å®¶æ‰€è¯´ï¼Œèƒ½ä»2å·çš„å‘è¨€ä¸­ï¼Œæ¨å‡º1å·å’Œ6å·ç©å®¶é‡Œé¢åº”è¯¥æ²¡æœ‰2å·ç©å®¶çš„ç‹¼åŒä¼´ã€‚æˆ‘è‡ªå·±æ¸…æ¥šæˆ‘çš„åº•ç‰Œèº«ä»½æ˜¯ä¸€å¼ æ°‘ç‰Œï¼Œé‚£ä¹ˆæœ€åä¸€å¼ ç‹¼åªèƒ½æ˜¯5å·ï¼Œä½†æ˜¯5å·åœ¨å‰ç½®ä½å‘è¨€è·³æ‘æ°‘åˆ’æ°´ï¼Œæ— ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å®šä¹‰ä»–æ˜¯å¦æ˜¯çœŸç‹¼äººã€‚å®ˆå«ä»Šå¤©å¯ä»¥å®ˆæŠ¤3å·å•è¾¹é¢„è¨€å®¶ç¡®ä¿å¹³å®‰å¤œã€‚ä»Šå¤©å¥½äººå…¨ç¥¨å‡º2å·ã€‚\\n\\nä½ ç›®å‰æ˜¯3å·é¢„è¨€å®¶ã€‚è¯·ç»“åˆä»¥ä¸Šè§’è‰²è®¾å®šã€å®¢è§‚ä¿¡æ¯å’Œä¸»è§‚ä¿¡æ¯ï¼ˆå®¢è§‚ä¿¡æ¯ä¸€å®šä¸ºçœŸï¼Œä¸»è§‚ä¿¡æ¯åŒ…å«æ¬ºéª—æ€§å†…å®¹ï¼‰ï¼Œæ ¹æ®æŠ•ç¥¨æƒ…å†µåˆ†ææ½œåœ¨çš„ç«™è¾¹å…³ç³»ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†æç©å®¶éšè—çš„çœŸå®èº«ä»½ä¿¡æ¯ï¼Œé€‰æ‹©ä½ è¦æŸ¥éªŒçš„ç©å®¶ï¼Œè¯·ç”¨å…³é”®å­—ä¸º\\'æŸ¥éªŒ\\'ã€â€˜åŸå› â€™çš„jsonæ ¼å¼è¾“å‡ºï¼Œç›´æ¥è¾“å‡ºç©å®¶ç¼–å·ã€‚\\n\\n</Prompt>\\n<Response>{\"æŸ¥éªŒ\": \"5\", \"åŸå› \": \"æˆ‘æ‰“ç®—æŸ¥éªŒ5å·ç©å®¶ï¼Œå› ä¸ºä»–ä¸€ç›´æ²¡æä¾›ä»»ä½•æœ‰ç”¨ä¿¡æ¯ï¼Œæˆ‘æ€€ç–‘ä»–æ˜¯åœ¨åˆ’æ°´é¿å…è¢«æ³¨æ„ã€‚\"}</Response>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # æŸ¥çœ‹æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# çˆ†æ˜¾å­˜, éœ€è¦å°†æ•°æ®é›† cut ä¸€åŠè¯•è¯•, ä¸»è¦ response ä¸èƒ½å¤Ÿè¢«æˆªæ–­, æ‰€ä»¥ä¸èƒ½ truncation. æœ€é•¿çš„è¾“å…¥åºåˆ—æ˜¯ 7482."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:06:04.508089Z",
     "iopub.status.busy": "2025-05-04T07:06:04.507815Z",
     "iopub.status.idle": "2025-05-04T07:06:13.286697Z",
     "shell.execute_reply": "2025-05-04T07:06:13.285877Z",
     "shell.execute_reply.started": "2025-05-04T07:06:04.508069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13668/2130614555.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 198.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 139.37 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250504_150608-6c031199\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_sft\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/tjotpx87zgzwpg9bwr2k2\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/tjotpx87zgzwpg9bwr2k2\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.49 GiB. GPU 0 has a total capacity of 22.20 GiB of which 2.95 GiB is free. Process 106081 has 19.25 GiB memory in use. Of the allocated memory 17.73 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     57\u001b[39m sft_trainer = Trainer(\n\u001b[32m     58\u001b[39m     model=model,\n\u001b[32m     59\u001b[39m     args=sft_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# callbacks=[swanlab_callback]\u001b[39;00m\n\u001b[32m     64\u001b[39m )\n\u001b[32m     65\u001b[39m torch.autograd.set_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43msft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# ä¿å­˜æœ€ç»ˆæ¨¡å‹\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# model.save_pretrained(\"./lora_final\")\u001b[39;00m\n\u001b[32m     70\u001b[39m tokenizer.save_pretrained(sft_model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/peft/peft_model.py:1756\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1755\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1757\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1767\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1769\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:843\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[32m    846\u001b[39m     loss=loss,\n\u001b[32m    847\u001b[39m     logits=logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    850\u001b[39m     attentions=outputs.attentions,\n\u001b[32m    851\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/loss/loss_utils.py:63\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     62\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/loss/loss_utils.py:35\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     28\u001b[39m     source: torch.Tensor,\n\u001b[32m     29\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     **kwargs,\n\u001b[32m     33\u001b[39m ) -> torch.Tensor:\n\u001b[32m     34\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     37\u001b[39m         loss = loss / num_items_in_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 8.49 GiB. GPU 0 has a total capacity of 22.20 GiB of which 2.95 GiB is free. Process 106081 has 19.25 GiB memory in use. Of the allocated memory 17.73 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# # åœ¨åŸºç¡€çŸ¥è¯†è®­ç»ƒç»“æŸåä¿å­˜é€‚é…å™¨å‚æ•°\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # åœ¨SFTè®­ç»ƒå¼€å§‹å‰åŠ è½½å¯¹æ¯”\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"å‚æ•°æœªç»§æ‰¿ï¼\"\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n",
    "\n",
    "# SFTå¾®è°ƒè®­ç»ƒ\n",
    "def tokenize_function(examples):  # æ–°å¢å‚æ•°\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # åº”ä¸ºæœ‰æ•ˆæ•°å€¼\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # ç¦ç”¨æˆªæ–­ï¼ˆéœ€ç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ â‰¤ max_lengthï¼‰\n",
    "        max_length=7500,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # æ–°ä¿å­˜è·¯å¾„\n",
    "    num_train_epochs=5,  # å¢åŠ è®­ç»ƒè½®æ¬¡\n",
    "    per_device_train_batch_size=2,  # å¢å¤§æ‰¹é‡å¤§å°\n",
    "    gradient_accumulation_steps=2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    learning_rate=2e-4,  # è°ƒæ•´å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # ç¦ç”¨è¯„ä¼°é˜¶æ®µçš„æ··åˆç²¾åº¦\n",
    "    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–æ˜¾å­˜[4](@ref)\n",
    "    eval_steps=10,  # é™ä½è¯„ä¼°æ­¥æ•°ä»¥é€‚é…å°æ•°æ®é‡\n",
    "    logging_steps=1,      # æ¯ä¸ªè®­ç»ƒæ­¥éª¤è®°å½•æ—¥å¿—\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # æ–°å¢TensorBoardæ—¥å¿—ç›®å½•\n",
    "    # report_to=[\"tensorboard\"],  # å¯ç”¨TensorBoardæŠ¥å‘Š\n",
    "    # load_best_model_at_end=True  # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹\n",
    ")\n",
    "\n",
    "# swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    # callbacks=[swanlab_callback]\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)  # å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\n",
    "sft_trainer.train()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:27:31.699833Z",
     "iopub.status.busy": "2025-05-03T12:27:31.699539Z",
     "iopub.status.idle": "2025-05-03T12:27:31.702763Z",
     "shell.execute_reply": "2025-05-03T12:27:31.702330Z",
     "shell.execute_reply.started": "2025-05-03T12:27:31.699818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # æå–è®­ç»ƒæ—¥å¿—ï¼ˆåŒ…å« loss æˆ– train_loss å’Œ stepï¼‰\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # ç»Ÿä¸€å­—æ®µåä¸º train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # è½¬æ¢ä¸º DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # æå–è¯„ä¼°æ—¥å¿—ï¼ˆåŒ…å« eval_loss å’Œ stepï¼‰\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # åˆå¹¶è®­ç»ƒå’Œè¯„ä¼°æ—¥å¿—\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # ä½¿ç”¨ ffill() æ›¿ä»£ fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆä»»åŠ¡è¯„ä¼°ï¼ˆBLEU/ROUGE/METEORï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T06:13:43.779716Z",
     "iopub.status.busy": "2025-05-04T06:13:43.779434Z",
     "iopub.status.idle": "2025-05-04T06:13:52.602799Z",
     "shell.execute_reply": "2025-05-04T06:13:52.602329Z",
     "shell.execute_reply.started": "2025-05-04T06:13:43.779700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # ä¸­æ–‡åˆ†è¯æ”¯æŒ\n",
    "\n",
    "# åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "# df = pd.read_csv(\"game_strategy_and_term.csv\").tail(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T06:14:04.910545Z",
     "iopub.status.busy": "2025-05-04T06:14:04.910083Z",
     "iopub.status.idle": "2025-05-04T06:16:14.603684Z",
     "shell.execute_reply": "2025-05-04T06:16:14.603211Z",
     "shell.execute_reply.started": "2025-05-04T06:14:04.910519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'instruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.exp(loss).item()  \u001b[38;5;66;03m# è¿”å›å›°æƒ‘åº¦\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataset):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     instruction = \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstruction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# print(f\"æŒ‡ä»¤: {instruction}\")\u001b[39;00m\n\u001b[32m     49\u001b[39m     prompt = example[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'instruction'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# å…³é—­jiebaè°ƒè¯•æ—¥å¿—\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# ä¸­æ–‡åˆ†è¯å·¥å…·ï¼ˆæ”¹ç”¨æœç´¢å¼•æ“æ¨¡å¼æå‡å¬å›ç‡ï¼‰\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼[[2]]\n",
    "\n",
    "# ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "def generate_response(model, instruction, prompt):\n",
    "    input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®pad_token_idï¼ˆé¿å…è­¦å‘Šï¼‰\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è®¡ç®—BLEU/ROUGEï¼ˆå¢åŠ å¹³æ»‘å‡½æ•°å’ŒROUGE-Lï¼‰\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¨¡å‹å¯¹ç»™å®špromptå’Œresponseçš„å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # æ‹¼æ¥è¾“å…¥ä¸å“åº”\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # æ„é€ æ ‡ç­¾ï¼šä»…è®¡ç®—responseéƒ¨åˆ†çš„loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„lossè®¡ç®—\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # è¿”å›å›°æƒ‘åº¦\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    instruction = example[\"instruction\"]\n",
    "    # print(f\"æŒ‡ä»¤: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"è¾“å…¥: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"å‚è€ƒç­”æ¡ˆ: {reference}\")\n",
    "    \n",
    "    # ç”Ÿæˆé¢„æµ‹\n",
    "    base_output = generate_response(base_model, instruction, prompt)\n",
    "    ft_output = generate_response(finetuned_model, instruction, prompt)\n",
    "    # æ–°å¢ï¼šè®¡ç®—å›°æƒ‘åº¦\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # åˆ†è¯å¤„ç†\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEUä¼˜åŒ–ï¼šä½¿ç”¨BLEU-2+å¹³æ»‘å‡½æ•°\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # ä½¿ç”¨BLEU-2\n",
    "        smoothing_function=smoother.method1  # æ·»åŠ å¹³æ»‘\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGEä¼˜åŒ–ï¼šä½¿ç”¨åˆ†è¯åçš„æ–‡æœ¬å¹¶å¢åŠ ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # æ–°å¢æ‰“å°å†…å®¹ï¼šå¯¹æ¯”ç”Ÿæˆç»“æœä¸å‚è€ƒç­”æ¡ˆ\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} åŸå§‹å†…å®¹ {'='*20}\")\n",
    "    # print(f\"ã€æŒ‡ä»¤ã€‘: {instruction}\")\n",
    "    # print(f\"ã€è¾“å…¥ã€‘: {prompt}\")\n",
    "    print(f\"ã€å‚è€ƒå›ç­”ã€‘: {reference}\")\n",
    "    print(f\"\\nã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: {base_output}\")\n",
    "    print(f\"ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: {ft_output}\")\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡æ‰“å°ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} è¯„ä¼°ç»“æœ {'='*20}\")\n",
    "    print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # æ‰“å°å›°æƒ‘åº¦ç»“æœ\n",
    "    print(f\"Perplexity - åŸå§‹: {base_ppl:.2f}, å¾®è°ƒ: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"æ ·æœ¬{i+1}è¯„ä¼°ç»“æœ:\")\n",
    "    # print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
