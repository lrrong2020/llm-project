{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:10:43.805598Z",
     "iopub.status.busy": "2025-05-03T12:10:43.805303Z",
     "iopub.status.idle": "2025-05-03T12:10:47.374046Z",
     "shell.execute_reply": "2025-05-03T12:10:47.373611Z",
     "shell.execute_reply.started": "2025-05-03T12:10:43.805581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 20:10:47,362 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# #模型下载\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T15:23:18.583965Z",
     "iopub.status.busy": "2025-05-03T15:23:18.583491Z",
     "iopub.status.idle": "2025-05-03T15:23:18.592908Z",
     "shell.execute_reply": "2025-05-03T15:23:18.592480Z",
     "shell.execute_reply.started": "2025-05-03T15:23:18.583949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # 正确的小写导入\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# 基础配置\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" #参数量和 gpt2 一样.\n",
    "basic_model_path = \"./lora_basic\"     # 基础微调结果\n",
    "sft_model_path = \"./lora_sft\"        # SFT微调结果\n",
    "# merged_model_path = \"./lora_basic_sft\"  # 合并模型\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # 替换为实际的本地模型路径\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # 禁用截断\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # 找到 <Response> 的起始位置\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # 增大 LoRA 矩阵秩\n",
    "    lora_alpha=16,  # 调整 alpha 值\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # 扩展目标模块\n",
    "    target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, 增强特征提取能力\n",
    "    lora_dropout=0.1,  # 增加 dropout 防止过拟合\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # 确保处于训练模式[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # 允许后续微调时更新头部[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:16.790096Z",
     "iopub.status.busy": "2025-05-03T14:20:16.789706Z",
     "iopub.status.idle": "2025-05-03T14:20:17.690541Z",
     "shell.execute_reply": "2025-05-03T14:20:17.690103Z",
     "shell.execute_reply.started": "2025-05-03T14:20:16.790075Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==================== 基础知识微调部分 ====================\n",
    "# 加载基础知识数据集\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # 确保只有prompt和response列\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# 基础知识数据预处理函数\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# 加载基础知识数据集\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # 选择前5个样本\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # 加载前10行数据\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # 加载迷你数据集\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # 加载数据集\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # 选择前5个样本 [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:17.691412Z",
     "iopub.status.busy": "2025-05-03T14:20:17.691220Z",
     "iopub.status.idle": "2025-05-03T14:20:17.700613Z",
     "shell.execute_reply": "2025-05-03T14:20:17.700107Z",
     "shell.execute_reply.started": "2025-05-03T14:20:17.691399Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>狼美人在狼人杀游戏中应该如何发挥其特殊技能“魅惑殉情”以获得胜利？</Prompt>\\n<Response>在狼人杀游戏中，狼美人是一个具有特殊技能的角色。她的技能是“魅惑殉情”，可以与另一名玩家绑定，若狼美人在游戏中出局，该玩家也会随之出局。为了帮助狼人杀新手玩家更好地了解狼美人的玩法，本文将详细介绍狼美人的玩法攻略。\\n1、了解狼美人的技能和胜利条件\\n狼美人的技能是“魅惑殉情”，可以将该技能使用在神职玩家身上，以发挥其最大效果。否则，如果只是杀死一个平民，对于狼美人来说就没有存在的必要了。狼美人的胜利条件与狼人一致，需要将所有神职玩家淘汰出局。\\n2、绑定强神\\n狼美人想要发挥出最大的效果，就需要绑定强神。在游戏初期，强神中的预言家会起跳，因此狼队可以选择抗推或者夜晚刀掉预言家。而女巫、守卫和猎人是强神中的重要角色，狼美人应该优先将他们作为魅惑对象。为了成功绑定强神，狼美人需要具备一定的抿神能力。\\n3、优先选择魅惑对象\\n狼美人需要学会隐藏自己的身份，并将自己的魅力对象选择为强大的神职角色，如女巫和守卫。在游戏初期，狼美人不需要悍跳神牌，只需暗中观察找到预言家、女巫和守卫中的一个，魅力优先级：女巫>守卫>预言家。下一步就是想方设法让自己出局把迷人的对象带走，这样可以给狼队增加一个轮次。\\n4、与狼队友共享信息\\n在游戏过程中，狼美人需要与狼队友共享迷惑的信息。如果狼美人没有连中神牌，狼队友可以选择自爆，创造新的机会让狼美人再次迷惑，间接给狼队增加新的轮次。\\n5、学会伪装\\n在发言时，狼美人需要将自己描述为普通狼人的样子。同时，需要发挥自己是狼人的视角优势，把自己打成焦点牌，但能故意聊得很差，让好人把你标为普通狼人票出局，这样你就能顺利带走强神！\\n总之，在狼人杀游戏中扮演狼美人时，你需要掌握以上攻略，以便更好地发挥自己的特殊技能。通过观察和抿神的能力，选择正确的魅惑对象和时机，与狼队友保持沟通并学会伪装自己。只有这样，才能在游戏中获得胜利！</Response>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:19.753901Z",
     "iopub.status.busy": "2025-05-03T14:20:19.753629Z",
     "iopub.status.idle": "2025-05-03T14:20:22.775573Z",
     "shell.execute_reply": "2025-05-03T14:20:22.775026Z",
     "shell.execute_reply.started": "2025-05-03T14:20:19.753883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,404,416 || all params: 1,553,118,720 || trainable%: 0.6055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # 基础知识微调训练\n",
    "# print(\"开始基础知识微调训练...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:55:40.202425Z",
     "iopub.status.busy": "2025-05-03T12:55:40.202120Z",
     "iopub.status.idle": "2025-05-03T12:55:40.205091Z",
     "shell.execute_reply": "2025-05-03T12:55:40.204511Z",
     "shell.execute_reply.started": "2025-05-03T12:55:40.202408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T14:20:22.776607Z",
     "iopub.status.busy": "2025-05-03T14:20:22.776296Z",
     "iopub.status.idle": "2025-05-03T14:53:42.231949Z",
     "shell.execute_reply": "2025-05-03T14:53:42.231421Z",
     "shell.execute_reply.started": "2025-05-03T14:20:22.776592Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始基础知识微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 676/676 [00:00<00:00, 1026.21 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 2062.30 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 33:10, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.983100</td>\n",
       "      <td>3.026573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.997700</td>\n",
       "      <td>2.866308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.927200</td>\n",
       "      <td>2.788158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.430700</td>\n",
       "      <td>2.732572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.526800</td>\n",
       "      <td>2.689067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.046700</td>\n",
       "      <td>2.664482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.816600</td>\n",
       "      <td>2.631630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.570300</td>\n",
       "      <td>2.604075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.321900</td>\n",
       "      <td>2.590086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.316100</td>\n",
       "      <td>2.572315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.695200</td>\n",
       "      <td>2.561522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.108000</td>\n",
       "      <td>2.547211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.373900</td>\n",
       "      <td>2.539508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.726000</td>\n",
       "      <td>2.528158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.810800</td>\n",
       "      <td>2.523940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.253500</td>\n",
       "      <td>2.518830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.384800</td>\n",
       "      <td>2.511735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.755600</td>\n",
       "      <td>2.514043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.733700</td>\n",
       "      <td>2.508328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.289600</td>\n",
       "      <td>2.508166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.522100</td>\n",
       "      <td>2.500364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>2.497128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.181400</td>\n",
       "      <td>2.496612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.952900</td>\n",
       "      <td>2.494183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.210500</td>\n",
       "      <td>2.492880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础知识微调完成!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic/tokenizer_config.json',\n",
       " './lora_basic/special_tokens_map.json',\n",
       " './lora_basic/vocab.json',\n",
       " './lora_basic/merges.txt',\n",
       " './lora_basic/added_tokens.json',\n",
       " './lora_basic/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基础知识微调训练\n",
    "print(\"开始基础知识微调训练...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # 基础知识微调可以使用较少的epoch\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # 基础知识微调可以使用稍高的学习率\n",
    "    fp16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"基础知识微调完成!\")\n",
    "\n",
    "# 新增保存逻辑\n",
    "# basic_model_path = \"./lora_basic\"  # 指定基础微调保存路径\n",
    "model.save_pretrained(basic_model_path)  # 保存LoRA适配器参数[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T15:24:59.273801Z",
     "iopub.status.busy": "2025-05-03T15:24:59.273499Z",
     "iopub.status.idle": "2025-05-03T15:25:02.311009Z",
     "shell.execute_reply": "2025-05-03T15:25:02.310323Z",
     "shell.execute_reply.started": "2025-05-03T15:24:59.273771Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,553,118,720 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 在SFT训练代码前重新初始化模型并加载基础微调结果：\n",
    "# 重新初始化基础模型（重要！）\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # 强制使用 FP16 计算\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # 从基础微调保存路径加载\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#启用梯度检查点\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# 加载第一阶段LoRA参数\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # 验证参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T15:25:03.287075Z",
     "iopub.status.busy": "2025-05-03T15:25:03.286793Z",
     "iopub.status.idle": "2025-05-03T15:25:04.258670Z",
     "shell.execute_reply": "2025-05-03T15:25:04.258156Z",
     "shell.execute_reply.started": "2025-05-03T15:25:03.287061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== SFT微调部分 ====================\n",
    "# SFT数据预处理函数\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# 加载SFT数据集\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "# sft_dataset = sft_dataset.select(range(50))  # 选择前5个样本\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T15:22:42.059018Z",
     "iopub.status.busy": "2025-05-03T15:22:42.058725Z",
     "iopub.status.idle": "2025-05-03T15:22:42.061454Z",
     "shell.execute_reply": "2025-05-03T15:22:42.060880Z",
     "shell.execute_reply.started": "2025-05-03T15:22:42.059002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # 查看模型支持的最大长度（如512）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# 爆显存, 需要将数据集 cut 一半试试, 主要 response 不能够被截断, 所以不能 truncation. 最长的输入序列是 7482."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T15:36:22.120961Z",
     "iopub.status.busy": "2025-05-03T15:36:22.120645Z",
     "iopub.status.idle": "2025-05-03T15:37:05.958667Z",
     "shell.execute_reply": "2025-05-03T15:37:05.957761Z",
     "shell.execute_reply.started": "2025-05-03T15:36:22.120942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40253/2706130334.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # 创建梯度缩放器\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始SFT微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10920/10920 [00:36<00:00, 300.04 examples/s]\n",
      "Map: 100%|██████████| 1214/1214 [00:04<00:00, 295.84 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.49 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.70 GiB is free. Process 103991 has 21.99 GiB memory in use. Of the allocated memory 19.84 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     55\u001b[39m sft_trainer = Trainer(\n\u001b[32m     56\u001b[39m     model=model,\n\u001b[32m     57\u001b[39m     args=sft_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     61\u001b[39m )\n\u001b[32m     62\u001b[39m torch.autograd.set_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# 开启梯度异常检测\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43msft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# 保存最终模型\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# model.save_pretrained(\"./lora_final\")\u001b[39;00m\n\u001b[32m     67\u001b[39m tokenizer.save_pretrained(sft_model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/peft/peft_model.py:1756\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1755\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1757\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1767\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1769\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:843\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[32m    846\u001b[39m     loss=loss,\n\u001b[32m    847\u001b[39m     logits=logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    850\u001b[39m     attentions=outputs.attentions,\n\u001b[32m    851\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/loss/loss_utils.py:63\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     62\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/loss/loss_utils.py:35\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     28\u001b[39m     source: torch.Tensor,\n\u001b[32m     29\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     **kwargs,\n\u001b[32m     33\u001b[39m ) -> torch.Tensor:\n\u001b[32m     34\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     37\u001b[39m         loss = loss / num_items_in_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 8.49 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.70 GiB is free. Process 103991 has 21.99 GiB memory in use. Of the allocated memory 19.84 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# # 在基础知识训练结束后保存适配器参数\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # 在SFT训练开始前加载对比\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"参数未继承！\"\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()  # 创建梯度缩放器\n",
    "\n",
    "# SFT微调训练\n",
    "def tokenize_function(examples):  # 新增参数\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # 应为有效数值\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # 禁用截断（需确保所有样本长度 ≤ max_length）\n",
    "        max_length=7500,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # 找到 <Response> 的起始位置\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "print(\"开始SFT微调训练...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # 新保存路径\n",
    "    num_train_epochs=5,  # 增加训练轮次\n",
    "    per_device_train_batch_size=2,  # 增大批量大小\n",
    "    gradient_accumulation_steps=2,  # 增加梯度累积步数\n",
    "    learning_rate=2e-4,  # 调整学习率\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # 禁用评估阶段的混合精度\n",
    "    gradient_checkpointing=True,  # 启用梯度检查点优化显存[4](@ref)\n",
    "    eval_steps=10,  # 降低评估步数以适配小数据量\n",
    "    logging_steps=1,      # 每个训练步骤记录日志\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # 新增TensorBoard日志目录\n",
    "    # report_to=[\"tensorboard\"],  # 启用TensorBoard报告\n",
    "    # load_best_model_at_end=True  # 自动加载最佳模型\n",
    ")\n",
    "\n",
    "# 创建Trainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)  # 开启梯度异常检测\n",
    "sft_trainer.train()\n",
    "\n",
    "# 保存最终模型\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"模型已保存至 {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:27:31.699833Z",
     "iopub.status.busy": "2025-05-03T12:27:31.699539Z",
     "iopub.status.idle": "2025-05-03T12:27:31.702763Z",
     "shell.execute_reply": "2025-05-03T12:27:31.702330Z",
     "shell.execute_reply.started": "2025-05-03T12:27:31.699818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # 提取训练日志（包含 loss 或 train_loss 和 step）\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # 统一字段名为 train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # 转换为 DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # 提取评估日志（包含 eval_loss 和 step）\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # 合并训练和评估日志\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # 使用 ffill() 替代 fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# 生成任务评估（BLEU/ROUGE/METEOR）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3e34c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T13:05:22.417384Z",
     "iopub.status.busy": "2025-05-03T13:05:22.417072Z",
     "iopub.status.idle": "2025-05-03T13:05:25.111606Z",
     "shell.execute_reply": "2025-05-03T13:05:25.111127Z",
     "shell.execute_reply.started": "2025-05-03T13:05:22.417366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # 中文分词支持\n",
    "\n",
    "# 加载原始模型和微调模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载测试数据\n",
    "df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b9c6613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T13:05:25.112735Z",
     "iopub.status.busy": "2025-05-03T13:05:25.112396Z",
     "iopub.status.idle": "2025-05-03T13:07:32.450866Z",
     "shell.execute_reply": "2025-05-03T13:07:32.450294Z",
     "shell.execute_reply.started": "2025-05-03T13:05:25.112717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本1 原始内容 ====================\n",
      "【参考回答】: {\"想要展示的身份\": \"村民\", \"身份标签\": {\"1号玩家\": \"未知身份\", \"2号玩家\": \"未知身份\", \"3号玩家\": \"未知身份\", \"4号玩家\": \"未知身份\", \"5号玩家\": \"未知身份\", \"6号玩家\": \"村民和狼人\", \"7号玩家\": \"猎人\", \"9号玩家\": \"未知身份\"}, \"归票\": \"无\", \"发言\": \"我是村民。目前只知道出局的7号是猎人，但是没有办法定义7号带走的6号是什么身份，我会听一下后置位的发言，再去考虑站边问题。\"}\n",
      "\n",
      "【基础模型生成】: [INST] <<SYS>>\n",
      "\n",
      "你现在正在玩一种叫做“狼人杀”的游戏。\n",
      "在这款游戏中，玩家通常被分为两个阵营：狼人和村民。\n",
      "狼人杀游戏中不同角色的玩家有不同的目标：\n",
      "- 村民的目的是识别出狼人，并通过投票使他们出局。\n",
      "- 对于狼人来说，他们的主要目标是隐藏他们的真实身份，在讨论中误导他人，以免被投票出局并尽可能的猎杀村民。\n",
      "以下是一些基本规则：\n",
      "- 身份：玩家的身份是秘密分配的。狼人彼此知道对方的身份，而村民只知道自己的身份。\n",
      "- 昼夜更替：游戏有交替的白天和黑夜阶段。夜里，狼人秘密选择一名村民猎杀。白天，所有玩家讨论并投票决定他们认为是狼人的玩家，票数最多的玩家被淘汰。\n",
      "- 特殊角色：游戏中有存在一些有特殊能力的角色，比如能得知玩家身份的“预言家”等。\n",
      "- 获胜条件：当游戏中有一个群体实现它们的获胜条件时游戏结束。如果所有狼人被淘汰，村民就获胜。如果狼人杀死了所有普通村民或所有特殊角色，狼人就获胜。\n",
      "\n",
      "在这个游戏中，我们有从1到9号共9名玩家 —— 6名村民和3名狼人。村民中有特殊角色，包括：\n",
      "- 1位预言家：\n",
      "    - 目标：预言家的目的是帮助村民识别狼人。\n",
      "    - 能力：在夜晚阶段，预言家可以秘密选择一名玩家，每晚了解他的真实身份（是否为狼人）。\n",
      "- 1位女巫：\n",
      "    - 目标：女巫的目的是策略性地使用她的特殊能力来帮助村民。\n",
      "    - 能力：女巫有一瓶解药和一瓶毒药。一旦使用，后续回合中不能再用。女巫不能在同一晚既使用解药又使用毒药。解药可以用来救一名在夜间被狼人猎杀的玩家。毒药可以淘汰一名很可能是狼人的玩家。\n",
      "- 1位猎人：\n",
      "    - 目标：猎人的目的是策略性地使用他的特殊能力帮助村民消灭狼人。\n",
      "    - 能力：当猎人被狼人杀害或者在白天被放逐出局后，他可以翻开自己的身份牌并向场上任意一位活着的玩家射出一发复仇的子弹，带着这位玩家一起死亡。猎人可以选择不翻牌，但是只要翻了牌就必须带人（注意，当猎人被女巫毒杀后，不能翻牌带人）。\n",
      "其他的都是普通村民。\n",
      "<</SYS>>\n",
      "\n",
      "在本场游戏中，你目前已知以下信息：\n",
      "1. 角色设定：你是8号玩家。\n",
      "你的身份是：村民。\n",
      "你没有特殊能力，但你的目标是观察、讨论并投票揭示狼人的身份，并努力生存下来。\n",
      "2. 客观信息：\n",
      "- 游戏进程：目前游戏进行到第1轮。\n",
      "- 当前存活的玩家有：1号，2号，3号，4号，5号，8号，9号，\n",
      "- 猎人开枪记录：7号猎人在第1轮夜晚被杀害出局，开枪带走了6号玩家。\n",
      "\n",
      "- 本轮的发言顺序为：5号玩家；8号玩家；9号玩家；1号玩家；2号玩家；3号玩家；4号玩家。\n",
      "- 夜晚信息：第1轮7号玩家死亡。\n",
      "- 投票情况：暂无\n",
      "3. 主观信息：\n",
      "\n",
      "目前是第1轮，本轮在你之前的玩家发言：\n",
      "**5号玩家**：我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，我再听一听后置位的玩家的发言，今天放逐后置位悍跳预言家的狼人，今晚上我去查验3号的身份。\n",
      "\n",
      "你目前是8号村民。请综合角色设定、客观信息以及主观信息分析场上目前的局势（注意客观信息一定为真实的，主观信息可能包含欺骗性的发言），总结接下来的发言意图（包括发言中希望向大家呈现的身份、发言中为每位玩家贴上的身份标签以及最终的归票）并组织你本轮的发言。请用关键字为“想要展示的身份”、“身份标签”、“归票”和“发言”的json格式输出。\n",
      " [/INST] {\n",
      "  \"想要展示的身份\": \"预言家\",\n",
      "  \"身份标签\": \"预言家\",\n",
      "  \"归票\": \"预言家\",\n",
      "  \"发言\": \"我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，\n",
      "【微调模型生成】: [INST] <<SYS>>\n",
      "\n",
      "你现在正在玩一种叫做“狼人杀”的游戏。\n",
      "在这款游戏中，玩家通常被分为两个阵营：狼人和村民。\n",
      "狼人杀游戏中不同角色的玩家有不同的目标：\n",
      "- 村民的目的是识别出狼人，并通过投票使他们出局。\n",
      "- 对于狼人来说，他们的主要目标是隐藏他们的真实身份，在讨论中误导他人，以免被投票出局并尽可能的猎杀村民。\n",
      "以下是一些基本规则：\n",
      "- 身份：玩家的身份是秘密分配的。狼人彼此知道对方的身份，而村民只知道自己的身份。\n",
      "- 昼夜更替：游戏有交替的白天和黑夜阶段。夜里，狼人秘密选择一名村民猎杀。白天，所有玩家讨论并投票决定他们认为是狼人的玩家，票数最多的玩家被淘汰。\n",
      "- 特殊角色：游戏中有存在一些有特殊能力的角色，比如能得知玩家身份的“预言家”等。\n",
      "- 获胜条件：当游戏中有一个群体实现它们的获胜条件时游戏结束。如果所有狼人被淘汰，村民就获胜。如果狼人杀死了所有普通村民或所有特殊角色，狼人就获胜。\n",
      "\n",
      "在这个游戏中，我们有从1到9号共9名玩家 —— 6名村民和3名狼人。村民中有特殊角色，包括：\n",
      "- 1位预言家：\n",
      "    - 目标：预言家的目的是帮助村民识别狼人。\n",
      "    - 能力：在夜晚阶段，预言家可以秘密选择一名玩家，每晚了解他的真实身份（是否为狼人）。\n",
      "- 1位女巫：\n",
      "    - 目标：女巫的目的是策略性地使用她的特殊能力来帮助村民。\n",
      "    - 能力：女巫有一瓶解药和一瓶毒药。一旦使用，后续回合中不能再用。女巫不能在同一晚既使用解药又使用毒药。解药可以用来救一名在夜间被狼人猎杀的玩家。毒药可以淘汰一名很可能是狼人的玩家。\n",
      "- 1位猎人：\n",
      "    - 目标：猎人的目的是策略性地使用他的特殊能力帮助村民消灭狼人。\n",
      "    - 能力：当猎人被狼人杀害或者在白天被放逐出局后，他可以翻开自己的身份牌并向场上任意一位活着的玩家射出一发复仇的子弹，带着这位玩家一起死亡。猎人可以选择不翻牌，但是只要翻了牌就必须带人（注意，当猎人被女巫毒杀后，不能翻牌带人）。\n",
      "其他的都是普通村民。\n",
      "<</SYS>>\n",
      "\n",
      "在本场游戏中，你目前已知以下信息：\n",
      "1. 角色设定：你是8号玩家。\n",
      "你的身份是：村民。\n",
      "你没有特殊能力，但你的目标是观察、讨论并投票揭示狼人的身份，并努力生存下来。\n",
      "2. 客观信息：\n",
      "- 游戏进程：目前游戏进行到第1轮。\n",
      "- 当前存活的玩家有：1号，2号，3号，4号，5号，8号，9号，\n",
      "- 猎人开枪记录：7号猎人在第1轮夜晚被杀害出局，开枪带走了6号玩家。\n",
      "\n",
      "- 本轮的发言顺序为：5号玩家；8号玩家；9号玩家；1号玩家；2号玩家；3号玩家；4号玩家。\n",
      "- 夜晚信息：第1轮7号玩家死亡。\n",
      "- 投票情况：暂无\n",
      "3. 主观信息：\n",
      "\n",
      "目前是第1轮，本轮在你之前的玩家发言：\n",
      "**5号玩家**：我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，我再听一听后置位的玩家的发言，今天放逐后置位悍跳预言家的狼人，今晚上我去查验3号的身份。\n",
      "\n",
      "你目前是8号村民。请综合角色设定、客观信息以及主观信息分析场上目前的局势（注意客观信息一定为真实的，主观信息可能包含欺骗性的发言），总结接下来的发言意图（包括发言中希望向大家呈现的身份、发言中为每位玩家贴上的身份标签以及最终的归票）并组织你本轮的发言。请用关键字为“想要展示的身份”、“身份标签”、“归票”和“发言”的json格式输出。\n",
      " [/INST] {\n",
      "  \"想要展示的身份\": \"预言家\",\n",
      "  \"身份标签\": \"预言家\",\n",
      "  \"归票\": \"预言家\",\n",
      "  \"发言\": \"我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，\n",
      "\n",
      "==================== 样本1 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.0792, 微调: 0.0792\n",
      "ROUGE-2 F1 - 原始: 0.2000, 微调: 0.2000\n",
      "ROUGE-L F1 - 原始: 0.3226, 微调: 0.3226\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 3.60, 微调: 3.60\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本2 原始内容 ====================\n",
      "【参考回答】: {\"想要展示的身份\": \"村民\", \"身份标签\": {\"1号玩家\": \"未知身份\", \"2号玩家\": \"未知身份\", \"3号玩家\": \"未知身份\", \"4号玩家\": \"村民\", \"5号玩家\": \"预言家\", \"6号玩家\": \"村民和狼人\", \"7号玩家\": \"猎人\", \"8号玩家\": \"未知身份\"}, \"归票\": \"无\", \"发言\": \"我9号底牌是村民身份，昨晚7号猎人吃刀，女巫没有使用解药。我不知道被7号猎人带走的6号玩家是什么身份，6号可能是村民也有可能是狼人，希望6号玩家是个狼人。5号玩家第一个发言跳预言家报4号金水，我认为5号还是比较有力度的，不怕4号反水立警。我认为5号是真预言家可能性较大，我听一下后置位玩家的发言来进行投票。\"}\n",
      "\n",
      "【基础模型生成】: [INST] <<SYS>>\n",
      "\n",
      "你现在正在玩一种叫做“狼人杀”的游戏。\n",
      "在这款游戏中，玩家通常被分为两个阵营：狼人和村民。\n",
      "狼人杀游戏中不同角色的玩家有不同的目标：\n",
      "- 村民的目的是识别出狼人，并通过投票使他们出局。\n",
      "- 对于狼人来说，他们的主要目标是隐藏他们的真实身份，在讨论中误导他人，以免被投票出局并尽可能的猎杀村民。\n",
      "以下是一些基本规则：\n",
      "- 身份：玩家的身份是秘密分配的。狼人彼此知道对方的身份，而村民只知道自己的身份。\n",
      "- 昼夜更替：游戏有交替的白天和黑夜阶段。夜里，狼人秘密选择一名村民猎杀。白天，所有玩家讨论并投票决定他们认为是狼人的玩家，票数最多的玩家被淘汰。\n",
      "- 特殊角色：游戏中有存在一些有特殊能力的角色，比如能得知玩家身份的“预言家”等。\n",
      "- 获胜条件：当游戏中有一个群体实现它们的获胜条件时游戏结束。如果所有狼人被淘汰，村民就获胜。如果狼人杀死了所有普通村民或所有特殊角色，狼人就获胜。\n",
      "\n",
      "在这个游戏中，我们有从1到9号共9名玩家 —— 6名村民和3名狼人。村民中有特殊角色，包括：\n",
      "- 1位预言家：\n",
      "    - 目标：预言家的目的是帮助村民识别狼人。\n",
      "    - 能力：在夜晚阶段，预言家可以秘密选择一名玩家，每晚了解他的真实身份（是否为狼人）。\n",
      "- 1位女巫：\n",
      "    - 目标：女巫的目的是策略性地使用她的特殊能力来帮助村民。\n",
      "    - 能力：女巫有一瓶解药和一瓶毒药。一旦使用，后续回合中不能再用。女巫不能在同一晚既使用解药又使用毒药。解药可以用来救一名在夜间被狼人猎杀的玩家。毒药可以淘汰一名很可能是狼人的玩家。\n",
      "- 1位猎人：\n",
      "    - 目标：猎人的目的是策略性地使用他的特殊能力帮助村民消灭狼人。\n",
      "    - 能力：当猎人被狼人杀害或者在白天被放逐出局后，他可以翻开自己的身份牌并向场上任意一位活着的玩家射出一发复仇的子弹，带着这位玩家一起死亡。猎人可以选择不翻牌，但是只要翻了牌就必须带人（注意，当猎人被女巫毒杀后，不能翻牌带人）。\n",
      "其他的都是普通村民。\n",
      "<</SYS>>\n",
      "\n",
      "在本场游戏中，你目前已知以下信息：\n",
      "1. 角色设定：你是9号玩家。\n",
      "你的身份是：村民。\n",
      "你没有特殊能力，但你的目标是观察、讨论并投票揭示狼人的身份，并努力生存下来。\n",
      "2. 客观信息：\n",
      "- 游戏进程：目前游戏进行到第1轮。\n",
      "- 当前存活的玩家有：1号，2号，3号，4号，5号，8号，9号，\n",
      "- 猎人开枪记录：7号猎人在第1轮夜晚被杀害出局，开枪带走了6号玩家。\n",
      "\n",
      "- 本轮的发言顺序为：5号玩家；8号玩家；9号玩家；1号玩家；2号玩家；3号玩家；4号玩家。\n",
      "- 夜晚信息：第1轮7号玩家死亡。\n",
      "- 投票情况：暂无\n",
      "3. 主观信息：\n",
      "\n",
      "目前是第1轮，本轮在你之前的玩家发言：\n",
      "**5号玩家**：我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，我再听一听后置位的玩家的发言，今天放逐后置位悍跳预言家的狼人，今晚上我去查验3号的身份。\n",
      "**8号玩家**：我是村民。目前只知道出局的7号是猎人，但是没有办法定义7号带走的6号是什么身份，我会听一下后置位的发言，再去考虑站边问题。\n",
      "\n",
      "你目前是9号村民。请综合角色设定、客观信息以及主观信息分析场上目前的局势（注意客观信息一定为真实的，主观信息可能包含欺骗性的发言），总结接下来的发言意图（包括发言中希望向大家呈现的身份、发言中为每位玩家贴上的身份标签以及最终的归票）并组织你本轮的发言。请用关键字为“想要展示的身份”、“身份标签”、“归票”和“发言”的json格式输出。\n",
      " [/INST] {\n",
      "  \"想要展示的身份\": \"预言家\",\n",
      "  \"身份标签\": \"预言家\",\n",
      "  \"归票\": \"预言家\",\n",
      "  \"发言\": \"我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，\n",
      "【微调模型生成】: [INST] <<SYS>>\n",
      "\n",
      "你现在正在玩一种叫做“狼人杀”的游戏。\n",
      "在这款游戏中，玩家通常被分为两个阵营：狼人和村民。\n",
      "狼人杀游戏中不同角色的玩家有不同的目标：\n",
      "- 村民的目的是识别出狼人，并通过投票使他们出局。\n",
      "- 对于狼人来说，他们的主要目标是隐藏他们的真实身份，在讨论中误导他人，以免被投票出局并尽可能的猎杀村民。\n",
      "以下是一些基本规则：\n",
      "- 身份：玩家的身份是秘密分配的。狼人彼此知道对方的身份，而村民只知道自己的身份。\n",
      "- 昼夜更替：游戏有交替的白天和黑夜阶段。夜里，狼人秘密选择一名村民猎杀。白天，所有玩家讨论并投票决定他们认为是狼人的玩家，票数最多的玩家被淘汰。\n",
      "- 特殊角色：游戏中有存在一些有特殊能力的角色，比如能得知玩家身份的“预言家”等。\n",
      "- 获胜条件：当游戏中有一个群体实现它们的获胜条件时游戏结束。如果所有狼人被淘汰，村民就获胜。如果狼人杀死了所有普通村民或所有特殊角色，狼人就获胜。\n",
      "\n",
      "在这个游戏中，我们有从1到9号共9名玩家 —— 6名村民和3名狼人。村民中有特殊角色，包括：\n",
      "- 1位预言家：\n",
      "    - 目标：预言家的目的是帮助村民识别狼人。\n",
      "    - 能力：在夜晚阶段，预言家可以秘密选择一名玩家，每晚了解他的真实身份（是否为狼人）。\n",
      "- 1位女巫：\n",
      "    - 目标：女巫的目的是策略性地使用她的特殊能力来帮助村民。\n",
      "    - 能力：女巫有一瓶解药和一瓶毒药。一旦使用，后续回合中不能再用。女巫不能在同一晚既使用解药又使用毒药。解药可以用来救一名在夜间被狼人猎杀的玩家。毒药可以淘汰一名很可能是狼人的玩家。\n",
      "- 1位猎人：\n",
      "    - 目标：猎人的目的是策略性地使用他的特殊能力帮助村民消灭狼人。\n",
      "    - 能力：当猎人被狼人杀害或者在白天被放逐出局后，他可以翻开自己的身份牌并向场上任意一位活着的玩家射出一发复仇的子弹，带着这位玩家一起死亡。猎人可以选择不翻牌，但是只要翻了牌就必须带人（注意，当猎人被女巫毒杀后，不能翻牌带人）。\n",
      "其他的都是普通村民。\n",
      "<</SYS>>\n",
      "\n",
      "在本场游戏中，你目前已知以下信息：\n",
      "1. 角色设定：你是9号玩家。\n",
      "你的身份是：村民。\n",
      "你没有特殊能力，但你的目标是观察、讨论并投票揭示狼人的身份，并努力生存下来。\n",
      "2. 客观信息：\n",
      "- 游戏进程：目前游戏进行到第1轮。\n",
      "- 当前存活的玩家有：1号，2号，3号，4号，5号，8号，9号，\n",
      "- 猎人开枪记录：7号猎人在第1轮夜晚被杀害出局，开枪带走了6号玩家。\n",
      "\n",
      "- 本轮的发言顺序为：5号玩家；8号玩家；9号玩家；1号玩家；2号玩家；3号玩家；4号玩家。\n",
      "- 夜晚信息：第1轮7号玩家死亡。\n",
      "- 投票情况：暂无\n",
      "3. 主观信息：\n",
      "\n",
      "目前是第1轮，本轮在你之前的玩家发言：\n",
      "**5号玩家**：我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，我再听一听后置位的玩家的发言，今天放逐后置位悍跳预言家的狼人，今晚上我去查验3号的身份。\n",
      "**8号玩家**：我是村民。目前只知道出局的7号是猎人，但是没有办法定义7号带走的6号是什么身份，我会听一下后置位的发言，再去考虑站边问题。\n",
      "\n",
      "你目前是9号村民。请综合角色设定、客观信息以及主观信息分析场上目前的局势（注意客观信息一定为真实的，主观信息可能包含欺骗性的发言），总结接下来的发言意图（包括发言中希望向大家呈现的身份、发言中为每位玩家贴上的身份标签以及最终的归票）并组织你本轮的发言。请用关键字为“想要展示的身份”、“身份标签”、“归票”和“发言”的json格式输出。\n",
      " [/INST] {\n",
      "  \"想要展示的身份\": \"预言家\",\n",
      "  \"身份标签\": \"预言家\",\n",
      "  \"归票\": \"预言家\",\n",
      "  \"发言\": \"我是预言家，我昨天晚上查验4号是金水。我查验4号的心路历程就是在我左边或右边随机选择了一名玩家查验，我最后选择了4号玩家，查验出来是好人。我第一个发言没有什么信息，7号猎人开枪带走的6号我也不知道是什么身份，\n",
      "\n",
      "==================== 样本2 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.1115, 微调: 0.1115\n",
      "ROUGE-2 F1 - 原始: 0.2778, 微调: 0.2778\n",
      "ROUGE-L F1 - 原始: 0.3784, 微调: 0.3784\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 6.46, 微调: 6.46\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 关闭jieba调试日志\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# 中文分词工具（改用搜索引擎模式提升召回率）\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # 使用搜索引擎模式[[2]]\n",
    "\n",
    "# 生成预测文本（增加生成长度）\n",
    "def generate_response(model, instruction, prompt):\n",
    "    input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)  # 增加生成长度[[1]]\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 显式设置pad_token_id（避免警告）\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 计算BLEU/ROUGE（增加平滑函数和ROUGE-L）\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    计算模型对给定prompt和response的困惑度\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # 拼接输入与响应\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 构造标签：仅计算response部分的loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # 忽略prompt部分的loss计算\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # 返回困惑度\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    instruction = example[\"instruction\"]\n",
    "    # print(f\"指令: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"输入: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"参考答案: {reference}\")\n",
    "    \n",
    "    # 生成预测\n",
    "    base_output = generate_response(base_model, instruction, prompt)\n",
    "    ft_output = generate_response(finetuned_model, instruction, prompt)\n",
    "    # 新增：计算困惑度\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # 分词处理\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEU优化：使用BLEU-2+平滑函数\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # 使用BLEU-2\n",
    "        smoothing_function=smoother.method1  # 添加平滑\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGE优化：使用分词后的文本并增加ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # 新增打印内容：对比生成结果与参考答案\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 原始内容 {'='*20}\")\n",
    "    # print(f\"【指令】: {instruction}\")\n",
    "    # print(f\"【输入】: {prompt}\")\n",
    "    print(f\"【参考回答】: {reference}\")\n",
    "    print(f\"\\n【基础模型生成】: {base_output}\")\n",
    "    print(f\"【微调模型生成】: {ft_output}\")\n",
    "    \n",
    "    # 评估指标打印（保持原有格式）\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 评估结果 {'='*20}\")\n",
    "    print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # 打印困惑度结果\n",
    "    print(f\"Perplexity - 原始: {base_ppl:.2f}, 微调: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"样本{i+1}评估结果:\")\n",
    "    # print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
