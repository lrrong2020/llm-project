{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #模型下载\n",
    "# from modelscope import snapshot_download\n",
    "# # model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')\n",
    "# model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # 拆分成 3 份\n",
    "\n",
    "# # 保存拆分后的文件（可选）\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的行数据字数为：5595\n",
      "最长的行数据字数为：7482\n",
      "最长的行数据字数为：5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T12:43:25.314494Z",
     "iopub.status.busy": "2025-05-04T12:43:25.314151Z",
     "iopub.status.idle": "2025-05-04T12:43:25.337068Z",
     "shell.execute_reply": "2025-05-04T12:43:25.336688Z",
     "shell.execute_reply.started": "2025-05-04T12:43:25.314474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # 使用官方PyPI源\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "# from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:01.889345Z",
     "iopub.status.busy": "2025-05-04T18:34:01.889211Z",
     "iopub.status.idle": "2025-05-04T18:34:10.302782Z",
     "shell.execute_reply": "2025-05-04T18:34:10.302300Z",
     "shell.execute_reply.started": "2025-05-04T18:34:01.889332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: swanlab in /usr/local/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: boto3>=1.35.49 in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (69.5.1)\n",
      "Requirement already satisfied: swankit==0.1.7 in /usr/local/lib/python3.11/site-packages (from swanlab) (0.1.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.570.86)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 02:34:09,534] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # 正确的小写导入\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# 基础配置\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #参数量和 gpt2 一样.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # 基础微调结果\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # 基础微调结果\n",
    "sft_model_path = \"./lora_sft\"        # SFT微调结果\n",
    "# merged_model_path = \"./lora_basic_sft\"  # 合并模型\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # 替换为实际的本地模型路径\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # 禁用截断\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # 找到 <Response> 的起始位置\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 增大 LoRA 矩阵秩 -------------------> next time to 8 #降低秩值\n",
    "    lora_alpha=32,  # 调整 alpha 值 ------------------> next time to 32 #保持alpha/r=4的比例\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # 扩展目标模块\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, 增强特征提取能力\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # 注意力子层\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # 前馈网络\n",
    "    ],\n",
    "    lora_dropout=0.05,  # 增加 dropout 防止过拟合 ---------------> next time to 0.2 #增加Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # 确保处于训练模式[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # 允许后续微调时更新头部[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:10.303982Z",
     "iopub.status.busy": "2025-05-04T18:34:10.303609Z",
     "iopub.status.idle": "2025-05-04T18:34:11.164644Z",
     "shell.execute_reply": "2025-05-04T18:34:11.164098Z",
     "shell.execute_reply.started": "2025-05-04T18:34:10.303966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 752 examples [00:00, 40450.88 examples/s]\n",
      "Map: 100%|██████████| 752/752 [00:00<00:00, 22389.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== 基础知识微调部分 ====================\n",
    "# 加载基础知识数据集\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # 确保只有prompt和response列\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# 基础知识数据预处理函数\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt.replace(\"\\n\", \"\")}\n",
    "\n",
    "# 加载基础知识数据集\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # 选择前5个样本\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # 加载前10行数据\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # 加载迷你数据集\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # 加载数据集\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # 选择前5个样本 [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.165392Z",
     "iopub.status.busy": "2025-05-04T18:34:11.165177Z",
     "iopub.status.idle": "2025-05-04T18:34:11.173861Z",
     "shell.execute_reply": "2025-05-04T18:34:11.173344Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.165374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>在“狼人杀”游戏的语境中，“拿得起、作得成”代表什么意思？</Prompt><Response>有可能</Response>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.176941Z",
     "iopub.status.busy": "2025-05-04T18:34:11.176648Z",
     "iopub.status.idle": "2025-05-04T18:34:14.400796Z",
     "shell.execute_reply": "2025-05-04T18:34:14.400299Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.176917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,881,280 || all params: 1,550,595,584 || trainable%: 0.4438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # 基础知识微调训练\n",
    "# print(\"开始基础知识微调训练...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:17.300981Z",
     "iopub.status.busy": "2025-05-04T18:34:17.300701Z",
     "iopub.status.idle": "2025-05-04T19:04:14.513970Z",
     "shell.execute_reply": "2025-05-04T19:04:14.513381Z",
     "shell.execute_reply.started": "2025-05-04T18:34:17.300964Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始基础知识微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 676/676 [00:00<00:00, 1052.94 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 2181.21 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_023420-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_basic_deepseek_r1\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 29:23, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.613800</td>\n",
       "      <td>2.696045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.690800</td>\n",
       "      <td>2.537914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.577800</td>\n",
       "      <td>2.465054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.433700</td>\n",
       "      <td>2.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.512500</td>\n",
       "      <td>2.391418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.396800</td>\n",
       "      <td>2.380171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/total_flos already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "基础知识微调完成!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic_deepseek_r1/tokenizer_config.json',\n",
       " './lora_basic_deepseek_r1/special_tokens_map.json',\n",
       " './lora_basic_deepseek_r1/vocab.json',\n",
       " './lora_basic_deepseek_r1/merges.txt',\n",
       " './lora_basic_deepseek_r1/added_tokens.json',\n",
       " './lora_basic_deepseek_r1/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基础知识微调训练\n",
    "print(\"开始基础知识微调训练...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # 基础知识微调可以使用较少的epoch\n",
    "    per_device_train_batch_size=4,     # -> next time to 4\n",
    "    gradient_accumulation_steps=8,      # -> next time to 8   # 等效批量=32\n",
    "    learning_rate=2e-4,  # 基础知识微调可以使用稍高的学习率\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"基础知识微调完成!\")\n",
    "\n",
    "# 新增保存逻辑\n",
    "# basic_model_path = \"./lora_basic\"  # 指定基础微调保存路径\n",
    "model.save_pretrained(basic_model_path)  # 保存LoRA适配器参数[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1ea24a-2efb-42bc-b010-6d847b0d66f9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:08:14.965411Z",
     "iopub.status.busy": "2025-05-04T20:08:14.965108Z",
     "iopub.status.idle": "2025-05-04T20:08:41.248398Z",
     "shell.execute_reply": "2025-05-04T20:08:41.247869Z",
     "shell.execute_reply.started": "2025-05-04T20:08:14.965395Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Collecting swanlab\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/43/95/52d0c012eb941bb6df31f31a4b3ee27536eb73191dce4adec2ded7d27c23/swanlab-0.5.7-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.35.49 (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/84/d9/1bd6c2a6c3d3bf1d8b0be52c39230bd1e14bb55b7ecc04f42fcb68b27343/boto3-1.38.8-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/b4/66/e5a314d1e868cd35ec5c5d11360387c2a85e8d408f084616337f1a282c61/botocore-1.38.8-py3-none-any.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (69.5.1)\n",
      "Collecting swankit==0.1.7 (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/74/75/8152b1f1a3034c3ba2369133264ddf9850df62b3ba2b75fd7c104d80090f/swankit-0.1.7-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3>=1.35.49->swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/89/64/d2b49620039b82688aeebd510bd62ff4cdcdb86cbf650cc72ae42c5254a3/s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.570.86)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: swankit, botocore, s3transfer, boto3, swanlab\n",
      "Successfully installed boto3-1.38.8 botocore-1.38.8 s3transfer-0.12.0 swankit-0.1.7 swanlab-0.5.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 04:08:40,045] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # 正确的小写导入\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# 基础配置\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #参数量和 gpt2 一样.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # 基础微调结果\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # 基础微调结果\n",
    "sft_model_path = \"./lora_sft\"        # SFT微调结果\n",
    "# merged_model_path = \"./lora_basic_sft\"  # 合并模型\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # 替换为实际的本地模型路径\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # 禁用截断\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # 找到 <Response> 的起始位置\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 增大 LoRA 矩阵秩 -------------------> next time to 8 #降低秩值\n",
    "    lora_alpha=32,  # 调整 alpha 值 ------------------> next time to 32 #保持alpha/r=4的比例\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # 扩展目标模块\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, 增强特征提取能力\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # 注意力子层\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # 前馈网络\n",
    "    ],\n",
    "    lora_dropout=0.05,  # 增加 dropout 防止过拟合 ---------------> next time to 0.2 #增加Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # 确保处于训练模式[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # 允许后续微调时更新头部[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:08:41.249511Z",
     "iopub.status.busy": "2025-05-04T20:08:41.249143Z",
     "iopub.status.idle": "2025-05-04T20:08:50.597941Z",
     "shell.execute_reply": "2025-05-04T20:08:50.597447Z",
     "shell.execute_reply.started": "2025-05-04T20:08:41.249495Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,550,595,584 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 在SFT训练代码前重新初始化模型并加载基础微调结果：\n",
    "# 重新初始化基础模型（重要！）\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # 强制使用 FP16 计算\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # 从基础微调保存路径加载\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#启用梯度检查点\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# 加载第一阶段LoRA参数\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # 验证参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:08:50.598681Z",
     "iopub.status.busy": "2025-05-04T20:08:50.598479Z",
     "iopub.status.idle": "2025-05-04T20:08:52.103197Z",
     "shell.execute_reply": "2025-05-04T20:08:52.102636Z",
     "shell.execute_reply.started": "2025-05-04T20:08:50.598668Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4045 examples [00:00, 13498.54 examples/s]\n",
      "Map: 100%|██████████| 4045/4045 [00:00<00:00, 14501.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== SFT微调部分 ====================\n",
    "# SFT数据预处理函数\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    # full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    full_prompt = f\"<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    # return {\"text\": full_prompt}\n",
    "    return {\"text\": full_prompt.replace(\"\\\\n\", \"\").replace(\"\\n\", \"\")}\n",
    "\n",
    "# 加载SFT数据集\n",
    "# sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh_part_1.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "# sft_dataset = sft_dataset.select(range(2))  # 选择前5个样本\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:08:52.104284Z",
     "iopub.status.busy": "2025-05-04T20:08:52.104081Z",
     "iopub.status.idle": "2025-05-04T20:08:52.109065Z",
     "shell.execute_reply": "2025-05-04T20:08:52.108639Z",
     "shell.execute_reply.started": "2025-05-04T20:08:52.104270Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4044\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # 查看模型支持的最大长度（如512）\n",
    "sft_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# 爆显存, 目前只 eval 1 个 data\n",
    "# 需要将数据集 cut 三半份, 最长的输入序列是 7482 -> 单词训练时间 epoch 1, batch 2 : 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755cf01-7e6b-4c64-b33d-bcb521087c9f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.724794Z",
     "iopub.status.busy": "2025-05-04T18:27:07.724602Z",
     "iopub.status.idle": "2025-05-04T18:27:07.727722Z",
     "shell.execute_reply": "2025-05-04T18:27:07.727259Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.724779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 滑动窗口分块处理函数\n",
    "# def sliding_window_tokenize(examples):\n",
    "#     max_length = 4096  # 模型支持的最大上下文长度\n",
    "#     stride = 2048      # 滑动窗口步长\n",
    "#     response_token = \"<Response>\"\n",
    "#     response_token_id = tokenizer.encode(response_token, add_special_tokens=False)[0]\n",
    "\n",
    "#     # 使用滑动窗口分块\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         stride=stride,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # 处理标签\n",
    "#     labels = tokenized[\"input_ids\"].clone()\n",
    "#     for i in range(labels.shape[0]):\n",
    "#         # 查找response标记位置\n",
    "#         response_positions = (tokenized[\"input_ids\"][i] == response_token_id).nonzero(as_tuple=True)[0]\n",
    "#         if len(response_positions) > 0:\n",
    "#             # 忽略response标记之前的loss计算\n",
    "#             labels[i, :response_positions[0]] = -100\n",
    "#         else:\n",
    "#             # 没有response标记的块完全忽略\n",
    "#             labels[i, :] = -100\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized[\"input_ids\"],\n",
    "#         \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "# # 应用分块处理\n",
    "# sft_tokenized = sft_dataset.map(\n",
    "#     sliding_window_tokenize,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"],\n",
    "#     batch_size=1  # 确保样本独立处理\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec5da23-c1c0-40e8-a6ca-1a0d2db50589",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.728610Z",
     "iopub.status.busy": "2025-05-04T18:27:07.728248Z",
     "iopub.status.idle": "2025-05-04T18:27:07.730611Z",
     "shell.execute_reply": "2025-05-04T18:27:07.730139Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.728588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(sft_tokenized)\n",
    "# print(sft_tokenized.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:08:52.109738Z",
     "iopub.status.busy": "2025-05-04T20:08:52.109549Z",
     "iopub.status.idle": "2025-05-04T20:09:02.851289Z",
     "shell.execute_reply": "2025-05-04T20:09:02.850727Z",
     "shell.execute_reply.started": "2025-05-04T20:08:52.109723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始SFT微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4044/4044 [00:10<00:00, 382.72 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 135.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # 在基础知识训练结束后保存适配器参数\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # 在SFT训练开始前加载对比\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"参数未继承！\"\n",
    "# from torch.cuda.amp import GradScaler\n",
    "# scaler = GradScaler()  # 创建梯度缩放器\n",
    "\n",
    "# SFT微调训练\n",
    "def tokenize_function(examples):  # 新增参数\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # 应为有效数值\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # 禁用截断（需确保所有样本长度 ≤ max_length）\n",
    "        max_length=6000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # 找到 <Response> 的起始位置\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    \n",
    "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "print(\"开始SFT微调训练...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcab0e53-1a91-4e4b-ada0-adb90b940cf4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T20:09:02.853050Z",
     "iopub.status.busy": "2025-05-04T20:09:02.852826Z",
     "iopub.status.idle": "2025-05-05T01:23:19.418734Z",
     "shell.execute_reply": "2025-05-05T01:23:19.418238Z",
     "shell.execute_reply.started": "2025-05-04T20:09:02.853036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Using SwanLab to track your experiments. Please refer to \u001b[33mhttps://docs.swanlab.cn\u001b[0m for more information.\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: (1) Create a SwanLab account.\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: (2) Use an existing SwanLab account.\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: (3) Don't visualize my results.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Enter your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: You chose 'Use an existing swanlab account'\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Logging into \u001b[33mhttps://swanlab.cn\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: You can find your API key at: \u001b[33mhttps://swanlab.cn/space/~/settings\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Paste an API key from your profile and hit enter, or press 'CTRL + C' to quit: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_040912-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_sft\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/t5v6yqb7vdmqtxk921dob\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/t5v6yqb7vdmqtxk921dob\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2022' max='2022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2022/2022 5:13:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.821400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.889400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.852300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.767100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.806800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.928700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.851800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.766900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.886700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.782800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.799300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.661000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.801400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.845100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.866200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.923500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.868600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.777200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.886200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.908800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.852000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.889500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.826400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.775200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.795500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.888400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.803600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.883800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.819600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.793500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.758500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.880500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.821100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.871400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.876800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.956200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.688800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.848800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.845000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.822800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.948800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.863500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.730200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.936200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.743000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.811800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.715500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.744200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.899600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.821800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.860600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.814600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.700500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.796100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.836200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.870200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.770800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.901200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.847200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.882400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.781100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.827900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.912300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.879400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.877700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>3.016900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.817500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.880700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.722400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.700300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.901100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.805000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.840400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.832300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.811700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.892900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.864300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.948500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.904200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.936200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.821700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.781300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.785100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.843800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.876900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.971500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.883200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.850400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.933600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.843600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.882500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.844300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.892700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.771400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.780200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.759200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.782800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.902800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.906900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.930300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.870400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.870100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.859000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.781400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.660400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.968000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.881300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.884900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.823800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.922100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.877900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.862700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.858300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.940600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.774400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.826700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.759000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.795200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.899300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.831000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.797400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.860400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.924100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.966800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.800400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>3.029900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.990700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.839800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.856400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.772800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.788300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.801800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.783600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.865200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.917300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.951900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.994300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.860500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.803600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.772700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.794500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.683500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.890700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.885600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.871400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.826200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.794100</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.859300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.806300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.844500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.811200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.938200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.761400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.943900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.877800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.868500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.907200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.801400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.823800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.812000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.887500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.817200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.940900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.850200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.760300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.670200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.878800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>2.742300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.800300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.861500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.882300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.775700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.825300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.776800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.864800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.888400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.893300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>2.976000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.770700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.864200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.777700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.838500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.838800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.983300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.930700</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>2.685600</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.887000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.861200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.878000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>2.843900</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.818400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>2.813000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.722800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.796200</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.840400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>2.790500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.930000</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>2.850400</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.928300</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>2.780800</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.957500</td>\n",
       "      <td>3.128969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1030 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1040 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1050 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1060 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1070 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1080 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1090 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1230 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1240 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1250 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1260 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1270 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1280 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1290 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1300 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1310 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1320 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1330 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1340 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1350 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1360 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1370 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1380 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1390 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1400 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1410 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1420 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1430 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1440 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1450 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1460 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1470 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1480 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1490 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1500 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1510 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1520 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1530 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1540 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1550 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1560 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1570 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1580 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1590 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1600 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1610 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1620 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1630 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1640 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1650 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1660 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1670 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1680 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1690 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1700 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1710 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1720 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1730 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1740 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1750 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1760 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1770 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1780 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1790 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1800 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1810 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1820 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1830 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1840 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1850 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1860 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1870 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1880 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1890 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1900 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1910 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1920 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1930 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1940 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1950 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1960 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1970 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1980 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1990 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2000 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2010 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2020 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key single_value/train_runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key single_value/train_samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key single_value/train_steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key single_value/total_flos already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key single_value/train_loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2022 on key train/global_step already exists, ignored.\n",
      "模型已保存至 ./lora_sft\n"
     ]
    }
   ],
   "source": [
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # 新保存路径\n",
    "    num_train_epochs=1,  # 增加训练轮次\n",
    "    per_device_train_batch_size=1,  # 增大批量大小\n",
    "    gradient_accumulation_steps=2,  # 增加梯度累积步数\n",
    "    # learning_rate=2e-4,  # 调整学习率\n",
    "    learning_rate=5e-5,  # 调整学习率\n",
    "    # fp16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # 禁用评估阶段的混合精度\n",
    "    gradient_checkpointing=True,  # 启用梯度检查点优化显存[4]\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,  # 降低评估步数以适配小数据量\n",
    "    # eval_strategy=\"no\",\n",
    "    # do_eval=False,\n",
    "    # 提高训练速度：省掉了 eval 的计算开销，能让训练更专注于前向/反向传播\n",
    "    # 节省显存和显卡带宽：避免每隔 N 步就加载整个验证集导致 OOM\n",
    "    # 简化流程：只在训练结束后一次性做评估，逻辑更简单\n",
    "        # 无法实时监控过拟合：中途看不到 validation loss，也就难以及时发现模型开始过拟合或学习率需要调整。\n",
    "        # 调参不便：如果想根据中途的评估结果自动调整学习率（如 load_best_model_at_end=True、lr_scheduler 回调），就必须保留 eval。\n",
    "        # 早停 (early stopping) 不可用：没法根据验证指标提前停止训练，可能浪费训练资源。\n",
    "    logging_steps=10,      # 每个训练步骤记录日志\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # 新增TensorBoard日志目录\n",
    "    # report_to=[\"tensorboard\"],  # 启用TensorBoard报告\n",
    "    load_best_model_at_end=True  # 自动加载最佳模型\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# 创建Trainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "# torch.autograd.set_detect_anomaly(True)  # 开启梯度异常检测\n",
    "sft_trainer.train()\n",
    "\n",
    "# 保存最终模型\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"模型已保存至 {sft_model_path}\")\n",
    "model.save_pretrained(sft_model_path)  # 保存LoRA适配器参数[8](@ref)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.status.busy": "2025-05-04T16:09:13.646199Z",
     "iopub.status.idle": "2025-05-04T16:09:13.646396Z",
     "shell.execute_reply": "2025-05-04T16:09:13.646304Z",
     "shell.execute_reply.started": "2025-05-04T16:09:13.646294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # 提取训练日志（包含 loss 或 train_loss 和 step）\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # 统一字段名为 train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # 转换为 DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # 提取评估日志（包含 eval_loss 和 step）\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # 合并训练和评估日志\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # 使用 ffill() 替代 fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5351809-5c02-4f17-90c6-61aec63bc568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T03:18:52.927329Z",
     "iopub.status.busy": "2025-05-05T03:18:52.927008Z",
     "iopub.status.idle": "2025-05-05T03:18:53.113332Z",
     "shell.execute_reply": "2025-05-05T03:18:53.112844Z",
     "shell.execute_reply.started": "2025-05-05T03:18:52.927313Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(sft_model_path)  # 保存LoRA适配器参数[8](@ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# 生成任务评估（BLEU/ROUGE/METEOR）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-05T03:25:35.243033Z",
     "iopub.status.busy": "2025-05-05T03:25:35.242746Z",
     "iopub.status.idle": "2025-05-05T03:25:40.551973Z",
     "shell.execute_reply": "2025-05-05T03:25:40.549730Z",
     "shell.execute_reply.started": "2025-05-05T03:25:35.243018Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # 中文分词支持\n",
    "\n",
    "# 加载原始模型和微调模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, sft_model_path)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载测试数据\n",
    "# df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "df = pd.read_csv(\"game_strategy_and_term.csv\").head(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-05T03:25:40.554958Z",
     "iopub.status.busy": "2025-05-05T03:25:40.554743Z",
     "iopub.status.idle": "2025-05-05T03:30:32.159572Z",
     "shell.execute_reply": "2025-05-05T03:30:32.159113Z",
     "shell.execute_reply.started": "2025-05-05T03:25:40.554942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本1 原始内容 ====================\n",
      "【参考回答】: 在狼人杀这款推理游戏中，猎人作为一个关键角色，其玩法策略极为重要。猎人具有独特的技能——在死亡时可以带走一名玩家，这使得猎人在游戏中具有不可忽视的影响力。本文将详细介绍猎人的玩法技巧，帮助新手玩家更好地掌握这一角色。\n",
      "首先，猎人的起跳时机至关重要。由于猎人的身份较为特殊，有时会有狼人选择悍跳猎人。因此，猎人需要在合适的时机表明自己的身份，以确保好人阵营的优势。在起跳之前，猎人需要充分观察场上的局势，确保自己的身份能够得到大家的认可。\n",
      "其次，猎人的枪法也是玩法中的一大关键。由于猎人在死亡时可以带走一名玩家，但无法得知该玩家的真实身份，因此开枪的时机和选择至关重要。在没有十足把握的情况下，猎人应该谨慎开枪，避免误杀好人，特别是预言家和女巫等重要角色。在前期，猎人可以尽量保持低调，通过观察和推理积累信息，以便在关键时刻做出正确的决策。\n",
      "在游戏进行到中段时，猎人需要注意女巫的动向。当女巫跳出来表态并表明自己救了谁时，猎人应该勇敢地站出来表态，以避免被女巫误杀。这是因为女巫在投毒时，猎人无法触发技能，因此及时表明身份是保护自己的重要手段。\n",
      "此外，猎人在游戏中还需要保持冷静和理智。在面对狼人的挑衅和质疑时，猎人需要坚定自己的立场，用逻辑和证据来回应。同时，猎人也需要与队友保持良好的沟通，共同分析局势，制定合适的策略。\n",
      "总之，猎人在狼人杀游戏中具有重要的作用。通过掌握起跳时机、谨慎开枪、注意女巫动向以及保持冷静理智等技巧，新手玩家可以更好地扮演猎人这一角色，为好人阵营的胜利贡献自己的力量。希望本文的攻略能够帮助你在狼人杀游戏中取得更好的成绩！\n",
      "\n",
      "【基础模型生成】: 狼人杀中，如何扮演好猎人角色并为好人阵营取得胜利？ 在狼人杀游戏中，扮演好猎人角色并为好人阵营取得胜利需要以下几点：\n",
      "1. 了解游戏规则和角色定位：猎人是好人阵营中的一员，需要在游戏过程中保护好人阵营成员的安全，同时也要注意观察和识别狼人阵营的行动。\n",
      "2. 保持警惕和观察：猎人需要时刻保持警惕，观察周围环境和好人阵营成员的行为，及时发现狼人阵营的行动。\n",
      "3. 选择合适的行动：猎人需要根据游戏情况选择合适的行动，如保护好人阵营成员、阻止狼人阵营的行动等。\n",
      "4. 与好人阵营成员合作：猎人需要与好人阵营成员合作，共同保护好人阵营成员的安全，同时也要注意保护自己。\n",
      "5. 注意狼人阵营的行动：猎人需要时刻注意狼人阵营的行动，及时发现狼人阵营的行动并采取相应的措施。\n",
      "6. 保持冷静和理智：猎人需要保持冷静和理智，不要被狼人阵营的行动所迷惑，要根据实际情况做出正确的决策。\n",
      "7. 与狼人阵营成员斗争：猎人需要与狼人阵营成员斗争，通过各种手段阻止狼人阵营的行动，保护好人阵营成员的安全。\n",
      "总之，扮演好猎人角色并为好人阵营取得胜利需要猎人具备良好的观察力、判断力和合作精神，同时也要保持冷静和理智，与好人阵营成员合作，与狼人阵营成员斗争。\n",
      "【微调模型生成】: 狼人杀中，如何扮演好猎人角色并为好人阵营取得胜利？ 在狼人杀游戏中，扮演好猎人角色并为好人阵营取得胜利需要以下几点：\n",
      "1. 了解游戏规则和角色定位：猎人是好人阵营中的一员，需要在游戏过程中保护好人阵营成员的安全，同时也要注意观察和识别狼人阵营的行动。\n",
      "2. 保持警惕和观察：猎人需要时刻保持警惕，观察周围环境和好人阵营成员的行为，及时发现狼人阵营的行动。\n",
      "3. 选择合适的行动：猎人需要根据游戏情况选择合适的行动，如保护好人阵营成员、阻止狼人阵营的行动等。\n",
      "4. 与好人阵营成员合作：猎人需要与好人阵营成员合作，共同保护好人阵营成员的安全，同时也要注意保护自己。\n",
      "5. 注意狼人阵营的行动：猎人需要时刻注意狼人阵营的行动，及时发现狼人阵营的行动并采取相应的措施。\n",
      "6. 保持冷静和理智：猎人需要保持冷静和理智，不要被狼人阵营的行动所迷惑，要根据实际情况做出正确的决策。\n",
      "7. 与狼人阵营成员斗争：猎人需要与狼人阵营成员斗争，通过各种手段阻止狼人阵营的行动，保护好人阵营成员的安全。\n",
      "总之，扮演好猎人角色并为好人阵营取得胜利需要猎人具备良好的观察力、判断力和合作精神，同时也要保持冷静和理智，与好人阵营成员合作，与狼人阵营成员斗争。\n",
      "\n",
      "==================== 样本1 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.1779, 微调: 0.1779\n",
      "ROUGE-2 F1 - 原始: 0.0000, 微调: 0.0000\n",
      "ROUGE-L F1 - 原始: 0.0000, 微调: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 6.54, 微调: 6.54\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本2 原始内容 ====================\n",
      "【参考回答】: 狼人杀作为一款心理推理游戏，不仅考验玩家的逻辑能力和洞察力，还涉及到发言的重要性。然而，随着游戏的深入，一些玩家可能会尝试篡改发言，给游戏增添了一些挑战和争议。在面对这种情况时，我们应该如何应对呢？下面就让我们来探讨一下。\n",
      "首先，我们要认识到篡改发言在狼人杀游戏中并不罕见。由于每个人对他人发言的理解有所偏差，篡改发言往往是人们根据自己的价值观进行选择性截取或修改后的产物。因此，不能简单地将篡改发言视为狼的行为，而是要理解其背后的动机和原因。\n",
      "其次，发言内容并不是唯一的判断依据。在狼人杀游戏中，玩家可以通过观察卦相、表情、状态等方式来推断其他玩家的身份。因此，即使发言被篡改，仍然有其他方式可以辅助确认玩家的真实身份。我们应该摆脱对发言内容的过度依赖，而是综合考虑多种因素进行判断。\n",
      "针对篡改发言行为，我们可以采取一些策略来有效地应对。首先，要尽量保持冷静，不要被篡改发言所影响，而是要坚持自己的判断和观点。其次，可以通过提出问题或向其他玩家求证来验证发言的真实性，从而避免被误导。此外，如果发现有玩家频繁篡改发言，可以在游戏规则中加入相应的限制或惩罚，以维护游戏的公平性和趣味性。\n",
      "最后，我们要记住，狼人杀是一款旨在娱乐和交流的游戏，而不是严肃的逻辑竞技。在游戏中，我们应该注重的是玩家之间的互动和合作，而不是纠缠于发言的细节和篡改行为。只有在轻松愉快的氛围中，才能真正体验到狼人杀游戏的乐趣。\n",
      "综上所述，面对狼人杀中的发言篡改行为，我们应该以宽容和理解的态度去应对，同时采取相应的策略来应对挑战。重要的是要保持冷静，不受干扰，从而更好地享受游戏带来的乐趣。\n",
      "\n",
      "【基础模型生成】: 狼人杀游戏中如何应对玩家篡改发言的行为？ 在狼人杀游戏中，玩家篡改发言的行为可能会导致游戏的不公平和混乱。以下是一些应对玩家篡改发言的行为的策略：\n",
      "\n",
      "1. **明确规则**：在游戏开始前，确保所有玩家都清楚了解游戏规则和发言规则。包括发言的优先级、发言的顺序以及发言的限制等。\n",
      "\n",
      "2. **监督机制**：建立一个监督机制，比如设置一个发言记录或使用聊天室中的时间戳功能，以确保发言的准确性和完整性。\n",
      "\n",
      "3. **惩罚措施**：一旦发现玩家篡改发言，立即采取相应的惩罚措施。这可以是暂时禁言、扣除积分或者直接从游戏中移除。\n",
      "\n",
      "4. **教育引导**：通过教育和引导的方式，让玩家了解篡改发言的后果，从而减少这种行为的发生。\n",
      "\n",
      "5. **技术手段**：利用软件或工具来监控发言，例如使用聊天室中的时间戳功能或者安装防篡改插件。\n",
      "\n",
      "6. **团队合作**：鼓励玩家之间互相监督和提醒，形成一个共同维护游戏秩序的团队。\n",
      "\n",
      "7. **透明沟通**：保持与玩家的透明沟通，让他们知道如何正确发言，以及如何举报违规行为。\n",
      "\n",
      "8. **定期审查**：定期审查游戏规则和发言规则，确保它们符合游戏的公平性和可玩性。\n",
      "\n",
      "通过这些策略，可以有效地应对玩家篡改发言的行为，维护游戏的公平性和秩序。\n",
      "【微调模型生成】: 狼人杀游戏中如何应对玩家篡改发言的行为？ 在狼人杀游戏中，玩家篡改发言的行为可能会导致游戏的不公平和混乱。以下是一些应对玩家篡改发言的行为的策略：\n",
      "\n",
      "1. **明确规则**：在游戏开始前，确保所有玩家都清楚了解游戏规则和发言规则。包括发言的优先级、发言的顺序以及发言的限制等。\n",
      "\n",
      "2. **监督机制**：建立一个监督机制，比如设置一个发言记录或使用聊天室中的时间戳功能，以确保发言的准确性和完整性。\n",
      "\n",
      "3. **惩罚措施**：一旦发现玩家篡改发言，立即采取相应的惩罚措施。这可以是暂时禁言、扣除积分或者直接从游戏中移除。\n",
      "\n",
      "4. **教育引导**：通过教育和引导的方式，让玩家了解篡改发言的后果，从而减少这种行为的发生。\n",
      "\n",
      "5. **技术手段**：利用软件或工具来监控发言，例如使用聊天室中的时间戳功能或者安装防篡改插件。\n",
      "\n",
      "6. **团队合作**：鼓励玩家之间互相监督和提醒，形成一个共同维护游戏秩序的团队。\n",
      "\n",
      "7. **透明沟通**：保持与玩家的透明沟通，让他们知道如何正确发言，以及如何举报违规行为。\n",
      "\n",
      "8. **定期审查**：定期审查游戏规则和发言规则，确保它们符合游戏的公平性和可玩性。\n",
      "\n",
      "通过这些策略，可以有效地应对玩家篡改发言的行为，维护游戏的公平性和秩序。\n",
      "\n",
      "==================== 样本2 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.2086, 微调: 0.2086\n",
      "ROUGE-2 F1 - 原始: 0.0000, 微调: 0.0000\n",
      "ROUGE-L F1 - 原始: 0.0000, 微调: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 6.07, 微调: 6.07\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 关闭jieba调试日志\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# 中文分词工具（改用搜索引擎模式提升召回率）\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # 使用搜索引擎模式[[2]]\n",
    "\n",
    "# # 生成预测文本（增加生成长度）\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # 增加生成长度[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 生成预测文本（增加生成长度）\n",
    "def generate_response(model, prompt):\n",
    "    input_text = f\"{prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)  # 增加生成长度\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 显式设置pad_token_id（避免警告）\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 计算BLEU/ROUGE（增加平滑函数和ROUGE-L）\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    计算模型对给定prompt和response的困惑度\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # 拼接输入与响应\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 构造标签：仅计算response部分的loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # 忽略prompt部分的loss计算\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # 返回困惑度\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    # instruction = example[\"instruction\"]#######################################for sft prompt\n",
    "    # print(f\"指令: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"输入: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"参考答案: {reference}\")\n",
    "    \n",
    "    # 生成预测\n",
    "    # base_output = generate_response(base_model, instruction, prompt)#######################################for sft prompt\n",
    "    # ft_output = generate_response(finetuned_model, instruction, prompt)#######################################for sft prompt\n",
    "    base_output = generate_response(base_model, prompt)\n",
    "    ft_output = generate_response(finetuned_model, prompt)\n",
    "    # 新增：计算困惑度\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # 分词处理\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEU优化：使用BLEU-2+平滑函数\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # 使用BLEU-2\n",
    "        smoothing_function=smoother.method1  # 添加平滑\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGE优化：使用分词后的文本并增加ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # 新增打印内容：对比生成结果与参考答案\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 原始内容 {'='*20}\")\n",
    "    # print(f\"【指令】: {instruction}\")\n",
    "    # print(f\"【输入】: {prompt}\")\n",
    "    print(f\"【参考回答】: {reference}\")\n",
    "    print(f\"\\n【基础模型生成】: {base_output}\")\n",
    "    print(f\"【微调模型生成】: {ft_output}\")\n",
    "    \n",
    "    # 评估指标打印（保持原有格式）\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 评估结果 {'='*20}\")\n",
    "    print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # 打印困惑度结果\n",
    "    print(f\"Perplexity - 原始: {base_ppl:.2f}, 微调: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"样本{i+1}评估结果:\")\n",
    "    # print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
