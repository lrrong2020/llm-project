Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 11317.63 examples/s]Generating train split: 12134 examples [00:01, 11583.51 examples/s]Generating train split: 12134 examples [00:01, 10232.23 examples/s]
Map:   0%|          | 0/12134 [00:00<?, ? examples/s]Map:  16%|█▋        | 2001/12134 [00:00<00:00, 18855.31 examples/s]Map:  33%|███▎      | 4000/12134 [00:00<00:00, 18481.94 examples/s]Map:  49%|████▉     | 6000/12134 [00:00<00:00, 17766.27 examples/s]Map:  66%|██████▌   | 8000/12134 [00:00<00:00, 16705.54 examples/s]Map:  82%|████████▏ | 10000/12134 [00:00<00:00, 17323.47 examples/s]Map:  99%|█████████▉| 12007/12134 [00:00<00:00, 18143.29 examples/s]Map: 100%|██████████| 12134/12134 [00:00<00:00, 13160.01 examples/s]
/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map:   0%|          | 0/10920 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10920 [00:09<01:32, 106.98 examples/s]Map:  18%|█▊        | 2000/10920 [00:13<00:58, 153.60 examples/s]Map:  27%|██▋       | 3000/10920 [00:17<00:42, 185.88 examples/s]Map:  37%|███▋      | 4000/10920 [00:21<00:32, 213.87 examples/s]Map:  46%|████▌     | 5000/10920 [00:25<00:25, 233.54 examples/s]Map:  55%|█████▍    | 6000/10920 [00:28<00:19, 249.99 examples/s]Map:  64%|██████▍   | 7000/10920 [00:32<00:15, 260.00 examples/s]Map:  73%|███████▎  | 8000/10920 [00:35<00:10, 266.29 examples/s]Map:  82%|████████▏ | 9000/10920 [00:39<00:07, 272.49 examples/s]Map:  92%|█████████▏| 10000/10920 [00:42<00:03, 278.20 examples/s]Map: 100%|██████████| 10920/10920 [00:45<00:00, 282.31 examples/s]Map: 100%|██████████| 10920/10920 [00:45<00:00, 238.29 examples/s]
Map:   0%|          | 0/1214 [00:00<?, ? examples/s]Map:  82%|████████▏ | 1000/1214 [00:08<00:01, 118.81 examples/s]Map: 100%|██████████| 1214/1214 [00:09<00:00, 133.50 examples/s]Map: 100%|██████████| 1214/1214 [00:09<00:00, 127.36 examples/s]
  0%|          | 0/4095 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  File "/home/rliubk/llm-project/werewolf_game_reasoning/train_werewolf.py", line 357, in <module>
    main()
  File "/home/rliubk/llm-project/werewolf_game_reasoning/train_werewolf.py", line 353, in main
    train_sft(args.model_dir, basic_dir, sft_dir, data_dir, args)
  File "/home/rliubk/llm-project/werewolf_game_reasoning/train_werewolf.py", line 288, in train_sft
    trainer.train()
  File "/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/transformers/trainer.py", line 2181, in _inner_training_loop
    self.optimizer.step()
  File "/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/accelerate/optimizer.py", line 132, in step
    self.scaler.step(self.optimizer, closure)
  File "/home/rliubk/.conda/envs/werewolf/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 449, in step
    len(optimizer_state["found_inf_per_device"]) > 0
No inf checks were recorded for this optimizer.
  0%|          | 0/4095 [00:16<?, ?it/s]
