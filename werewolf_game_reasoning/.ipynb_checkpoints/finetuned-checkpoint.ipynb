{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #模型下载\n",
    "# from modelscope import snapshot_download\n",
    "# # model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')\n",
    "# model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # 拆分成 3 份\n",
    "\n",
    "# # 保存拆分后的文件（可选）\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的行数据字数为：5595\n",
      "最长的行数据字数为：7482\n",
      "最长的行数据字数为：5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # 将当前行的所有字段拼接为字符串\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"最长的行数据字数为：{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T12:43:25.314494Z",
     "iopub.status.busy": "2025-05-04T12:43:25.314151Z",
     "iopub.status.idle": "2025-05-04T12:43:25.337068Z",
     "shell.execute_reply": "2025-05-04T12:43:25.336688Z",
     "shell.execute_reply.started": "2025-05-04T12:43:25.314474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # 使用官方PyPI源\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "# from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:01.889345Z",
     "iopub.status.busy": "2025-05-04T18:34:01.889211Z",
     "iopub.status.idle": "2025-05-04T18:34:10.302782Z",
     "shell.execute_reply": "2025-05-04T18:34:10.302300Z",
     "shell.execute_reply.started": "2025-05-04T18:34:01.889332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: swanlab in /usr/local/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: boto3>=1.35.49 in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (69.5.1)\n",
      "Requirement already satisfied: swankit==0.1.7 in /usr/local/lib/python3.11/site-packages (from swanlab) (0.1.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.570.86)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 02:34:09,534] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # 正确的小写导入\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# 基础配置\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #参数量和 gpt2 一样.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # 基础微调结果\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # 基础微调结果\n",
    "sft_model_path = \"./lora_sft\"        # SFT微调结果\n",
    "# merged_model_path = \"./lora_basic_sft\"  # 合并模型\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # 替换为实际的本地模型路径\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # 禁用截断\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # 找到 <Response> 的起始位置\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 增大 LoRA 矩阵秩 -------------------> next time to 8 #降低秩值\n",
    "    lora_alpha=32,  # 调整 alpha 值 ------------------> next time to 32 #保持alpha/r=4的比例\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # 扩展目标模块\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, 增强特征提取能力\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # 注意力子层\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # 前馈网络\n",
    "    ],\n",
    "    lora_dropout=0.05,  # 增加 dropout 防止过拟合 ---------------> next time to 0.2 #增加Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # 确保处于训练模式[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # 允许后续微调时更新头部[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:10.303982Z",
     "iopub.status.busy": "2025-05-04T18:34:10.303609Z",
     "iopub.status.idle": "2025-05-04T18:34:11.164644Z",
     "shell.execute_reply": "2025-05-04T18:34:11.164098Z",
     "shell.execute_reply.started": "2025-05-04T18:34:10.303966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 752 examples [00:00, 40450.88 examples/s]\n",
      "Map: 100%|██████████| 752/752 [00:00<00:00, 22389.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== 基础知识微调部分 ====================\n",
    "# 加载基础知识数据集\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # 确保只有prompt和response列\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# 基础知识数据预处理函数\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt.replace(\"\\n\", \"\")}\n",
    "\n",
    "# 加载基础知识数据集\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # 选择前5个样本\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # 加载前10行数据\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # 加载迷你数据集\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # 加载数据集\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # 选择前5个样本 [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.165392Z",
     "iopub.status.busy": "2025-05-04T18:34:11.165177Z",
     "iopub.status.idle": "2025-05-04T18:34:11.173861Z",
     "shell.execute_reply": "2025-05-04T18:34:11.173344Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.165374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>在“狼人杀”游戏的语境中，“拿得起、作得成”代表什么意思？</Prompt><Response>有可能</Response>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.176941Z",
     "iopub.status.busy": "2025-05-04T18:34:11.176648Z",
     "iopub.status.idle": "2025-05-04T18:34:14.400796Z",
     "shell.execute_reply": "2025-05-04T18:34:14.400299Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.176917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,881,280 || all params: 1,550,595,584 || trainable%: 0.4438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # 基础知识微调训练\n",
    "# print(\"开始基础知识微调训练...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:17.300981Z",
     "iopub.status.busy": "2025-05-04T18:34:17.300701Z",
     "iopub.status.idle": "2025-05-04T19:04:14.513970Z",
     "shell.execute_reply": "2025-05-04T19:04:14.513381Z",
     "shell.execute_reply.started": "2025-05-04T18:34:17.300964Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始基础知识微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 676/676 [00:00<00:00, 1052.94 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 2181.21 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_023420-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_basic_deepseek_r1\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 29:23, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.613800</td>\n",
       "      <td>2.696045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.690800</td>\n",
       "      <td>2.537914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.577800</td>\n",
       "      <td>2.465054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.433700</td>\n",
       "      <td>2.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.512500</td>\n",
       "      <td>2.391418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.396800</td>\n",
       "      <td>2.380171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/total_flos already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "基础知识微调完成!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic_deepseek_r1/tokenizer_config.json',\n",
       " './lora_basic_deepseek_r1/special_tokens_map.json',\n",
       " './lora_basic_deepseek_r1/vocab.json',\n",
       " './lora_basic_deepseek_r1/merges.txt',\n",
       " './lora_basic_deepseek_r1/added_tokens.json',\n",
       " './lora_basic_deepseek_r1/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基础知识微调训练\n",
    "print(\"开始基础知识微调训练...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # 基础知识微调可以使用较少的epoch\n",
    "    per_device_train_batch_size=4,     # -> next time to 4\n",
    "    gradient_accumulation_steps=8,      # -> next time to 8   # 等效批量=32\n",
    "    learning_rate=2e-4,  # 基础知识微调可以使用稍高的学习率\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"基础知识微调完成!\")\n",
    "\n",
    "# 新增保存逻辑\n",
    "# basic_model_path = \"./lora_basic\"  # 指定基础微调保存路径\n",
    "model.save_pretrained(basic_model_path)  # 保存LoRA适配器参数[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1ea24a-2efb-42bc-b010-6d847b0d66f9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:08.347908Z",
     "iopub.status.busy": "2025-05-04T19:34:08.347782Z",
     "iopub.status.idle": "2025-05-04T19:34:13.328618Z",
     "shell.execute_reply": "2025-05-04T19:34:13.328075Z",
     "shell.execute_reply.started": "2025-05-04T19:34:08.347895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 03:34:12,548] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # 正确的小写导入\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# 基础配置\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #参数量和 gpt2 一样.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # 基础微调结果\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # 基础微调结果\n",
    "sft_model_path = \"./lora_sft\"        # SFT微调结果\n",
    "# merged_model_path = \"./lora_basic_sft\"  # 合并模型\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # 替换为实际的本地模型路径\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # 禁用截断\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # 找到 <Response> 的起始位置\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 增大 LoRA 矩阵秩 -------------------> next time to 8 #降低秩值\n",
    "    lora_alpha=32,  # 调整 alpha 值 ------------------> next time to 32 #保持alpha/r=4的比例\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # 扩展目标模块\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, 增强特征提取能力\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # 注意力子层\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # 前馈网络\n",
    "    ],\n",
    "    lora_dropout=0.05,  # 增加 dropout 防止过拟合 ---------------> next time to 0.2 #增加Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # 确保处于训练模式[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # 允许后续微调时更新头部[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:13.329921Z",
     "iopub.status.busy": "2025-05-04T19:34:13.329504Z",
     "iopub.status.idle": "2025-05-04T19:34:16.622681Z",
     "shell.execute_reply": "2025-05-04T19:34:16.622194Z",
     "shell.execute_reply.started": "2025-05-04T19:34:13.329900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,550,595,584 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 在SFT训练代码前重新初始化模型并加载基础微调结果：\n",
    "# 重新初始化基础模型（重要！）\n",
    "# 加载模型和分词器\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # 强制使用 FP16 计算\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # 从基础微调保存路径加载\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#启用梯度检查点\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# 加载第一阶段LoRA参数\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # 验证参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:16.623576Z",
     "iopub.status.busy": "2025-05-04T19:34:16.623232Z",
     "iopub.status.idle": "2025-05-04T19:34:17.435327Z",
     "shell.execute_reply": "2025-05-04T19:34:17.434906Z",
     "shell.execute_reply.started": "2025-05-04T19:34:16.623562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== SFT微调部分 ====================\n",
    "# SFT数据预处理函数\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    # full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    full_prompt = f\"<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    # return {\"text\": full_prompt}\n",
    "    return {\"text\": full_prompt.replace(\"\\\\n\", \"\").replace(\"\\n\", \"\")}\n",
    "\n",
    "# 加载SFT数据集\n",
    "# sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh_part_1.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "# sft_dataset = sft_dataset.select(range(2))  # 选择前5个样本\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:17.437437Z",
     "iopub.status.busy": "2025-05-04T19:34:17.437098Z",
     "iopub.status.idle": "2025-05-04T19:34:17.441825Z",
     "shell.execute_reply": "2025-05-04T19:34:17.441393Z",
     "shell.execute_reply.started": "2025-05-04T19:34:17.437418Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4044\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # 查看模型支持的最大长度（如512）\n",
    "sft_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# 爆显存, 目前只 eval 1 个 data\n",
    "# 需要将数据集 cut 三半份, 最长的输入序列是 7482 -> 单词训练时间 epoch 1, batch 2 : 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755cf01-7e6b-4c64-b33d-bcb521087c9f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.724794Z",
     "iopub.status.busy": "2025-05-04T18:27:07.724602Z",
     "iopub.status.idle": "2025-05-04T18:27:07.727722Z",
     "shell.execute_reply": "2025-05-04T18:27:07.727259Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.724779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 滑动窗口分块处理函数\n",
    "# def sliding_window_tokenize(examples):\n",
    "#     max_length = 4096  # 模型支持的最大上下文长度\n",
    "#     stride = 2048      # 滑动窗口步长\n",
    "#     response_token = \"<Response>\"\n",
    "#     response_token_id = tokenizer.encode(response_token, add_special_tokens=False)[0]\n",
    "\n",
    "#     # 使用滑动窗口分块\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         stride=stride,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # 处理标签\n",
    "#     labels = tokenized[\"input_ids\"].clone()\n",
    "#     for i in range(labels.shape[0]):\n",
    "#         # 查找response标记位置\n",
    "#         response_positions = (tokenized[\"input_ids\"][i] == response_token_id).nonzero(as_tuple=True)[0]\n",
    "#         if len(response_positions) > 0:\n",
    "#             # 忽略response标记之前的loss计算\n",
    "#             labels[i, :response_positions[0]] = -100\n",
    "#         else:\n",
    "#             # 没有response标记的块完全忽略\n",
    "#             labels[i, :] = -100\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized[\"input_ids\"],\n",
    "#         \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "# # 应用分块处理\n",
    "# sft_tokenized = sft_dataset.map(\n",
    "#     sliding_window_tokenize,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"],\n",
    "#     batch_size=1  # 确保样本独立处理\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec5da23-c1c0-40e8-a6ca-1a0d2db50589",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.728610Z",
     "iopub.status.busy": "2025-05-04T18:27:07.728248Z",
     "iopub.status.idle": "2025-05-04T18:27:07.730611Z",
     "shell.execute_reply": "2025-05-04T18:27:07.730139Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.728588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(sft_tokenized)\n",
    "# print(sft_tokenized.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:17.442942Z",
     "iopub.status.busy": "2025-05-04T19:34:17.442703Z",
     "iopub.status.idle": "2025-05-04T19:34:28.428742Z",
     "shell.execute_reply": "2025-05-04T19:34:28.428306Z",
     "shell.execute_reply.started": "2025-05-04T19:34:17.442928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始SFT微调训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4044/4044 [00:10<00:00, 373.80 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # 在基础知识训练结束后保存适配器参数\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # 在SFT训练开始前加载对比\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"参数未继承！\"\n",
    "# from torch.cuda.amp import GradScaler\n",
    "# scaler = GradScaler()  # 创建梯度缩放器\n",
    "\n",
    "# SFT微调训练\n",
    "def tokenize_function(examples):  # 新增参数\n",
    "    # 假设 response 以 \"<Response>\" 开始\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # 应为有效数值\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # 动态填充到批次中最长样本的长度\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # 禁用截断（需确保所有样本长度 ≤ max_length）\n",
    "        max_length=6000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # 找到 <Response> 的起始位置\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # 忽略 instruction + prompt 的 loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    \n",
    "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 直接复制输入作为标签\n",
    "    return tokenized\n",
    "\n",
    "print(\"开始SFT微调训练...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab0e53-1a91-4e4b-ada0-adb90b940cf4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:28.429810Z",
     "iopub.status.busy": "2025-05-04T19:34:28.429501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_033430-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_sft\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/i9xs4y9twq5kjeb0qdrkb\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/i9xs4y9twq5kjeb0qdrkb\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='2022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 226/2022 31:37 < 4:13:32, 0.12 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.858800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.861700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.908700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.684500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.968400</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.848000</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.785300</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.823800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.949700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.963400</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.737300</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.889100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.807100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.818500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.700800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.907600</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.853800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.768100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.801800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.896500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.757600</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.805800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n"
     ]
    }
   ],
   "source": [
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # 新保存路径\n",
    "    num_train_epochs=1,  # 增加训练轮次\n",
    "    per_device_train_batch_size=1,  # 增大批量大小\n",
    "    gradient_accumulation_steps=2,  # 增加梯度累积步数\n",
    "    # learning_rate=2e-4,  # 调整学习率\n",
    "    learning_rate=5e-5,  # 调整学习率\n",
    "    # fp16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # 禁用评估阶段的混合精度\n",
    "    gradient_checkpointing=True,  # 启用梯度检查点优化显存[4]\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,  # 降低评估步数以适配小数据量\n",
    "    # eval_strategy=\"no\",\n",
    "    # do_eval=False,\n",
    "    # 提高训练速度：省掉了 eval 的计算开销，能让训练更专注于前向/反向传播\n",
    "    # 节省显存和显卡带宽：避免每隔 N 步就加载整个验证集导致 OOM\n",
    "    # 简化流程：只在训练结束后一次性做评估，逻辑更简单\n",
    "        # 无法实时监控过拟合：中途看不到 validation loss，也就难以及时发现模型开始过拟合或学习率需要调整。\n",
    "        # 调参不便：如果想根据中途的评估结果自动调整学习率（如 load_best_model_at_end=True、lr_scheduler 回调），就必须保留 eval。\n",
    "        # 早停 (early stopping) 不可用：没法根据验证指标提前停止训练，可能浪费训练资源。\n",
    "    logging_steps=10,      # 每个训练步骤记录日志\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # 新增TensorBoard日志目录\n",
    "    # report_to=[\"tensorboard\"],  # 启用TensorBoard报告\n",
    "    load_best_model_at_end=True  # 自动加载最佳模型\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# 创建Trainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "# torch.autograd.set_detect_anomaly(True)  # 开启梯度异常检测\n",
    "sft_trainer.train()\n",
    "\n",
    "# 保存最终模型\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"模型已保存至 {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.status.busy": "2025-05-04T16:09:13.646199Z",
     "iopub.status.idle": "2025-05-04T16:09:13.646396Z",
     "shell.execute_reply": "2025-05-04T16:09:13.646304Z",
     "shell.execute_reply.started": "2025-05-04T16:09:13.646294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # 提取训练日志（包含 loss 或 train_loss 和 step）\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # 统一字段名为 train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # 转换为 DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # 提取评估日志（包含 eval_loss 和 step）\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # 合并训练和评估日志\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # 使用 ffill() 替代 fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# 生成任务评估（BLEU/ROUGE/METEOR）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T10:23:50.792129Z",
     "iopub.status.busy": "2025-05-04T10:23:50.791929Z",
     "iopub.status.idle": "2025-05-04T10:23:52.045236Z",
     "shell.execute_reply": "2025-05-04T10:23:52.044779Z",
     "shell.execute_reply.started": "2025-05-04T10:23:50.792112Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # 中文分词支持\n",
    "\n",
    "# 加载原始模型和微调模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载测试数据\n",
    "# df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "df = pd.read_csv(\"game_strategy_and_term.csv\").head(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T10:23:52.046094Z",
     "iopub.status.busy": "2025-05-04T10:23:52.045886Z",
     "iopub.status.idle": "2025-05-04T10:28:56.237544Z",
     "shell.execute_reply": "2025-05-04T10:28:56.237052Z",
     "shell.execute_reply.started": "2025-05-04T10:23:52.046081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本1 原始内容 ====================\n",
      "【参考回答】: 在狼人杀这款推理游戏中，猎人作为一个关键角色，其玩法策略极为重要。猎人具有独特的技能——在死亡时可以带走一名玩家，这使得猎人在游戏中具有不可忽视的影响力。本文将详细介绍猎人的玩法技巧，帮助新手玩家更好地掌握这一角色。\n",
      "首先，猎人的起跳时机至关重要。由于猎人的身份较为特殊，有时会有狼人选择悍跳猎人。因此，猎人需要在合适的时机表明自己的身份，以确保好人阵营的优势。在起跳之前，猎人需要充分观察场上的局势，确保自己的身份能够得到大家的认可。\n",
      "其次，猎人的枪法也是玩法中的一大关键。由于猎人在死亡时可以带走一名玩家，但无法得知该玩家的真实身份，因此开枪的时机和选择至关重要。在没有十足把握的情况下，猎人应该谨慎开枪，避免误杀好人，特别是预言家和女巫等重要角色。在前期，猎人可以尽量保持低调，通过观察和推理积累信息，以便在关键时刻做出正确的决策。\n",
      "在游戏进行到中段时，猎人需要注意女巫的动向。当女巫跳出来表态并表明自己救了谁时，猎人应该勇敢地站出来表态，以避免被女巫误杀。这是因为女巫在投毒时，猎人无法触发技能，因此及时表明身份是保护自己的重要手段。\n",
      "此外，猎人在游戏中还需要保持冷静和理智。在面对狼人的挑衅和质疑时，猎人需要坚定自己的立场，用逻辑和证据来回应。同时，猎人也需要与队友保持良好的沟通，共同分析局势，制定合适的策略。\n",
      "总之，猎人在狼人杀游戏中具有重要的作用。通过掌握起跳时机、谨慎开枪、注意女巫动向以及保持冷静理智等技巧，新手玩家可以更好地扮演猎人这一角色，为好人阵营的胜利贡献自己的力量。希望本文的攻略能够帮助你在狼人杀游戏中取得更好的成绩！\n",
      "\n",
      "【基础模型生成】: 狼人杀中，如何扮演好猎人角色并为好人阵营取得胜利？ 在狼人杀游戏中，猎人是一个非常重要的角色，他的主要任务是通过毒杀狼人来帮助好人阵营获胜。然而，猎人并不是一个简单的角色，需要玩家具备一定的策略和技巧。本文将为大家介绍如何扮演好猎人角色并为好人阵营取得胜利。\n",
      "1. 了解猎人的技能和特点\n",
      "猎人是狼人杀游戏中最强大的角色之一，他的技能是毒杀狼人。然而，猎人也有自己的特点，比如他需要在夜晚进行毒杀，而且毒杀的狼人必须是自己观察到的。此外，猎人还需要注意自己的行动，避免被狼人发现并被狼人攻击。\n",
      "2. 选择合适的毒杀目标\n",
      "猎人需要选择合适的毒杀目标，这需要玩家具备一定的观察和分析能力。猎人需要观察狼人的行为和表情，寻找狼人攻击的目标。同时，猎人还需要注意好人阵营的行动，避免毒杀无辜的玩家。\n",
      "3. 保持冷静和谨慎\n",
      "猎人需要保持冷静和谨慎，避免在夜晚过于激动或紧张。猎人需要在夜晚进行毒杀，需要保持冷静和谨慎，避免被狼人发现并被狼人攻击。此外，猎人还需要注意自己的行动，避免被好人阵营发现并被好人阵营攻击。\n",
      "4. 与其他玩家合作\n",
      "猎人需要与其他玩家合作，与其他玩家一起制定策略，共同为好人阵营取得胜利。猎人需要与其他玩家分享自己的观察和分析结果，与其他玩家一起制定毒杀计划。此外，猎人还需要与其他玩家一起合作，共同应对狼人的攻击。\n",
      "5. 注意自己的行动\n",
      "猎人需要注意自己的行动，避免在夜晚过于激动或紧张。猎人需要在夜晚进行毒杀，需要保持冷静和谨慎，避免被狼人发现并被狼人攻击。此外，猎人还需要注意自己的行动，避免被好人阵营发现并被\n",
      "【微调模型生成】: 狼人杀中，如何扮演好猎人角色并为好人阵营取得胜利？ 在狼人杀游戏中，猎人是一个非常重要的角色，他的主要任务是通过毒杀狼人来帮助好人阵营获胜。然而，猎人并不是一个简单的角色，需要玩家具备一定的策略和技巧。本文将为大家介绍如何扮演好猎人角色并为好人阵营取得胜利。\n",
      "1. 了解猎人的技能和特点\n",
      "猎人是狼人杀游戏中最强大的角色之一，他的技能是毒杀狼人。然而，猎人也有自己的特点，比如他需要在夜晚进行毒杀，而且毒杀的狼人必须是自己观察到的。此外，猎人还需要注意自己的行动，避免被狼人发现并被狼人攻击。\n",
      "2. 选择合适的毒杀目标\n",
      "猎人需要选择合适的毒杀目标，这需要玩家具备一定的观察和分析能力。猎人需要观察狼人的行为和表情，寻找狼人攻击的目标。同时，猎人还需要注意好人阵营的行动，避免毒杀无辜的玩家。\n",
      "3. 保持冷静和谨慎\n",
      "猎人需要保持冷静和谨慎，避免在夜晚过于激动或紧张。猎人需要在夜晚进行毒杀，需要保持冷静和谨慎，避免被狼人发现并被狼人攻击。此外，猎人还需要注意自己的行动，避免被好人阵营发现并被好人阵营攻击。\n",
      "4. 与其他玩家合作\n",
      "猎人需要与其他玩家合作，与其他玩家一起制定策略，共同为好人阵营取得胜利。猎人需要与其他玩家分享自己的观察和分析结果，与其他玩家一起制定毒杀计划。此外，猎人还需要与其他玩家一起合作，共同应对狼人的攻击。\n",
      "5. 注意自己的行动\n",
      "猎人需要注意自己的行动，避免在夜晚过于激动或紧张。猎人需要在夜晚进行毒杀，需要保持冷静和谨慎，避免被狼人发现并被狼人攻击。此外，猎人还需要注意自己的行动，避免被好人阵营发现并被\n",
      "\n",
      "==================== 样本1 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.2682, 微调: 0.2682\n",
      "ROUGE-2 F1 - 原始: 0.0000, 微调: 0.0000\n",
      "ROUGE-L F1 - 原始: 0.0000, 微调: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 5.27, 微调: 5.27\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 样本2 原始内容 ====================\n",
      "【参考回答】: 狼人杀作为一款心理推理游戏，不仅考验玩家的逻辑能力和洞察力，还涉及到发言的重要性。然而，随着游戏的深入，一些玩家可能会尝试篡改发言，给游戏增添了一些挑战和争议。在面对这种情况时，我们应该如何应对呢？下面就让我们来探讨一下。\n",
      "首先，我们要认识到篡改发言在狼人杀游戏中并不罕见。由于每个人对他人发言的理解有所偏差，篡改发言往往是人们根据自己的价值观进行选择性截取或修改后的产物。因此，不能简单地将篡改发言视为狼的行为，而是要理解其背后的动机和原因。\n",
      "其次，发言内容并不是唯一的判断依据。在狼人杀游戏中，玩家可以通过观察卦相、表情、状态等方式来推断其他玩家的身份。因此，即使发言被篡改，仍然有其他方式可以辅助确认玩家的真实身份。我们应该摆脱对发言内容的过度依赖，而是综合考虑多种因素进行判断。\n",
      "针对篡改发言行为，我们可以采取一些策略来有效地应对。首先，要尽量保持冷静，不要被篡改发言所影响，而是要坚持自己的判断和观点。其次，可以通过提出问题或向其他玩家求证来验证发言的真实性，从而避免被误导。此外，如果发现有玩家频繁篡改发言，可以在游戏规则中加入相应的限制或惩罚，以维护游戏的公平性和趣味性。\n",
      "最后，我们要记住，狼人杀是一款旨在娱乐和交流的游戏，而不是严肃的逻辑竞技。在游戏中，我们应该注重的是玩家之间的互动和合作，而不是纠缠于发言的细节和篡改行为。只有在轻松愉快的氛围中，才能真正体验到狼人杀游戏的乐趣。\n",
      "综上所述，面对狼人杀中的发言篡改行为，我们应该以宽容和理解的态度去应对，同时采取相应的策略来应对挑战。重要的是要保持冷静，不受干扰，从而更好地享受游戏带来的乐趣。\n",
      "\n",
      "【基础模型生成】: 狼人杀游戏中如何应对玩家篡改发言的行为？ 在狼人杀游戏中，玩家篡改发言是一种常见的行为，它可能源于各种原因，如紧张、情绪激动或试图掩盖自己的身份。面对这种情况，作为玩家，应该采取以下策略来应对：\n",
      "\n",
      "1. **保持冷静**：首先，不要被其他玩家的发言所影响，保持自己的冷静。情绪激动可能会导致你做出错误的判断。\n",
      "\n",
      "2. **观察其他玩家的行为**：注意其他玩家的发言和行为，看看他们是否真的有篡改发言的动机。有时候，玩家可能会因为紧张而误报身份。\n",
      "\n",
      "3. **分析发言内容**：仔细分析其他玩家的发言内容，看是否有明显的逻辑漏洞或不一致的地方。如果发现有篡改的迹象，可以考虑进一步调查。\n",
      "\n",
      "4. **使用逻辑推理**：利用逻辑推理来判断发言的真实性。如果发言内容与游戏规则相悖，或者缺乏足够的证据支持，那么这个发言可能是篡改的。\n",
      "\n",
      "5. **尊重其他玩家**：即使你怀疑某个玩家的发言，也应该尊重其他玩家的发言，不要轻易地否定他们的发言。尊重其他玩家的发言是游戏的正常秩序。\n",
      "\n",
      "6. **寻求团队支持**：如果怀疑某个玩家的发言，可以向团队成员寻求帮助，共同分析和判断。团队的力量往往比个人更强大。\n",
      "\n",
      "7. **避免直接冲突**：尽量避免直接冲突，而是通过间接的方式表达你的观点。这样可以避免引起不必要的争议，同时也能更好地保护其他玩家的发言权。\n",
      "\n",
      "8. **学习和适应**：在游戏中，玩家会逐渐学会如何应对篡改发言的行为。通过不断的实践和学习，你可以提高自己的判断力和应对能力。\n",
      "\n",
      "总之，面对玩家篡改发言的行为，关键是要保持冷静，通过观察、分析和逻辑推理来判断发言的真实性。同时，尊重其他玩家的发言，避免直接冲突，是应对这种行为的基本策略。\n",
      "【微调模型生成】: 狼人杀游戏中如何应对玩家篡改发言的行为？ 在狼人杀游戏中，玩家篡改发言是一种常见的行为，它可能源于各种原因，如紧张、情绪激动或试图掩盖自己的身份。面对这种情况，作为玩家，应该采取以下策略来应对：\n",
      "\n",
      "1. **保持冷静**：首先，不要被其他玩家的发言所影响，保持自己的冷静。情绪激动可能会导致你做出错误的判断。\n",
      "\n",
      "2. **观察其他玩家的行为**：注意其他玩家的发言和行为，看看他们是否真的有篡改发言的动机。有时候，玩家可能会因为紧张而误报身份。\n",
      "\n",
      "3. **分析发言内容**：仔细分析其他玩家的发言内容，看是否有明显的逻辑漏洞或不一致的地方。如果发现有篡改的迹象，可以考虑进一步调查。\n",
      "\n",
      "4. **使用逻辑推理**：利用逻辑推理来判断发言的真实性。如果发言内容与游戏规则相悖，或者缺乏足够的证据支持，那么这个发言可能是篡改的。\n",
      "\n",
      "5. **尊重其他玩家**：即使你怀疑某个玩家的发言，也应该尊重其他玩家的发言，不要轻易地否定他们的发言。尊重其他玩家的发言是游戏的正常秩序。\n",
      "\n",
      "6. **寻求团队支持**：如果怀疑某个玩家的发言，可以向团队成员寻求帮助，共同分析和判断。团队的力量往往比个人更强大。\n",
      "\n",
      "7. **避免直接冲突**：尽量避免直接冲突，而是通过间接的方式表达你的观点。这样可以避免引起不必要的争议，同时也能更好地保护其他玩家的发言权。\n",
      "\n",
      "8. **学习和适应**：在游戏中，玩家会逐渐学会如何应对篡改发言的行为。通过不断的实践和学习，你可以提高自己的判断力和应对能力。\n",
      "\n",
      "总之，面对玩家篡改发言的行为，关键是要保持冷静，通过观察、分析和逻辑推理来判断发言的真实性。同时，尊重其他玩家的发言，避免直接冲突，是应对这种行为的基本策略。\n",
      "\n",
      "==================== 样本2 评估结果 ====================\n",
      "BLEU-2 - 原始: 0.2893, 微调: 0.2893\n",
      "ROUGE-2 F1 - 原始: 0.0000, 微调: 0.0000\n",
      "ROUGE-L F1 - 原始: 0.0000, 微调: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - 原始: 5.59, 微调: 5.59\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 关闭jieba调试日志\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# 中文分词工具（改用搜索引擎模式提升召回率）\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # 使用搜索引擎模式[[2]]\n",
    "\n",
    "# # 生成预测文本（增加生成长度）\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # 增加生成长度[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 生成预测文本（增加生成长度）\n",
    "def generate_response(model, prompt):\n",
    "    input_text = f\"{prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)  # 增加生成长度\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 显式设置pad_token_id（避免警告）\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 计算BLEU/ROUGE（增加平滑函数和ROUGE-L）\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    计算模型对给定prompt和response的困惑度\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # 拼接输入与响应\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 构造标签：仅计算response部分的loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # 忽略prompt部分的loss计算\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # 返回困惑度\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    # instruction = example[\"instruction\"]#######################################for sft prompt\n",
    "    # print(f\"指令: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"输入: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"参考答案: {reference}\")\n",
    "    \n",
    "    # 生成预测\n",
    "    # base_output = generate_response(base_model, instruction, prompt)#######################################for sft prompt\n",
    "    # ft_output = generate_response(finetuned_model, instruction, prompt)#######################################for sft prompt\n",
    "    base_output = generate_response(base_model, prompt)\n",
    "    ft_output = generate_response(finetuned_model, prompt)\n",
    "    # 新增：计算困惑度\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # 分词处理\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEU优化：使用BLEU-2+平滑函数\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # 使用BLEU-2\n",
    "        smoothing_function=smoother.method1  # 添加平滑\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGE优化：使用分词后的文本并增加ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # 新增打印内容：对比生成结果与参考答案\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 原始内容 {'='*20}\")\n",
    "    # print(f\"【指令】: {instruction}\")\n",
    "    # print(f\"【输入】: {prompt}\")\n",
    "    print(f\"【参考回答】: {reference}\")\n",
    "    print(f\"\\n【基础模型生成】: {base_output}\")\n",
    "    print(f\"【微调模型生成】: {ft_output}\")\n",
    "    \n",
    "    # 评估指标打印（保持原有格式）\n",
    "    print(f\"\\n{'='*20} 样本{i+1} 评估结果 {'='*20}\")\n",
    "    print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # 打印困惑度结果\n",
    "    print(f\"Perplexity - 原始: {base_ppl:.2f}, 微调: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"样本{i+1}评估结果:\")\n",
    "    # print(f\"BLEU-2 - 原始: {bleu_base:.4f}, 微调: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - 原始: {rouge_base['rouge2'].fmeasure:.4f}, 微调: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - 原始: {rouge_base['rougeL'].fmeasure:.4f}, 微调: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
