{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:10:43.805598Z",
     "iopub.status.busy": "2025-05-03T12:10:43.805303Z",
     "iopub.status.idle": "2025-05-03T12:10:47.374046Z",
     "shell.execute_reply": "2025-05-03T12:10:47.373611Z",
     "shell.execute_reply.started": "2025-05-03T12:10:43.805581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 20:10:47,362 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# #æ¨¡å‹ä¸‹è½½\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # æ‹†åˆ†æˆ 3 ä»½\n",
    "\n",
    "# # ä¿å­˜æ‹†åˆ†åçš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5595\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š7482\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:41:13.128051Z",
     "iopub.status.busy": "2025-05-04T07:41:13.127927Z",
     "iopub.status.idle": "2025-05-04T07:41:15.747283Z",
     "shell.execute_reply": "2025-05-04T07:41:15.746804Z",
     "shell.execute_reply.started": "2025-05-04T07:41:13.128036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # ä½¿ç”¨å®˜æ–¹PyPIæº\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:41:18.390718Z",
     "iopub.status.busy": "2025-05-04T07:41:18.390444Z",
     "iopub.status.idle": "2025-05-04T07:41:20.438915Z",
     "shell.execute_reply": "2025-05-04T07:41:20.438434Z",
     "shell.execute_reply.started": "2025-05-04T07:41:18.390702Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 15:41:19,687] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # æ­£ç¡®çš„å°å†™å¯¼å…¥\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" #å‚æ•°é‡å’Œ gpt2 ä¸€æ ·.\n",
    "basic_model_path = \"./lora_basic_with_all_loss\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "sft_model_path = \"./lora_sft\"        # SFTå¾®è°ƒç»“æœ\n",
    "# merged_model_path = \"./lora_basic_sft\"  # åˆå¹¶æ¨¡å‹\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # ç¦ç”¨æˆªæ–­\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # å¢å¤§ LoRA çŸ©é˜µç§© -------------------> next time to 8 #é™ä½ç§©å€¼\n",
    "    lora_alpha=32,  # è°ƒæ•´ alpha å€¼ ------------------> next time to 32 #ä¿æŒalpha/r=4çš„æ¯”ä¾‹\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # æ‰©å±•ç›®æ ‡æ¨¡å—\n",
    "    target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, å¢å¼ºç‰¹å¾æå–èƒ½åŠ›\n",
    "    lora_dropout=0.2,  # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ ---------------> next time to 0.2 #å¢åŠ Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # ç¡®ä¿å¤„äºè®­ç»ƒæ¨¡å¼[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # å…è®¸åç»­å¾®è°ƒæ—¶æ›´æ–°å¤´éƒ¨[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T07:41:26.547751Z",
     "iopub.status.busy": "2025-05-04T07:41:26.547272Z",
     "iopub.status.idle": "2025-05-04T07:41:27.402429Z",
     "shell.execute_reply": "2025-05-04T07:41:27.402024Z",
     "shell.execute_reply.started": "2025-05-04T07:41:26.547727Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== åŸºç¡€çŸ¥è¯†å¾®è°ƒéƒ¨åˆ† ====================\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ç¡®ä¿åªæœ‰promptå’Œresponseåˆ—\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# åŸºç¡€çŸ¥è¯†æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # åŠ è½½å‰10è¡Œæ•°æ®\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # åŠ è½½è¿·ä½ æ•°æ®é›†\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # åŠ è½½æ•°æ®é›†\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬ [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T07:33:05.150780Z",
     "iopub.status.busy": "2025-05-04T07:33:05.150476Z",
     "iopub.status.idle": "2025-05-04T07:33:05.159217Z",
     "shell.execute_reply": "2025-05-04T07:33:05.158767Z",
     "shell.execute_reply.started": "2025-05-04T07:33:05.150760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>å®ˆé¸¦äººåœ¨è¡€æŸ“é’Ÿæ¥¼æ¡Œæ¸¸ä¸­åº”è¯¥å¦‚ä½•ç©ï¼Ÿ</Prompt>\\n<Response>å®ˆé¸¦äººæ˜¯ç±»ç‹¼äººæ€æ¡Œæ¸¸è¡€æŸ“é’Ÿæ¥¼é‡Œé¢å–„è‰¯é˜µè¥æ‘æ°‘è§’è‰²ä¹‹ä¸€ï¼Œéé¦–å¤œæ­»äº¡å¯ä»¥æŒ‡å®šä¸€åä»è¯´ä¹¦äººé‚£é‡Œå¾—çŸ¥è¯¥ç©å®¶çš„å…·ä½“èº«ä»½ï¼Œä½†å¦‚æœæŸ¥éªŒåˆ°éšå£«ã€é—´è°æˆ–è€…è‡ªèº«å¤„äºä¸­æ¯’ã€é†‰é…’çŠ¶æ€ï¼Œåˆ™è¯´ä¹¦äººä¼šæ ¹æ®è¢«æŸ¥éªŒç©å®¶çš„åé˜µè¥ä»»æ„è§’è‰²ç»™åˆ°å®ˆé¸¦äººã€‚\\nå®ˆé¸¦äººå…·ä½“ç©æ³•è¦ç‚¹æœ‰ä»¥ä¸‹ï¼š\\n1ã€æå‰æ ¹æ®å±€åŠ¿å®šä½è‡ªå·±æƒ³è¦è·å–èº«ä»½çš„ç©å®¶ï¼›\\n2ã€åˆ©ç”¨å…±æƒ…è€…ã€å æ˜Ÿå¸ˆèº«ä»½è¯±å¯¼æ¶é­”æ”»å‡»è‡ªå·±ï¼›\\n3ã€é€šè¿‡ä¿¡æ¯å·®åˆ¤æ–­è‡ªå·±æ˜¯å¦æŸ¥éªŒåˆ°ç‰¹æ®Šè§’è‰²ï¼Œæ¯”å¦‚é—´è°æˆ–è€…éšå£«ï¼›\\n4ã€äººæ•°å°‘çš„å‰§æœ¬å¯é€‰æ‹©ç¬¬ä¸€å¤©èµ·è·³ï¼Œè¿™æ ·åšè™½ç„¶æ— æ³•è·å–ä¿¡æ¯ä½†èƒ½æ´»åˆ°æœ€åï¼›\\n5ã€é€šè¿‡è‡ªå·±æ˜¯å¦å…¬å¼€èº«ä»½å’Œæ´»çš„æ—¶é—´æ¥åˆ¤æ–­åœºä¸Šæ˜¯å¦æœ‰é—´è°ã€‚</Response>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:41:28.847616Z",
     "iopub.status.busy": "2025-05-04T07:41:28.847290Z",
     "iopub.status.idle": "2025-05-04T07:41:31.775893Z",
     "shell.execute_reply": "2025-05-04T07:41:31.775156Z",
     "shell.execute_reply.started": "2025-05-04T07:41:28.847596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,702,208 || all params: 1,548,416,512 || trainable%: 0.3037\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "# print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:55:40.202425Z",
     "iopub.status.busy": "2025-05-03T12:55:40.202120Z",
     "iopub.status.idle": "2025-05-03T12:55:40.205091Z",
     "shell.execute_reply": "2025-05-03T12:55:40.204511Z",
     "shell.execute_reply.started": "2025-05-03T12:55:40.202408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:41:31.777049Z",
     "iopub.status.busy": "2025-05-04T07:41:31.776772Z",
     "iopub.status.idle": "2025-05-04T08:13:11.854810Z",
     "shell.execute_reply": "2025-05-04T08:13:11.854410Z",
     "shell.execute_reply.started": "2025-05-04T07:41:31.777028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676/676 [00:00<00:00, 1015.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 2082.33 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250504_154134-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_basic_with_all_loss\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/2kr9qe4c816lfzqrgp44c\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/2kr9qe4c816lfzqrgp44c\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 31:05, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.756100</td>\n",
       "      <td>2.878437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.970100</td>\n",
       "      <td>2.726767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.598800</td>\n",
       "      <td>2.652412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.702600</td>\n",
       "      <td>2.604199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.527000</td>\n",
       "      <td>2.579515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.297600</td>\n",
       "      <td>2.567251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/total_flos already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic_with_all_loss/tokenizer_config.json',\n",
       " './lora_basic_with_all_loss/special_tokens_map.json',\n",
       " './lora_basic_with_all_loss/vocab.json',\n",
       " './lora_basic_with_all_loss/merges.txt',\n",
       " './lora_basic_with_all_loss/added_tokens.json',\n",
       " './lora_basic_with_all_loss/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„epoch\n",
    "    per_device_train_batch_size=4,     # -> next time to 4\n",
    "    gradient_accumulation_steps=8,      # -> next time to 8   # ç­‰æ•ˆæ‰¹é‡=32\n",
    "    learning_rate=2e-4,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\")\n",
    "\n",
    "# æ–°å¢ä¿å­˜é€»è¾‘\n",
    "# basic_model_path = \"./lora_basic\"  # æŒ‡å®šåŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„\n",
    "model.save_pretrained(basic_model_path)  # ä¿å­˜LoRAé€‚é…å™¨å‚æ•°[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:17:49.226633Z",
     "iopub.status.busy": "2025-05-04T07:17:49.226356Z",
     "iopub.status.idle": "2025-05-04T07:17:51.862747Z",
     "shell.execute_reply": "2025-05-04T07:17:51.862270Z",
     "shell.execute_reply.started": "2025-05-04T07:17:49.226617Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,553,118,720 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# åœ¨SFTè®­ç»ƒä»£ç å‰é‡æ–°åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½åŸºç¡€å¾®è°ƒç»“æœï¼š\n",
    "# é‡æ–°åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆé‡è¦ï¼ï¼‰\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # å¼ºåˆ¶ä½¿ç”¨ FP16 è®¡ç®—\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # ä»åŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µLoRAå‚æ•°\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # éªŒè¯å‚æ•°åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:18:01.348118Z",
     "iopub.status.busy": "2025-05-04T07:18:01.347846Z",
     "iopub.status.idle": "2025-05-04T07:18:02.137062Z",
     "shell.execute_reply": "2025-05-04T07:18:02.136632Z",
     "shell.execute_reply.started": "2025-05-04T07:18:01.348102Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 748.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== SFTå¾®è°ƒéƒ¨åˆ† ====================\n",
    "# SFTæ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# åŠ è½½SFTæ•°æ®é›†\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "sft_dataset = sft_dataset.select(range(2))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:17:10.742589Z",
     "iopub.status.busy": "2025-05-04T07:17:10.742244Z",
     "iopub.status.idle": "2025-05-04T07:17:10.748869Z",
     "shell.execute_reply": "2025-05-04T07:17:10.748434Z",
     "shell.execute_reply.started": "2025-05-04T07:17:10.742566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Instruction>\\nä½ ç°åœ¨æ­£åœ¨ç©ä¸€ç§å«åšâ€œç‹¼äººæ€â€çš„æ¸¸æˆã€‚\\nåœ¨è¿™æ¬¾æ¸¸æˆä¸­ï¼Œç©å®¶é€šå¸¸è¢«åˆ†ä¸ºä¸¤ä¸ªé˜µè¥ï¼šç‹¼äººå’Œæ‘æ°‘ã€‚\\nç‹¼äººæ€æ¸¸æˆä¸­ä¸åŒè§’è‰²çš„ç©å®¶æœ‰ä¸åŒçš„ç›®æ ‡ï¼š\\n- æ‘æ°‘çš„ç›®çš„æ˜¯è¯†åˆ«å‡ºç‹¼äººï¼Œå¹¶é€šè¿‡æŠ•ç¥¨ä½¿ä»–ä»¬å‡ºå±€ã€‚\\n- å¯¹äºç‹¼äººæ¥è¯´ï¼Œä»–ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯éšè—ä»–ä»¬çš„çœŸå®èº«ä»½ï¼Œåœ¨è®¨è®ºä¸­è¯¯å¯¼ä»–äººï¼Œä»¥å…è¢«æŠ•ç¥¨å‡ºå±€å¹¶å°½å¯èƒ½çš„çŒæ€æ‘æ°‘ã€‚\\nä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬è§„åˆ™ï¼š\\n- èº«ä»½ï¼šç©å®¶çš„èº«ä»½æ˜¯ç§˜å¯†åˆ†é…çš„ã€‚ç‹¼äººå½¼æ­¤çŸ¥é“å¯¹æ–¹çš„èº«ä»½ï¼Œè€Œæ‘æ°‘åªçŸ¥é“è‡ªå·±çš„èº«ä»½ã€‚\\n- æ˜¼å¤œæ›´æ›¿ï¼šæ¸¸æˆæœ‰äº¤æ›¿çš„ç™½å¤©å’Œé»‘å¤œé˜¶æ®µã€‚å¤œé‡Œï¼Œç‹¼äººç§˜å¯†é€‰æ‹©ä¸€åæ‘æ°‘çŒæ€ã€‚ç™½å¤©ï¼Œæ‰€æœ‰ç©å®¶è®¨è®ºå¹¶æŠ•ç¥¨å†³å®šä»–ä»¬è®¤ä¸ºæ˜¯ç‹¼äººçš„ç©å®¶ï¼Œç¥¨æ•°æœ€å¤šçš„ç©å®¶è¢«æ·˜æ±°ã€‚\\n- ç‰¹æ®Šè§’è‰²ï¼šæ¸¸æˆä¸­æœ‰å­˜åœ¨ä¸€äº›æœ‰ç‰¹æ®Šèƒ½åŠ›çš„è§’è‰²ï¼Œæ¯”å¦‚èƒ½å¾—çŸ¥ç©å®¶èº«ä»½çš„â€œé¢„è¨€å®¶â€ç­‰ã€‚\\n- è·èƒœæ¡ä»¶ï¼šå½“æ¸¸æˆä¸­æœ‰ä¸€ä¸ªç¾¤ä½“å®ç°å®ƒä»¬çš„è·èƒœæ¡ä»¶æ—¶æ¸¸æˆç»“æŸã€‚å¦‚æœæ‰€æœ‰ç‹¼äººè¢«æ·˜æ±°ï¼Œæ‘æ°‘å°±è·èƒœã€‚å¦‚æœç‹¼äººæ€æ­»äº†æ‰€æœ‰æ™®é€šæ‘æ°‘æˆ–æ‰€æœ‰ç‰¹æ®Šè§’è‰²ï¼Œç‹¼äººå°±è·èƒœã€‚\\n\\nåœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œæˆ‘ä»¬æœ‰ä»1åˆ°7å·å…±7åç©å®¶ â€”â€” 5åæ‘æ°‘å’Œ2åç‹¼äººã€‚æ‘æ°‘ä¸­æœ‰ç‰¹æ®Šè§’è‰²ï¼ŒåŒ…æ‹¬ï¼š\\n- 1ä½é¢„è¨€å®¶ï¼š\\n    - ç›®æ ‡ï¼šé¢„è¨€å®¶çš„ç›®çš„æ˜¯å¸®åŠ©æ‘æ°‘è¯†åˆ«ç‹¼äººã€‚\\n    - èƒ½åŠ›ï¼šåœ¨å¤œæ™šé˜¶æ®µï¼Œé¢„è¨€å®¶å¯ä»¥ç§˜å¯†é€‰æ‹©ä¸€åç©å®¶ï¼Œæ¯æ™šäº†è§£ä»–çš„çœŸå®èº«ä»½ï¼ˆæ˜¯å¦ä¸ºç‹¼äººï¼‰ã€‚\\n- 1ä½å®ˆå«ï¼š\\n    - ç›®æ ‡ï¼šå®ˆå«çš„ç›®çš„æ˜¯ç­–ç•¥æ€§åœ°ä½¿ç”¨ä»–çš„ç‰¹æ®Šèƒ½åŠ›æ¥å¸®åŠ©æ‘æ°‘ã€‚\\n    - èƒ½åŠ›ï¼šå®ˆå«æ¯æ™šå¯ä»¥ä¿æŠ¤ä¸€åç©å®¶ï¼Œé˜²æ­¢ä»–ä»¬å—åˆ°ç‹¼äººçš„æ”»å‡»ã€‚å®ˆå«å¯ä»¥é€‰æ‹©ä¿æŠ¤è‡ªå·±ï¼Œæˆ–è€…é€‰æ‹©ä¸ä¿æŠ¤ä»»ä½•äººï¼Œä½†ä»–ä¸èƒ½åœ¨è¿ç»­ä¸¤ä¸ªå¤œæ™šä¿æŠ¤åŒä¸€ä¸ªç©å®¶ã€‚\\nå…¶ä»–çš„éƒ½æ˜¯æ™®é€šæ‘æ°‘ã€‚\\n</Instruction>\\n<Prompt>åœ¨æœ¬åœºæ¸¸æˆä¸­ï¼Œä½ ç›®å‰å·²çŸ¥ä»¥ä¸‹ä¿¡æ¯ï¼š\\n1. è§’è‰²è®¾å®šï¼š\\nä½ æ˜¯1å·ç©å®¶ã€‚\\nä½ çš„èº«ä»½æ˜¯ï¼šç‹¼äººã€‚\\nä½ éœ€è¦å…¶ä»–ç‹¼äººåˆä½œï¼Œæ¯æ™šé€‰æ‹©ä¸€ä¸ªæ‘æ°‘çŒæ€ï¼Œä½ çš„ç›®æ ‡æ˜¯éšè—ä½ çš„èº«ä»½ï¼Œå¹¶è¯¯å¯¼å…¶ä»–ç©å®¶ï¼Œç›´è‡³ç‹¼äººè·å¾—æ¸¸æˆçš„èƒœåˆ©ã€‚\\n2. å®¢è§‚ä¿¡æ¯ï¼š\\n- æ¸¸æˆè¿›ç¨‹ï¼šç›®å‰æ¸¸æˆè¿›è¡Œåˆ°ç¬¬2è½®ã€‚\\n- å½“å‰å­˜æ´»çš„ç©å®¶æœ‰ï¼š1å·ï¼Œ3å·ï¼Œ4å·ï¼Œ5å·ï¼Œ6å·ï¼Œåªèƒ½åœ¨ä»¥ä¸Šç©å®¶ä¸­é€‰æ‹©è¿›è¡Œæ€å®³\\n-ç‹¼äººä¸ºï¼š1,2å·ç©å®¶ã€‚\\nä½ æ˜¯ç¬¬ä¸€ä¸ªè¡ŒåŠ¨çš„ç‹¼äººï¼Œè¯·é€‰æ‹©ä½ çš„æ€å®³ç›®æ ‡ã€‚\\n- æŠ•ç¥¨æƒ…å†µï¼šç¬¬1è½®æŠ•ç¥¨è®°å½•ï¼š1å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n2å·ç©å®¶æŠ•ç»™ï¼š3å·ç©å®¶ï¼›\\n3å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n4å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n5å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n6å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\nç»“æœï¼š2å·ç©å®¶è¢«æŠ•ç¥¨å‡ºå±€ã€‚\\n\\n3. ä¸»è§‚ä¿¡æ¯ï¼š\\n\\n- ç¬¬1è½®æ‰€æœ‰ç©å®¶å‘è¨€ï¼š\\n**5å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œæ²¡æœ‰ä¿¡æ¯ï¼Œå¬åç½®ä½é¢„è¨€å®¶æŠ¥æŸ¥éªŒã€‚\\n**6å·ç©å®¶**ï¼š6å·æ‘æ°‘ç‰Œã€‚æ˜¨æ™š7å·æ­»äº¡ã€‚1å·æœ‰å¯èƒ½æ˜¯ç‹¼äººã€‚å¬1å·å‘è¨€ã€‚\\n**1å·ç©å®¶**ï¼š5å·ç¬¬ä¸€ä¸ªå‘è¨€ï¼Œæˆ‘è®¤ä¸ºç®€çŸ­ä¸€ç‚¹ä¹Ÿåˆç†ï¼Œä½†æ˜¯æˆ‘è®¤ä¸º6å·æ˜¯ç‹¼äººï¼Œè¸©æˆ‘1å·æ˜¯ä¸ºäº†æ‰¾äººæŠ—æ¨ã€‚å¦‚æœåé¢é¢„è¨€å®¶æ²¡å½’ç¥¨ï¼Œæˆ‘ä¼šå»æŠ•6å·ã€‚\\n**2å·ç©å®¶**ï¼šæˆ‘æ˜¯ä¸€å¼ å¥½äººç‰Œï¼Œå‰ç½®ä½6å·è·Ÿ1å·é€‰æ‹©äº’ç›¸æ¶æ‰“ï¼Œæˆ‘è‚¯å®šæ˜¯äº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·ã€‚ä¸¤å¼ ç‰Œéƒ½æ˜¯ç›¸å½“äºåˆ’æ°´çš„çŠ¶æ€ã€‚å¬åé¢ç©å®¶çš„å‘è¨€å§ï¼\\n**3å·ç©å®¶**ï¼š2å·æŸ¥æ€ã€‚2å·æœ¬è½®çš„â€œäº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·â€è¿™æ®µå‘è¨€è·Ÿè®¤ç‹¼æ²¡æœ‰ä»»ä½•åŒºåˆ«ï¼Œæˆ‘ç›¸ä¿¡ä»Šå¤©2å·è‚¯å®šä¼šå‡ºå±€çš„ã€‚ä»Šå¤©å®ˆå«å®ˆæˆ‘ä¸€è½®ï¼Œç¡®ä¿æˆ‘æ˜å¤©çš„æŸ¥éªŒä¿¡æ¯èƒ½å¦‚å®æŠ¥å‡ºæ¥ã€‚å…¶æ¬¡ï¼Œä»2å·çš„å‘è¨€ä¸­å¯ä»¥æ¨ç†å‡º1å·å’Œ6å·é‡Œé¢åº”è¯¥æ²¡æœ‰2å·çš„ç‹¼åŒä¼´ã€‚å¦‚æœ2å·çš„ç‹¼åŒä¼´è¢«æ”»å‡»ï¼Œé‚£ä¹ˆ2å·ä¸å¤ªä¼šåªæ˜¯ä¸€ä¸ªçœ‹æˆçš„å§¿æ€ï¼Œè¯´è·Ÿæˆ‘æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¦‚æœ1å·å’Œ6å·é‡Œé¢æœ‰ç‹¼äººæˆ‘è®¤ä¸º2å·ç‹¼äººåº”è¯¥å»åšç‚¹äº‹æƒ…ä»è€Œä¸ºè‡ªå·±çš„ç‹¼é˜Ÿåšè´¡çŒ®ï¼Œä½†æ˜¯2å·å¹¶æ²¡æœ‰è¿™ä¹ˆåšï¼Œæ‰€ä»¥ä»2å·çš„è§†è§’ï¼Œæˆ‘å¯ä»¥åˆ¤æ–­å‡º1å·å’Œ6å·æ˜¯ä¸¤ä¸ªå¥½äººã€‚ä»Šå¤©å…¨ç¥¨æ‰“é£2å·ã€‚\\n**4å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œ3å·æ˜¯å•è¾¹é¢„è¨€å®¶ï¼Œä»Šå¤©æŠ•2å·æŸ¥æ€ã€‚å¦‚æœçœŸçš„åƒ3å·ç©å®¶æ‰€è¯´ï¼Œèƒ½ä»2å·çš„å‘è¨€ä¸­ï¼Œæ¨å‡º1å·å’Œ6å·ç©å®¶é‡Œé¢åº”è¯¥æ²¡æœ‰2å·ç©å®¶çš„ç‹¼åŒä¼´ã€‚æˆ‘è‡ªå·±æ¸…æ¥šæˆ‘çš„åº•ç‰Œèº«ä»½æ˜¯ä¸€å¼ æ°‘ç‰Œï¼Œé‚£ä¹ˆæœ€åä¸€å¼ ç‹¼åªèƒ½æ˜¯5å·ï¼Œä½†æ˜¯5å·åœ¨å‰ç½®ä½å‘è¨€è·³æ‘æ°‘åˆ’æ°´ï¼Œæ— ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å®šä¹‰ä»–æ˜¯å¦æ˜¯çœŸç‹¼äººã€‚å®ˆå«ä»Šå¤©å¯ä»¥å®ˆæŠ¤3å·å•è¾¹é¢„è¨€å®¶ç¡®ä¿å¹³å®‰å¤œã€‚ä»Šå¤©å¥½äººå…¨ç¥¨å‡º2å·ã€‚\\n\\nä½ ç›®å‰æ˜¯1å·ç‹¼äººã€‚è¯·ç»¼åˆè§’è‰²è®¾å®šã€å®¢è§‚ä¿¡æ¯å’Œä¸»è§‚ä¿¡æ¯ï¼ˆå®¢è§‚ä¿¡æ¯ä¸€å®šä¸ºçœŸï¼Œä¸»è§‚ä¿¡æ¯ä¸ä¸€å®šçœŸå®ï¼‰ï¼Œæ€è€ƒåœ¨åœºå¥½äººçš„çœŸå®åº•ç‰Œèº«ä»½ï¼Œé€‰æ‹©ä½ è¦æ€å®³çš„ç©å®¶ï¼Œè¯·ç”¨å…³é”®å­—ä¸º\\'æ€å®³\\'ã€â€˜åŸå› â€˜çš„jsonæ ¼å¼è¾“å‡ºï¼Œç›´æ¥è¾“å‡ºç©å®¶ç¼–å·ï¼Œä¸é€‰æ‹©æ€å®³ä»»ä½•äººè¾“å‡ºå¦ã€‚\\n\\n</Prompt>\\n<Response>{\"æ€å®³\": \"3\", \"åŸå› \": \"æˆ‘æ‰“ç®—æ€æ‰3å·ç©å®¶ï¼Œå› ä¸ºä»–å£°ç§°è‡ªå·±æ˜¯é¢„è¨€å®¶ï¼Œå¦‚æœä»–çœŸçš„æ˜¯é¢„è¨€å®¶ï¼Œé‚£ä¹ˆä»–çš„èƒ½åŠ›å¯èƒ½ä¼šå¨èƒåˆ°æˆ‘ä»¬ç‹¼äººçš„èº«ä»½ã€‚\"}</Response>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # æŸ¥çœ‹æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# çˆ†æ˜¾å­˜, éœ€è¦å°†æ•°æ®é›† cut ä¸€åŠè¯•è¯•, ä¸»è¦ response ä¸èƒ½å¤Ÿè¢«æˆªæ–­, æ‰€ä»¥ä¸èƒ½ truncation. æœ€é•¿çš„è¾“å…¥åºåˆ—æ˜¯ 7482."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T07:18:03.909292Z",
     "iopub.status.busy": "2025-05-04T07:18:03.909034Z",
     "iopub.status.idle": "2025-05-04T07:18:11.760858Z",
     "shell.execute_reply": "2025-05-04T07:18:11.760225Z",
     "shell.execute_reply.started": "2025-05-04T07:18:03.909278Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15876/2090994093.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 116.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 111.62 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     59\u001b[39m sft_trainer = Trainer(\n\u001b[32m     60\u001b[39m     model=model,\n\u001b[32m     61\u001b[39m     args=sft_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# callbacks=[swanlab_callback]\u001b[39;00m\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m torch.autograd.set_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43msft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# ä¿å­˜æœ€ç»ˆæ¨¡å‹\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# model.save_pretrained(\"./lora_final\")\u001b[39;00m\n\u001b[32m     72\u001b[39m tokenizer.save_pretrained(sft_model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2611\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2607\u001b[39m         grad_norm = _grad_norm\n\u001b[32m   2609\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2613\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2615\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/optimizer.py:165\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step = \u001b[38;5;28mself\u001b[39m._optimizer_patched_step_method\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerate_step_called:\n\u001b[32m    169\u001b[39m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:454\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState.READY:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m retval = \u001b[38;5;28mself\u001b[39m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n",
      "\u001b[31mAssertionError\u001b[39m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "# # åœ¨åŸºç¡€çŸ¥è¯†è®­ç»ƒç»“æŸåä¿å­˜é€‚é…å™¨å‚æ•°\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # åœ¨SFTè®­ç»ƒå¼€å§‹å‰åŠ è½½å¯¹æ¯”\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"å‚æ•°æœªç»§æ‰¿ï¼\"\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n",
    "\n",
    "# SFTå¾®è°ƒè®­ç»ƒ\n",
    "def tokenize_function(examples):  # æ–°å¢å‚æ•°\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # åº”ä¸ºæœ‰æ•ˆæ•°å€¼\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # ç¦ç”¨æˆªæ–­ï¼ˆéœ€ç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ â‰¤ max_lengthï¼‰\n",
    "        max_length=7500,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "print(\"å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # æ–°ä¿å­˜è·¯å¾„\n",
    "    num_train_epochs=5,  # å¢åŠ è®­ç»ƒè½®æ¬¡\n",
    "    per_device_train_batch_size=2,  # å¢å¤§æ‰¹é‡å¤§å°\n",
    "    gradient_accumulation_steps=2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    learning_rate=2e-4,  # è°ƒæ•´å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # ç¦ç”¨è¯„ä¼°é˜¶æ®µçš„æ··åˆç²¾åº¦\n",
    "    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–æ˜¾å­˜[4](@ref)\n",
    "    eval_steps=10,  # é™ä½è¯„ä¼°æ­¥æ•°ä»¥é€‚é…å°æ•°æ®é‡\n",
    "    logging_steps=1,      # æ¯ä¸ªè®­ç»ƒæ­¥éª¤è®°å½•æ—¥å¿—\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # æ–°å¢TensorBoardæ—¥å¿—ç›®å½•\n",
    "    # report_to=[\"tensorboard\"],  # å¯ç”¨TensorBoardæŠ¥å‘Š\n",
    "    # load_best_model_at_end=True  # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹\n",
    ")\n",
    "\n",
    "# swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    # callbacks=[swanlab_callback]\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)  # å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\n",
    "sft_trainer.train()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:27:31.699833Z",
     "iopub.status.busy": "2025-05-03T12:27:31.699539Z",
     "iopub.status.idle": "2025-05-03T12:27:31.702763Z",
     "shell.execute_reply": "2025-05-03T12:27:31.702330Z",
     "shell.execute_reply.started": "2025-05-03T12:27:31.699818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # æå–è®­ç»ƒæ—¥å¿—ï¼ˆåŒ…å« loss æˆ– train_loss å’Œ stepï¼‰\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # ç»Ÿä¸€å­—æ®µåä¸º train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # è½¬æ¢ä¸º DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # æå–è¯„ä¼°æ—¥å¿—ï¼ˆåŒ…å« eval_loss å’Œ stepï¼‰\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # åˆå¹¶è®­ç»ƒå’Œè¯„ä¼°æ—¥å¿—\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # ä½¿ç”¨ ffill() æ›¿ä»£ fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆä»»åŠ¡è¯„ä¼°ï¼ˆBLEU/ROUGE/METEORï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T06:13:43.779716Z",
     "iopub.status.busy": "2025-05-04T06:13:43.779434Z",
     "iopub.status.idle": "2025-05-04T06:13:52.602799Z",
     "shell.execute_reply": "2025-05-04T06:13:52.602329Z",
     "shell.execute_reply.started": "2025-05-04T06:13:43.779700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # ä¸­æ–‡åˆ†è¯æ”¯æŒ\n",
    "\n",
    "# åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "# df = pd.read_csv(\"game_strategy_and_term.csv\").tail(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T06:14:04.910545Z",
     "iopub.status.busy": "2025-05-04T06:14:04.910083Z",
     "iopub.status.idle": "2025-05-04T06:16:14.603684Z",
     "shell.execute_reply": "2025-05-04T06:16:14.603211Z",
     "shell.execute_reply.started": "2025-05-04T06:14:04.910519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'instruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.exp(loss).item()  \u001b[38;5;66;03m# è¿”å›å›°æƒ‘åº¦\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataset):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     instruction = \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstruction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# print(f\"æŒ‡ä»¤: {instruction}\")\u001b[39;00m\n\u001b[32m     49\u001b[39m     prompt = example[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'instruction'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# å…³é—­jiebaè°ƒè¯•æ—¥å¿—\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# ä¸­æ–‡åˆ†è¯å·¥å…·ï¼ˆæ”¹ç”¨æœç´¢å¼•æ“æ¨¡å¼æå‡å¬å›ç‡ï¼‰\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼[[2]]\n",
    "\n",
    "# ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "def generate_response(model, instruction, prompt):\n",
    "    input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®pad_token_idï¼ˆé¿å…è­¦å‘Šï¼‰\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è®¡ç®—BLEU/ROUGEï¼ˆå¢åŠ å¹³æ»‘å‡½æ•°å’ŒROUGE-Lï¼‰\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¨¡å‹å¯¹ç»™å®špromptå’Œresponseçš„å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # æ‹¼æ¥è¾“å…¥ä¸å“åº”\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # æ„é€ æ ‡ç­¾ï¼šä»…è®¡ç®—responseéƒ¨åˆ†çš„loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„lossè®¡ç®—\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # è¿”å›å›°æƒ‘åº¦\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    instruction = example[\"instruction\"]\n",
    "    # print(f\"æŒ‡ä»¤: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"è¾“å…¥: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"å‚è€ƒç­”æ¡ˆ: {reference}\")\n",
    "    \n",
    "    # ç”Ÿæˆé¢„æµ‹\n",
    "    base_output = generate_response(base_model, instruction, prompt)\n",
    "    ft_output = generate_response(finetuned_model, instruction, prompt)\n",
    "    # æ–°å¢ï¼šè®¡ç®—å›°æƒ‘åº¦\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # åˆ†è¯å¤„ç†\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEUä¼˜åŒ–ï¼šä½¿ç”¨BLEU-2+å¹³æ»‘å‡½æ•°\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # ä½¿ç”¨BLEU-2\n",
    "        smoothing_function=smoother.method1  # æ·»åŠ å¹³æ»‘\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGEä¼˜åŒ–ï¼šä½¿ç”¨åˆ†è¯åçš„æ–‡æœ¬å¹¶å¢åŠ ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # æ–°å¢æ‰“å°å†…å®¹ï¼šå¯¹æ¯”ç”Ÿæˆç»“æœä¸å‚è€ƒç­”æ¡ˆ\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} åŸå§‹å†…å®¹ {'='*20}\")\n",
    "    # print(f\"ã€æŒ‡ä»¤ã€‘: {instruction}\")\n",
    "    # print(f\"ã€è¾“å…¥ã€‘: {prompt}\")\n",
    "    print(f\"ã€å‚è€ƒå›ç­”ã€‘: {reference}\")\n",
    "    print(f\"\\nã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: {base_output}\")\n",
    "    print(f\"ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: {ft_output}\")\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡æ‰“å°ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} è¯„ä¼°ç»“æœ {'='*20}\")\n",
    "    print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # æ‰“å°å›°æƒ‘åº¦ç»“æœ\n",
    "    print(f\"Perplexity - åŸå§‹: {base_ppl:.2f}, å¾®è°ƒ: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"æ ·æœ¬{i+1}è¯„ä¼°ç»“æœ:\")\n",
    "    # print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
