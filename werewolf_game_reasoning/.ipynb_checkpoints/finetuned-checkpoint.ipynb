{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #æ¨¡å‹ä¸‹è½½\n",
    "# from modelscope import snapshot_download\n",
    "# # model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')\n",
    "# model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # æ‹†åˆ†æˆ 3 ä»½\n",
    "\n",
    "# # ä¿å­˜æ‹†åˆ†åçš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5595\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š7482\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T12:43:25.314494Z",
     "iopub.status.busy": "2025-05-04T12:43:25.314151Z",
     "iopub.status.idle": "2025-05-04T12:43:25.337068Z",
     "shell.execute_reply": "2025-05-04T12:43:25.336688Z",
     "shell.execute_reply.started": "2025-05-04T12:43:25.314474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # ä½¿ç”¨å®˜æ–¹PyPIæº\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "# from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:01.889345Z",
     "iopub.status.busy": "2025-05-04T18:34:01.889211Z",
     "iopub.status.idle": "2025-05-04T18:34:10.302782Z",
     "shell.execute_reply": "2025-05-04T18:34:10.302300Z",
     "shell.execute_reply.started": "2025-05-04T18:34:01.889332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: swanlab in /usr/local/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: boto3>=1.35.49 in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.11/site-packages (from swanlab) (1.38.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (69.5.1)\n",
      "Requirement already satisfied: swankit==0.1.7 in /usr/local/lib/python3.11/site-packages (from swanlab) (0.1.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.570.86)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 02:34:09,534] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # æ­£ç¡®çš„å°å†™å¯¼å…¥\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #å‚æ•°é‡å’Œ gpt2 ä¸€æ ·.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "sft_model_path = \"./lora_sft\"        # SFTå¾®è°ƒç»“æœ\n",
    "# merged_model_path = \"./lora_basic_sft\"  # åˆå¹¶æ¨¡å‹\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # ç¦ç”¨æˆªæ–­\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # å¢å¤§ LoRA çŸ©é˜µç§© -------------------> next time to 8 #é™ä½ç§©å€¼\n",
    "    lora_alpha=32,  # è°ƒæ•´ alpha å€¼ ------------------> next time to 32 #ä¿æŒalpha/r=4çš„æ¯”ä¾‹\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # æ‰©å±•ç›®æ ‡æ¨¡å—\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, å¢å¼ºç‰¹å¾æå–èƒ½åŠ›\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # æ³¨æ„åŠ›å­å±‚\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # å‰é¦ˆç½‘ç»œ\n",
    "    ],\n",
    "    lora_dropout=0.05,  # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ ---------------> next time to 0.2 #å¢åŠ Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # ç¡®ä¿å¤„äºè®­ç»ƒæ¨¡å¼[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # å…è®¸åç»­å¾®è°ƒæ—¶æ›´æ–°å¤´éƒ¨[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:10.303982Z",
     "iopub.status.busy": "2025-05-04T18:34:10.303609Z",
     "iopub.status.idle": "2025-05-04T18:34:11.164644Z",
     "shell.execute_reply": "2025-05-04T18:34:11.164098Z",
     "shell.execute_reply.started": "2025-05-04T18:34:10.303966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 752 examples [00:00, 40450.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 752/752 [00:00<00:00, 22389.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== åŸºç¡€çŸ¥è¯†å¾®è°ƒéƒ¨åˆ† ====================\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ç¡®ä¿åªæœ‰promptå’Œresponseåˆ—\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# åŸºç¡€çŸ¥è¯†æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt.replace(\"\\n\", \"\")}\n",
    "\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # åŠ è½½å‰10è¡Œæ•°æ®\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # åŠ è½½è¿·ä½ æ•°æ®é›†\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # åŠ è½½æ•°æ®é›†\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬ [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.165392Z",
     "iopub.status.busy": "2025-05-04T18:34:11.165177Z",
     "iopub.status.idle": "2025-05-04T18:34:11.173861Z",
     "shell.execute_reply": "2025-05-04T18:34:11.173344Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.165374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œâ€œæ‹¿å¾—èµ·ã€ä½œå¾—æˆâ€ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼Ÿ</Prompt><Response>æœ‰å¯èƒ½</Response>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:11.176941Z",
     "iopub.status.busy": "2025-05-04T18:34:11.176648Z",
     "iopub.status.idle": "2025-05-04T18:34:14.400796Z",
     "shell.execute_reply": "2025-05-04T18:34:14.400299Z",
     "shell.execute_reply.started": "2025-05-04T18:34:11.176917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,881,280 || all params: 1,550,595,584 || trainable%: 0.4438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "# print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:34:17.300981Z",
     "iopub.status.busy": "2025-05-04T18:34:17.300701Z",
     "iopub.status.idle": "2025-05-04T19:04:14.513970Z",
     "shell.execute_reply": "2025-05-04T19:04:14.513381Z",
     "shell.execute_reply.started": "2025-05-04T18:34:17.300964Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676/676 [00:00<00:00, 1052.94 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 2181.21 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_023420-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_basic_deepseek_r1\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/6hk9d9wc8kcxtjomnp5ie\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 29:23, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.613800</td>\n",
       "      <td>2.696045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.690800</td>\n",
       "      <td>2.537914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.577800</td>\n",
       "      <td>2.465054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.433700</td>\n",
       "      <td>2.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.512500</td>\n",
       "      <td>2.391418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.396800</td>\n",
       "      <td>2.380171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 1 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 2 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 3 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 4 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 5 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 6 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 7 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 8 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 9 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 11 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 12 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 13 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 14 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 15 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 16 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 17 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 18 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 19 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 21 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 22 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 23 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 24 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 25 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 26 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 27 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 28 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 29 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 31 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 32 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 33 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 34 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 35 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 36 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 37 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 38 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 39 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 41 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 42 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 43 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 44 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 45 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 46 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 47 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 48 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 49 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 51 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 52 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 53 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 54 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 55 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 56 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 57 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 58 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 59 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 61 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 62 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/total_flos already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key single_value/train_loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 63 on key train/global_step already exists, ignored.\n",
      "åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_basic_deepseek_r1/tokenizer_config.json',\n",
       " './lora_basic_deepseek_r1/special_tokens_map.json',\n",
       " './lora_basic_deepseek_r1/vocab.json',\n",
       " './lora_basic_deepseek_r1/merges.txt',\n",
       " './lora_basic_deepseek_r1/added_tokens.json',\n",
       " './lora_basic_deepseek_r1/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„epoch\n",
    "    per_device_train_batch_size=4,     # -> next time to 4\n",
    "    gradient_accumulation_steps=8,      # -> next time to 8   # ç­‰æ•ˆæ‰¹é‡=32\n",
    "    learning_rate=2e-4,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\")\n",
    "\n",
    "# æ–°å¢ä¿å­˜é€»è¾‘\n",
    "# basic_model_path = \"./lora_basic\"  # æŒ‡å®šåŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„\n",
    "model.save_pretrained(basic_model_path)  # ä¿å­˜LoRAé€‚é…å™¨å‚æ•°[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1ea24a-2efb-42bc-b010-6d847b0d66f9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:08.347908Z",
     "iopub.status.busy": "2025-05-04T19:34:08.347782Z",
     "iopub.status.idle": "2025-05-04T19:34:13.328618Z",
     "shell.execute_reply": "2025-05-04T19:34:13.328075Z",
     "shell.execute_reply.started": "2025-05-04T19:34:08.347895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 03:34:12,548] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # æ­£ç¡®çš„å°å†™å¯¼å…¥\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #å‚æ•°é‡å’Œ gpt2 ä¸€æ ·.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "sft_model_path = \"./lora_sft\"        # SFTå¾®è°ƒç»“æœ\n",
    "# merged_model_path = \"./lora_basic_sft\"  # åˆå¹¶æ¨¡å‹\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # ç¦ç”¨æˆªæ–­\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # å¢å¤§ LoRA çŸ©é˜µç§© -------------------> next time to 8 #é™ä½ç§©å€¼\n",
    "    lora_alpha=32,  # è°ƒæ•´ alpha å€¼ ------------------> next time to 32 #ä¿æŒalpha/r=4çš„æ¯”ä¾‹\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # æ‰©å±•ç›®æ ‡æ¨¡å—\n",
    "    # target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, å¢å¼ºç‰¹å¾æå–èƒ½åŠ›\n",
    "    target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # æ³¨æ„åŠ›å­å±‚\n",
    "    \"mlp.down_proj\", \"mlp.up_proj\"               # å‰é¦ˆç½‘ç»œ\n",
    "    ],\n",
    "    lora_dropout=0.05,  # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ ---------------> next time to 0.2 #å¢åŠ Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # ç¡®ä¿å¤„äºè®­ç»ƒæ¨¡å¼[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # å…è®¸åç»­å¾®è°ƒæ—¶æ›´æ–°å¤´éƒ¨[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:13.329921Z",
     "iopub.status.busy": "2025-05-04T19:34:13.329504Z",
     "iopub.status.idle": "2025-05-04T19:34:16.622681Z",
     "shell.execute_reply": "2025-05-04T19:34:16.622194Z",
     "shell.execute_reply.started": "2025-05-04T19:34:13.329900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,550,595,584 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# åœ¨SFTè®­ç»ƒä»£ç å‰é‡æ–°åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½åŸºç¡€å¾®è°ƒç»“æœï¼š\n",
    "# é‡æ–°åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆé‡è¦ï¼ï¼‰\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # å¼ºåˆ¶ä½¿ç”¨ FP16 è®¡ç®—\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # ä»åŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µLoRAå‚æ•°\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # éªŒè¯å‚æ•°åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:16.623576Z",
     "iopub.status.busy": "2025-05-04T19:34:16.623232Z",
     "iopub.status.idle": "2025-05-04T19:34:17.435327Z",
     "shell.execute_reply": "2025-05-04T19:34:17.434906Z",
     "shell.execute_reply.started": "2025-05-04T19:34:16.623562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== SFTå¾®è°ƒéƒ¨åˆ† ====================\n",
    "# SFTæ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    # full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    full_prompt = f\"<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    # return {\"text\": full_prompt}\n",
    "    return {\"text\": full_prompt.replace(\"\\\\n\", \"\").replace(\"\\n\", \"\")}\n",
    "\n",
    "# åŠ è½½SFTæ•°æ®é›†\n",
    "# sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh_part_1.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "# sft_dataset = sft_dataset.select(range(2))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:17.437437Z",
     "iopub.status.busy": "2025-05-04T19:34:17.437098Z",
     "iopub.status.idle": "2025-05-04T19:34:17.441825Z",
     "shell.execute_reply": "2025-05-04T19:34:17.441393Z",
     "shell.execute_reply.started": "2025-05-04T19:34:17.437418Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4044\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # æŸ¥çœ‹æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰\n",
    "sft_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# çˆ†æ˜¾å­˜, ç›®å‰åª eval 1 ä¸ª data\n",
    "# éœ€è¦å°†æ•°æ®é›† cut ä¸‰åŠä»½, æœ€é•¿çš„è¾“å…¥åºåˆ—æ˜¯ 7482 -> å•è¯è®­ç»ƒæ—¶é—´ epoch 1, batch 2 : 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755cf01-7e6b-4c64-b33d-bcb521087c9f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.724794Z",
     "iopub.status.busy": "2025-05-04T18:27:07.724602Z",
     "iopub.status.idle": "2025-05-04T18:27:07.727722Z",
     "shell.execute_reply": "2025-05-04T18:27:07.727259Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.724779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # æ»‘åŠ¨çª—å£åˆ†å—å¤„ç†å‡½æ•°\n",
    "# def sliding_window_tokenize(examples):\n",
    "#     max_length = 4096  # æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "#     stride = 2048      # æ»‘åŠ¨çª—å£æ­¥é•¿\n",
    "#     response_token = \"<Response>\"\n",
    "#     response_token_id = tokenizer.encode(response_token, add_special_tokens=False)[0]\n",
    "\n",
    "#     # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ†å—\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         stride=stride,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # å¤„ç†æ ‡ç­¾\n",
    "#     labels = tokenized[\"input_ids\"].clone()\n",
    "#     for i in range(labels.shape[0]):\n",
    "#         # æŸ¥æ‰¾responseæ ‡è®°ä½ç½®\n",
    "#         response_positions = (tokenized[\"input_ids\"][i] == response_token_id).nonzero(as_tuple=True)[0]\n",
    "#         if len(response_positions) > 0:\n",
    "#             # å¿½ç•¥responseæ ‡è®°ä¹‹å‰çš„lossè®¡ç®—\n",
    "#             labels[i, :response_positions[0]] = -100\n",
    "#         else:\n",
    "#             # æ²¡æœ‰responseæ ‡è®°çš„å—å®Œå…¨å¿½ç•¥\n",
    "#             labels[i, :] = -100\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized[\"input_ids\"],\n",
    "#         \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "# # åº”ç”¨åˆ†å—å¤„ç†\n",
    "# sft_tokenized = sft_dataset.map(\n",
    "#     sliding_window_tokenize,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"],\n",
    "#     batch_size=1  # ç¡®ä¿æ ·æœ¬ç‹¬ç«‹å¤„ç†\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec5da23-c1c0-40e8-a6ca-1a0d2db50589",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:07.728610Z",
     "iopub.status.busy": "2025-05-04T18:27:07.728248Z",
     "iopub.status.idle": "2025-05-04T18:27:07.730611Z",
     "shell.execute_reply": "2025-05-04T18:27:07.730139Z",
     "shell.execute_reply.started": "2025-05-04T18:27:07.728588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(sft_tokenized)\n",
    "# print(sft_tokenized.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:17.442942Z",
     "iopub.status.busy": "2025-05-04T19:34:17.442703Z",
     "iopub.status.idle": "2025-05-04T19:34:28.428742Z",
     "shell.execute_reply": "2025-05-04T19:34:28.428306Z",
     "shell.execute_reply.started": "2025-05-04T19:34:17.442928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4044/4044 [00:10<00:00, 373.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 111.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # åœ¨åŸºç¡€çŸ¥è¯†è®­ç»ƒç»“æŸåä¿å­˜é€‚é…å™¨å‚æ•°\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # åœ¨SFTè®­ç»ƒå¼€å§‹å‰åŠ è½½å¯¹æ¯”\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"å‚æ•°æœªç»§æ‰¿ï¼\"\n",
    "# from torch.cuda.amp import GradScaler\n",
    "# scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n",
    "\n",
    "# SFTå¾®è°ƒè®­ç»ƒ\n",
    "def tokenize_function(examples):  # æ–°å¢å‚æ•°\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # åº”ä¸ºæœ‰æ•ˆæ•°å€¼\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # ç¦ç”¨æˆªæ–­ï¼ˆéœ€ç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ â‰¤ max_lengthï¼‰\n",
    "        max_length=6000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i in range(len(labels)):\n",
    "        # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "        start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(start_pos) > 0:\n",
    "            labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    \n",
    "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "print(\"å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab0e53-1a91-4e4b-ada0-adb90b940cf4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T19:34:28.429810Z",
     "iopub.status.busy": "2025-05-04T19:34:28.429501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250505_033430-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_sft\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/i9xs4y9twq5kjeb0qdrkb\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/i9xs4y9twq5kjeb0qdrkb\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='2022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 226/2022 31:37 < 4:13:32, 0.12 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.858800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.861700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.908700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.684500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.968400</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.848000</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.785300</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.823800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.949700</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.963400</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.737300</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.889100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.807100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.818500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.700800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.907600</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.853800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.768100</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.801800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.896500</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.757600</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.805800</td>\n",
       "      <td>2.574530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 10 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 20 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 30 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 40 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 60 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 70 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 80 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 90 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 110 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 120 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 130 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 140 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 150 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 160 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 170 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 180 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 190 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 200 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 210 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/grad_norm already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/learning_rate already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/loss already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/runtime already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/samples_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key eval/steps_per_second already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 220 on key train/global_step already exists, ignored.\n"
     ]
    }
   ],
   "source": [
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # æ–°ä¿å­˜è·¯å¾„\n",
    "    num_train_epochs=1,  # å¢åŠ è®­ç»ƒè½®æ¬¡\n",
    "    per_device_train_batch_size=1,  # å¢å¤§æ‰¹é‡å¤§å°\n",
    "    gradient_accumulation_steps=2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    # learning_rate=2e-4,  # è°ƒæ•´å­¦ä¹ ç‡\n",
    "    learning_rate=5e-5,  # è°ƒæ•´å­¦ä¹ ç‡\n",
    "    # fp16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # ç¦ç”¨è¯„ä¼°é˜¶æ®µçš„æ··åˆç²¾åº¦\n",
    "    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–æ˜¾å­˜[4]\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,  # é™ä½è¯„ä¼°æ­¥æ•°ä»¥é€‚é…å°æ•°æ®é‡\n",
    "    # eval_strategy=\"no\",\n",
    "    # do_eval=False,\n",
    "    # æé«˜è®­ç»ƒé€Ÿåº¦ï¼šçœæ‰äº† eval çš„è®¡ç®—å¼€é”€ï¼Œèƒ½è®©è®­ç»ƒæ›´ä¸“æ³¨äºå‰å‘/åå‘ä¼ æ’­\n",
    "    # èŠ‚çœæ˜¾å­˜å’Œæ˜¾å¡å¸¦å®½ï¼šé¿å…æ¯éš” N æ­¥å°±åŠ è½½æ•´ä¸ªéªŒè¯é›†å¯¼è‡´ OOM\n",
    "    # ç®€åŒ–æµç¨‹ï¼šåªåœ¨è®­ç»ƒç»“æŸåä¸€æ¬¡æ€§åšè¯„ä¼°ï¼Œé€»è¾‘æ›´ç®€å•\n",
    "        # æ— æ³•å®æ—¶ç›‘æ§è¿‡æ‹Ÿåˆï¼šä¸­é€”çœ‹ä¸åˆ° validation lossï¼Œä¹Ÿå°±éš¾ä»¥åŠæ—¶å‘ç°æ¨¡å‹å¼€å§‹è¿‡æ‹Ÿåˆæˆ–å­¦ä¹ ç‡éœ€è¦è°ƒæ•´ã€‚\n",
    "        # è°ƒå‚ä¸ä¾¿ï¼šå¦‚æœæƒ³æ ¹æ®ä¸­é€”çš„è¯„ä¼°ç»“æœè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚ load_best_model_at_end=Trueã€lr_scheduler å›è°ƒï¼‰ï¼Œå°±å¿…é¡»ä¿ç•™ evalã€‚\n",
    "        # æ—©åœ (early stopping) ä¸å¯ç”¨ï¼šæ²¡æ³•æ ¹æ®éªŒè¯æŒ‡æ ‡æå‰åœæ­¢è®­ç»ƒï¼Œå¯èƒ½æµªè´¹è®­ç»ƒèµ„æºã€‚\n",
    "    logging_steps=10,      # æ¯ä¸ªè®­ç»ƒæ­¥éª¤è®°å½•æ—¥å¿—\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # æ–°å¢TensorBoardæ—¥å¿—ç›®å½•\n",
    "    # report_to=[\"tensorboard\"],  # å¯ç”¨TensorBoardæŠ¥å‘Š\n",
    "    load_best_model_at_end=True  # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "# torch.autograd.set_detect_anomaly(True)  # å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\n",
    "sft_trainer.train()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.status.busy": "2025-05-04T16:09:13.646199Z",
     "iopub.status.idle": "2025-05-04T16:09:13.646396Z",
     "shell.execute_reply": "2025-05-04T16:09:13.646304Z",
     "shell.execute_reply.started": "2025-05-04T16:09:13.646294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # æå–è®­ç»ƒæ—¥å¿—ï¼ˆåŒ…å« loss æˆ– train_loss å’Œ stepï¼‰\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # ç»Ÿä¸€å­—æ®µåä¸º train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # è½¬æ¢ä¸º DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # æå–è¯„ä¼°æ—¥å¿—ï¼ˆåŒ…å« eval_loss å’Œ stepï¼‰\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # åˆå¹¶è®­ç»ƒå’Œè¯„ä¼°æ—¥å¿—\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # ä½¿ç”¨ ffill() æ›¿ä»£ fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆä»»åŠ¡è¯„ä¼°ï¼ˆBLEU/ROUGE/METEORï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T10:23:50.792129Z",
     "iopub.status.busy": "2025-05-04T10:23:50.791929Z",
     "iopub.status.idle": "2025-05-04T10:23:52.045236Z",
     "shell.execute_reply": "2025-05-04T10:23:52.044779Z",
     "shell.execute_reply.started": "2025-05-04T10:23:50.792112Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # ä¸­æ–‡åˆ†è¯æ”¯æŒ\n",
    "\n",
    "# åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "df = pd.read_csv(\"game_strategy_and_term.csv\").head(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T10:23:52.046094Z",
     "iopub.status.busy": "2025-05-04T10:23:52.045886Z",
     "iopub.status.idle": "2025-05-04T10:28:56.237544Z",
     "shell.execute_reply": "2025-05-04T10:28:56.237052Z",
     "shell.execute_reply.started": "2025-05-04T10:23:52.046081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== æ ·æœ¬1 åŸå§‹å†…å®¹ ====================\n",
      "ã€å‚è€ƒå›ç­”ã€‘: åœ¨ç‹¼äººæ€è¿™æ¬¾æ¨ç†æ¸¸æˆä¸­ï¼ŒçŒäººä½œä¸ºä¸€ä¸ªå…³é”®è§’è‰²ï¼Œå…¶ç©æ³•ç­–ç•¥æä¸ºé‡è¦ã€‚çŒäººå…·æœ‰ç‹¬ç‰¹çš„æŠ€èƒ½â€”â€”åœ¨æ­»äº¡æ—¶å¯ä»¥å¸¦èµ°ä¸€åç©å®¶ï¼Œè¿™ä½¿å¾—çŒäººåœ¨æ¸¸æˆä¸­å…·æœ‰ä¸å¯å¿½è§†çš„å½±å“åŠ›ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»çŒäººçš„ç©æ³•æŠ€å·§ï¼Œå¸®åŠ©æ–°æ‰‹ç©å®¶æ›´å¥½åœ°æŒæ¡è¿™ä¸€è§’è‰²ã€‚\n",
      "é¦–å…ˆï¼ŒçŒäººçš„èµ·è·³æ—¶æœºè‡³å…³é‡è¦ã€‚ç”±äºçŒäººçš„èº«ä»½è¾ƒä¸ºç‰¹æ®Šï¼Œæœ‰æ—¶ä¼šæœ‰ç‹¼äººé€‰æ‹©æ‚è·³çŒäººã€‚å› æ­¤ï¼ŒçŒäººéœ€è¦åœ¨åˆé€‚çš„æ—¶æœºè¡¨æ˜è‡ªå·±çš„èº«ä»½ï¼Œä»¥ç¡®ä¿å¥½äººé˜µè¥çš„ä¼˜åŠ¿ã€‚åœ¨èµ·è·³ä¹‹å‰ï¼ŒçŒäººéœ€è¦å……åˆ†è§‚å¯Ÿåœºä¸Šçš„å±€åŠ¿ï¼Œç¡®ä¿è‡ªå·±çš„èº«ä»½èƒ½å¤Ÿå¾—åˆ°å¤§å®¶çš„è®¤å¯ã€‚\n",
      "å…¶æ¬¡ï¼ŒçŒäººçš„æªæ³•ä¹Ÿæ˜¯ç©æ³•ä¸­çš„ä¸€å¤§å…³é”®ã€‚ç”±äºçŒäººåœ¨æ­»äº¡æ—¶å¯ä»¥å¸¦èµ°ä¸€åç©å®¶ï¼Œä½†æ— æ³•å¾—çŸ¥è¯¥ç©å®¶çš„çœŸå®èº«ä»½ï¼Œå› æ­¤å¼€æªçš„æ—¶æœºå’Œé€‰æ‹©è‡³å…³é‡è¦ã€‚åœ¨æ²¡æœ‰åè¶³æŠŠæ¡çš„æƒ…å†µä¸‹ï¼ŒçŒäººåº”è¯¥è°¨æ…å¼€æªï¼Œé¿å…è¯¯æ€å¥½äººï¼Œç‰¹åˆ«æ˜¯é¢„è¨€å®¶å’Œå¥³å·«ç­‰é‡è¦è§’è‰²ã€‚åœ¨å‰æœŸï¼ŒçŒäººå¯ä»¥å°½é‡ä¿æŒä½è°ƒï¼Œé€šè¿‡è§‚å¯Ÿå’Œæ¨ç†ç§¯ç´¯ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨å…³é”®æ—¶åˆ»åšå‡ºæ­£ç¡®çš„å†³ç­–ã€‚\n",
      "åœ¨æ¸¸æˆè¿›è¡Œåˆ°ä¸­æ®µæ—¶ï¼ŒçŒäººéœ€è¦æ³¨æ„å¥³å·«çš„åŠ¨å‘ã€‚å½“å¥³å·«è·³å‡ºæ¥è¡¨æ€å¹¶è¡¨æ˜è‡ªå·±æ•‘äº†è°æ—¶ï¼ŒçŒäººåº”è¯¥å‹‡æ•¢åœ°ç«™å‡ºæ¥è¡¨æ€ï¼Œä»¥é¿å…è¢«å¥³å·«è¯¯æ€ã€‚è¿™æ˜¯å› ä¸ºå¥³å·«åœ¨æŠ•æ¯’æ—¶ï¼ŒçŒäººæ— æ³•è§¦å‘æŠ€èƒ½ï¼Œå› æ­¤åŠæ—¶è¡¨æ˜èº«ä»½æ˜¯ä¿æŠ¤è‡ªå·±çš„é‡è¦æ‰‹æ®µã€‚\n",
      "æ­¤å¤–ï¼ŒçŒäººåœ¨æ¸¸æˆä¸­è¿˜éœ€è¦ä¿æŒå†·é™å’Œç†æ™ºã€‚åœ¨é¢å¯¹ç‹¼äººçš„æŒ‘è¡…å’Œè´¨ç–‘æ—¶ï¼ŒçŒäººéœ€è¦åšå®šè‡ªå·±çš„ç«‹åœºï¼Œç”¨é€»è¾‘å’Œè¯æ®æ¥å›åº”ã€‚åŒæ—¶ï¼ŒçŒäººä¹Ÿéœ€è¦ä¸é˜Ÿå‹ä¿æŒè‰¯å¥½çš„æ²Ÿé€šï¼Œå…±åŒåˆ†æå±€åŠ¿ï¼Œåˆ¶å®šåˆé€‚çš„ç­–ç•¥ã€‚\n",
      "æ€»ä¹‹ï¼ŒçŒäººåœ¨ç‹¼äººæ€æ¸¸æˆä¸­å…·æœ‰é‡è¦çš„ä½œç”¨ã€‚é€šè¿‡æŒæ¡èµ·è·³æ—¶æœºã€è°¨æ…å¼€æªã€æ³¨æ„å¥³å·«åŠ¨å‘ä»¥åŠä¿æŒå†·é™ç†æ™ºç­‰æŠ€å·§ï¼Œæ–°æ‰‹ç©å®¶å¯ä»¥æ›´å¥½åœ°æ‰®æ¼”çŒäººè¿™ä¸€è§’è‰²ï¼Œä¸ºå¥½äººé˜µè¥çš„èƒœåˆ©è´¡çŒ®è‡ªå·±çš„åŠ›é‡ã€‚å¸Œæœ›æœ¬æ–‡çš„æ”»ç•¥èƒ½å¤Ÿå¸®åŠ©ä½ åœ¨ç‹¼äººæ€æ¸¸æˆä¸­å–å¾—æ›´å¥½çš„æˆç»©ï¼\n",
      "\n",
      "ã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: ç‹¼äººæ€ä¸­ï¼Œå¦‚ä½•æ‰®æ¼”å¥½çŒäººè§’è‰²å¹¶ä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ï¼Ÿ åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒçŒäººæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œä»–çš„ä¸»è¦ä»»åŠ¡æ˜¯é€šè¿‡æ¯’æ€ç‹¼äººæ¥å¸®åŠ©å¥½äººé˜µè¥è·èƒœã€‚ç„¶è€Œï¼ŒçŒäººå¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„è§’è‰²ï¼Œéœ€è¦ç©å®¶å…·å¤‡ä¸€å®šçš„ç­–ç•¥å’ŒæŠ€å·§ã€‚æœ¬æ–‡å°†ä¸ºå¤§å®¶ä»‹ç»å¦‚ä½•æ‰®æ¼”å¥½çŒäººè§’è‰²å¹¶ä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ã€‚\n",
      "1. äº†è§£çŒäººçš„æŠ€èƒ½å’Œç‰¹ç‚¹\n",
      "çŒäººæ˜¯ç‹¼äººæ€æ¸¸æˆä¸­æœ€å¼ºå¤§çš„è§’è‰²ä¹‹ä¸€ï¼Œä»–çš„æŠ€èƒ½æ˜¯æ¯’æ€ç‹¼äººã€‚ç„¶è€Œï¼ŒçŒäººä¹Ÿæœ‰è‡ªå·±çš„ç‰¹ç‚¹ï¼Œæ¯”å¦‚ä»–éœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œè€Œä¸”æ¯’æ€çš„ç‹¼äººå¿…é¡»æ˜¯è‡ªå·±è§‚å¯Ÿåˆ°çš„ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚\n",
      "2. é€‰æ‹©åˆé€‚çš„æ¯’æ€ç›®æ ‡\n",
      "çŒäººéœ€è¦é€‰æ‹©åˆé€‚çš„æ¯’æ€ç›®æ ‡ï¼Œè¿™éœ€è¦ç©å®¶å…·å¤‡ä¸€å®šçš„è§‚å¯Ÿå’Œåˆ†æèƒ½åŠ›ã€‚çŒäººéœ€è¦è§‚å¯Ÿç‹¼äººçš„è¡Œä¸ºå’Œè¡¨æƒ…ï¼Œå¯»æ‰¾ç‹¼äººæ”»å‡»çš„ç›®æ ‡ã€‚åŒæ—¶ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„å¥½äººé˜µè¥çš„è¡ŒåŠ¨ï¼Œé¿å…æ¯’æ€æ— è¾œçš„ç©å®¶ã€‚\n",
      "3. ä¿æŒå†·é™å’Œè°¨æ…\n",
      "çŒäººéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…åœ¨å¤œæ™šè¿‡äºæ¿€åŠ¨æˆ–ç´§å¼ ã€‚çŒäººéœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«å¥½äººé˜µè¥å‘ç°å¹¶è¢«å¥½äººé˜µè¥æ”»å‡»ã€‚\n",
      "4. ä¸å…¶ä»–ç©å®¶åˆä½œ\n",
      "çŒäººéœ€è¦ä¸å…¶ä»–ç©å®¶åˆä½œï¼Œä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆ¶å®šç­–ç•¥ï¼Œå…±åŒä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ã€‚çŒäººéœ€è¦ä¸å…¶ä»–ç©å®¶åˆ†äº«è‡ªå·±çš„è§‚å¯Ÿå’Œåˆ†æç»“æœï¼Œä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆ¶å®šæ¯’æ€è®¡åˆ’ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦ä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆä½œï¼Œå…±åŒåº”å¯¹ç‹¼äººçš„æ”»å‡»ã€‚\n",
      "5. æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨\n",
      "çŒäººéœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…åœ¨å¤œæ™šè¿‡äºæ¿€åŠ¨æˆ–ç´§å¼ ã€‚çŒäººéœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«å¥½äººé˜µè¥å‘ç°å¹¶è¢«\n",
      "ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: ç‹¼äººæ€ä¸­ï¼Œå¦‚ä½•æ‰®æ¼”å¥½çŒäººè§’è‰²å¹¶ä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ï¼Ÿ åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒçŒäººæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œä»–çš„ä¸»è¦ä»»åŠ¡æ˜¯é€šè¿‡æ¯’æ€ç‹¼äººæ¥å¸®åŠ©å¥½äººé˜µè¥è·èƒœã€‚ç„¶è€Œï¼ŒçŒäººå¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„è§’è‰²ï¼Œéœ€è¦ç©å®¶å…·å¤‡ä¸€å®šçš„ç­–ç•¥å’ŒæŠ€å·§ã€‚æœ¬æ–‡å°†ä¸ºå¤§å®¶ä»‹ç»å¦‚ä½•æ‰®æ¼”å¥½çŒäººè§’è‰²å¹¶ä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ã€‚\n",
      "1. äº†è§£çŒäººçš„æŠ€èƒ½å’Œç‰¹ç‚¹\n",
      "çŒäººæ˜¯ç‹¼äººæ€æ¸¸æˆä¸­æœ€å¼ºå¤§çš„è§’è‰²ä¹‹ä¸€ï¼Œä»–çš„æŠ€èƒ½æ˜¯æ¯’æ€ç‹¼äººã€‚ç„¶è€Œï¼ŒçŒäººä¹Ÿæœ‰è‡ªå·±çš„ç‰¹ç‚¹ï¼Œæ¯”å¦‚ä»–éœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œè€Œä¸”æ¯’æ€çš„ç‹¼äººå¿…é¡»æ˜¯è‡ªå·±è§‚å¯Ÿåˆ°çš„ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚\n",
      "2. é€‰æ‹©åˆé€‚çš„æ¯’æ€ç›®æ ‡\n",
      "çŒäººéœ€è¦é€‰æ‹©åˆé€‚çš„æ¯’æ€ç›®æ ‡ï¼Œè¿™éœ€è¦ç©å®¶å…·å¤‡ä¸€å®šçš„è§‚å¯Ÿå’Œåˆ†æèƒ½åŠ›ã€‚çŒäººéœ€è¦è§‚å¯Ÿç‹¼äººçš„è¡Œä¸ºå’Œè¡¨æƒ…ï¼Œå¯»æ‰¾ç‹¼äººæ”»å‡»çš„ç›®æ ‡ã€‚åŒæ—¶ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„å¥½äººé˜µè¥çš„è¡ŒåŠ¨ï¼Œé¿å…æ¯’æ€æ— è¾œçš„ç©å®¶ã€‚\n",
      "3. ä¿æŒå†·é™å’Œè°¨æ…\n",
      "çŒäººéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…åœ¨å¤œæ™šè¿‡äºæ¿€åŠ¨æˆ–ç´§å¼ ã€‚çŒäººéœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«å¥½äººé˜µè¥å‘ç°å¹¶è¢«å¥½äººé˜µè¥æ”»å‡»ã€‚\n",
      "4. ä¸å…¶ä»–ç©å®¶åˆä½œ\n",
      "çŒäººéœ€è¦ä¸å…¶ä»–ç©å®¶åˆä½œï¼Œä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆ¶å®šç­–ç•¥ï¼Œå…±åŒä¸ºå¥½äººé˜µè¥å–å¾—èƒœåˆ©ã€‚çŒäººéœ€è¦ä¸å…¶ä»–ç©å®¶åˆ†äº«è‡ªå·±çš„è§‚å¯Ÿå’Œåˆ†æç»“æœï¼Œä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆ¶å®šæ¯’æ€è®¡åˆ’ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦ä¸å…¶ä»–ç©å®¶ä¸€èµ·åˆä½œï¼Œå…±åŒåº”å¯¹ç‹¼äººçš„æ”»å‡»ã€‚\n",
      "5. æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨\n",
      "çŒäººéœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…åœ¨å¤œæ™šè¿‡äºæ¿€åŠ¨æˆ–ç´§å¼ ã€‚çŒäººéœ€è¦åœ¨å¤œæ™šè¿›è¡Œæ¯’æ€ï¼Œéœ€è¦ä¿æŒå†·é™å’Œè°¨æ…ï¼Œé¿å…è¢«ç‹¼äººå‘ç°å¹¶è¢«ç‹¼äººæ”»å‡»ã€‚æ­¤å¤–ï¼ŒçŒäººè¿˜éœ€è¦æ³¨æ„è‡ªå·±çš„è¡ŒåŠ¨ï¼Œé¿å…è¢«å¥½äººé˜µè¥å‘ç°å¹¶è¢«\n",
      "\n",
      "==================== æ ·æœ¬1 è¯„ä¼°ç»“æœ ====================\n",
      "BLEU-2 - åŸå§‹: 0.2682, å¾®è°ƒ: 0.2682\n",
      "ROUGE-2 F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "ROUGE-L F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - åŸå§‹: 5.27, å¾®è°ƒ: 5.27\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== æ ·æœ¬2 åŸå§‹å†…å®¹ ====================\n",
      "ã€å‚è€ƒå›ç­”ã€‘: ç‹¼äººæ€ä½œä¸ºä¸€æ¬¾å¿ƒç†æ¨ç†æ¸¸æˆï¼Œä¸ä»…è€ƒéªŒç©å®¶çš„é€»è¾‘èƒ½åŠ›å’Œæ´å¯ŸåŠ›ï¼Œè¿˜æ¶‰åŠåˆ°å‘è¨€çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œéšç€æ¸¸æˆçš„æ·±å…¥ï¼Œä¸€äº›ç©å®¶å¯èƒ½ä¼šå°è¯•ç¯¡æ”¹å‘è¨€ï¼Œç»™æ¸¸æˆå¢æ·»äº†ä¸€äº›æŒ‘æˆ˜å’Œäº‰è®®ã€‚åœ¨é¢å¯¹è¿™ç§æƒ…å†µæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•åº”å¯¹å‘¢ï¼Ÿä¸‹é¢å°±è®©æˆ‘ä»¬æ¥æ¢è®¨ä¸€ä¸‹ã€‚\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬è¦è®¤è¯†åˆ°ç¯¡æ”¹å‘è¨€åœ¨ç‹¼äººæ€æ¸¸æˆä¸­å¹¶ä¸ç½•è§ã€‚ç”±äºæ¯ä¸ªäººå¯¹ä»–äººå‘è¨€çš„ç†è§£æœ‰æ‰€åå·®ï¼Œç¯¡æ”¹å‘è¨€å¾€å¾€æ˜¯äººä»¬æ ¹æ®è‡ªå·±çš„ä»·å€¼è§‚è¿›è¡Œé€‰æ‹©æ€§æˆªå–æˆ–ä¿®æ”¹åçš„äº§ç‰©ã€‚å› æ­¤ï¼Œä¸èƒ½ç®€å•åœ°å°†ç¯¡æ”¹å‘è¨€è§†ä¸ºç‹¼çš„è¡Œä¸ºï¼Œè€Œæ˜¯è¦ç†è§£å…¶èƒŒåçš„åŠ¨æœºå’ŒåŸå› ã€‚\n",
      "å…¶æ¬¡ï¼Œå‘è¨€å†…å®¹å¹¶ä¸æ˜¯å”¯ä¸€çš„åˆ¤æ–­ä¾æ®ã€‚åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œç©å®¶å¯ä»¥é€šè¿‡è§‚å¯Ÿå¦ç›¸ã€è¡¨æƒ…ã€çŠ¶æ€ç­‰æ–¹å¼æ¥æ¨æ–­å…¶ä»–ç©å®¶çš„èº«ä»½ã€‚å› æ­¤ï¼Œå³ä½¿å‘è¨€è¢«ç¯¡æ”¹ï¼Œä»ç„¶æœ‰å…¶ä»–æ–¹å¼å¯ä»¥è¾…åŠ©ç¡®è®¤ç©å®¶çš„çœŸå®èº«ä»½ã€‚æˆ‘ä»¬åº”è¯¥æ‘†è„±å¯¹å‘è¨€å†…å®¹çš„è¿‡åº¦ä¾èµ–ï¼Œè€Œæ˜¯ç»¼åˆè€ƒè™‘å¤šç§å› ç´ è¿›è¡Œåˆ¤æ–­ã€‚\n",
      "é’ˆå¯¹ç¯¡æ”¹å‘è¨€è¡Œä¸ºï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä¸€äº›ç­–ç•¥æ¥æœ‰æ•ˆåœ°åº”å¯¹ã€‚é¦–å…ˆï¼Œè¦å°½é‡ä¿æŒå†·é™ï¼Œä¸è¦è¢«ç¯¡æ”¹å‘è¨€æ‰€å½±å“ï¼Œè€Œæ˜¯è¦åšæŒè‡ªå·±çš„åˆ¤æ–­å’Œè§‚ç‚¹ã€‚å…¶æ¬¡ï¼Œå¯ä»¥é€šè¿‡æå‡ºé—®é¢˜æˆ–å‘å…¶ä»–ç©å®¶æ±‚è¯æ¥éªŒè¯å‘è¨€çš„çœŸå®æ€§ï¼Œä»è€Œé¿å…è¢«è¯¯å¯¼ã€‚æ­¤å¤–ï¼Œå¦‚æœå‘ç°æœ‰ç©å®¶é¢‘ç¹ç¯¡æ”¹å‘è¨€ï¼Œå¯ä»¥åœ¨æ¸¸æˆè§„åˆ™ä¸­åŠ å…¥ç›¸åº”çš„é™åˆ¶æˆ–æƒ©ç½šï¼Œä»¥ç»´æŠ¤æ¸¸æˆçš„å…¬å¹³æ€§å’Œè¶£å‘³æ€§ã€‚\n",
      "æœ€åï¼Œæˆ‘ä»¬è¦è®°ä½ï¼Œç‹¼äººæ€æ˜¯ä¸€æ¬¾æ—¨åœ¨å¨±ä¹å’Œäº¤æµçš„æ¸¸æˆï¼Œè€Œä¸æ˜¯ä¸¥è‚ƒçš„é€»è¾‘ç«æŠ€ã€‚åœ¨æ¸¸æˆä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ³¨é‡çš„æ˜¯ç©å®¶ä¹‹é—´çš„äº’åŠ¨å’Œåˆä½œï¼Œè€Œä¸æ˜¯çº ç¼ äºå‘è¨€çš„ç»†èŠ‚å’Œç¯¡æ”¹è¡Œä¸ºã€‚åªæœ‰åœ¨è½»æ¾æ„‰å¿«çš„æ°›å›´ä¸­ï¼Œæ‰èƒ½çœŸæ­£ä½“éªŒåˆ°ç‹¼äººæ€æ¸¸æˆçš„ä¹è¶£ã€‚\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œé¢å¯¹ç‹¼äººæ€ä¸­çš„å‘è¨€ç¯¡æ”¹è¡Œä¸ºï¼Œæˆ‘ä»¬åº”è¯¥ä»¥å®½å®¹å’Œç†è§£çš„æ€åº¦å»åº”å¯¹ï¼ŒåŒæ—¶é‡‡å–ç›¸åº”çš„ç­–ç•¥æ¥åº”å¯¹æŒ‘æˆ˜ã€‚é‡è¦çš„æ˜¯è¦ä¿æŒå†·é™ï¼Œä¸å—å¹²æ‰°ï¼Œä»è€Œæ›´å¥½åœ°äº«å—æ¸¸æˆå¸¦æ¥çš„ä¹è¶£ã€‚\n",
      "\n",
      "ã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: ç‹¼äººæ€æ¸¸æˆä¸­å¦‚ä½•åº”å¯¹ç©å®¶ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºï¼Ÿ åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œç©å®¶ç¯¡æ”¹å‘è¨€æ˜¯ä¸€ç§å¸¸è§çš„è¡Œä¸ºï¼Œå®ƒå¯èƒ½æºäºå„ç§åŸå› ï¼Œå¦‚ç´§å¼ ã€æƒ…ç»ªæ¿€åŠ¨æˆ–è¯•å›¾æ©ç›–è‡ªå·±çš„èº«ä»½ã€‚é¢å¯¹è¿™ç§æƒ…å†µï¼Œä½œä¸ºç©å®¶ï¼Œåº”è¯¥é‡‡å–ä»¥ä¸‹ç­–ç•¥æ¥åº”å¯¹ï¼š\n",
      "\n",
      "1. **ä¿æŒå†·é™**ï¼šé¦–å…ˆï¼Œä¸è¦è¢«å…¶ä»–ç©å®¶çš„å‘è¨€æ‰€å½±å“ï¼Œä¿æŒè‡ªå·±çš„å†·é™ã€‚æƒ…ç»ªæ¿€åŠ¨å¯èƒ½ä¼šå¯¼è‡´ä½ åšå‡ºé”™è¯¯çš„åˆ¤æ–­ã€‚\n",
      "\n",
      "2. **è§‚å¯Ÿå…¶ä»–ç©å®¶çš„è¡Œä¸º**ï¼šæ³¨æ„å…¶ä»–ç©å®¶çš„å‘è¨€å’Œè¡Œä¸ºï¼Œçœ‹çœ‹ä»–ä»¬æ˜¯å¦çœŸçš„æœ‰ç¯¡æ”¹å‘è¨€çš„åŠ¨æœºã€‚æœ‰æ—¶å€™ï¼Œç©å®¶å¯èƒ½ä¼šå› ä¸ºç´§å¼ è€Œè¯¯æŠ¥èº«ä»½ã€‚\n",
      "\n",
      "3. **åˆ†æå‘è¨€å†…å®¹**ï¼šä»”ç»†åˆ†æå…¶ä»–ç©å®¶çš„å‘è¨€å†…å®¹ï¼Œçœ‹æ˜¯å¦æœ‰æ˜æ˜¾çš„é€»è¾‘æ¼æ´æˆ–ä¸ä¸€è‡´çš„åœ°æ–¹ã€‚å¦‚æœå‘ç°æœ‰ç¯¡æ”¹çš„è¿¹è±¡ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚\n",
      "\n",
      "4. **ä½¿ç”¨é€»è¾‘æ¨ç†**ï¼šåˆ©ç”¨é€»è¾‘æ¨ç†æ¥åˆ¤æ–­å‘è¨€çš„çœŸå®æ€§ã€‚å¦‚æœå‘è¨€å†…å®¹ä¸æ¸¸æˆè§„åˆ™ç›¸æ‚–ï¼Œæˆ–è€…ç¼ºä¹è¶³å¤Ÿçš„è¯æ®æ”¯æŒï¼Œé‚£ä¹ˆè¿™ä¸ªå‘è¨€å¯èƒ½æ˜¯ç¯¡æ”¹çš„ã€‚\n",
      "\n",
      "5. **å°Šé‡å…¶ä»–ç©å®¶**ï¼šå³ä½¿ä½ æ€€ç–‘æŸä¸ªç©å®¶çš„å‘è¨€ï¼Œä¹Ÿåº”è¯¥å°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€ï¼Œä¸è¦è½»æ˜“åœ°å¦å®šä»–ä»¬çš„å‘è¨€ã€‚å°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€æ˜¯æ¸¸æˆçš„æ­£å¸¸ç§©åºã€‚\n",
      "\n",
      "6. **å¯»æ±‚å›¢é˜Ÿæ”¯æŒ**ï¼šå¦‚æœæ€€ç–‘æŸä¸ªç©å®¶çš„å‘è¨€ï¼Œå¯ä»¥å‘å›¢é˜Ÿæˆå‘˜å¯»æ±‚å¸®åŠ©ï¼Œå…±åŒåˆ†æå’Œåˆ¤æ–­ã€‚å›¢é˜Ÿçš„åŠ›é‡å¾€å¾€æ¯”ä¸ªäººæ›´å¼ºå¤§ã€‚\n",
      "\n",
      "7. **é¿å…ç›´æ¥å†²çª**ï¼šå°½é‡é¿å…ç›´æ¥å†²çªï¼Œè€Œæ˜¯é€šè¿‡é—´æ¥çš„æ–¹å¼è¡¨è¾¾ä½ çš„è§‚ç‚¹ã€‚è¿™æ ·å¯ä»¥é¿å…å¼•èµ·ä¸å¿…è¦çš„äº‰è®®ï¼ŒåŒæ—¶ä¹Ÿèƒ½æ›´å¥½åœ°ä¿æŠ¤å…¶ä»–ç©å®¶çš„å‘è¨€æƒã€‚\n",
      "\n",
      "8. **å­¦ä¹ å’Œé€‚åº”**ï¼šåœ¨æ¸¸æˆä¸­ï¼Œç©å®¶ä¼šé€æ¸å­¦ä¼šå¦‚ä½•åº”å¯¹ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºã€‚é€šè¿‡ä¸æ–­çš„å®è·µå’Œå­¦ä¹ ï¼Œä½ å¯ä»¥æé«˜è‡ªå·±çš„åˆ¤æ–­åŠ›å’Œåº”å¯¹èƒ½åŠ›ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œé¢å¯¹ç©å®¶ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºï¼Œå…³é”®æ˜¯è¦ä¿æŒå†·é™ï¼Œé€šè¿‡è§‚å¯Ÿã€åˆ†æå’Œé€»è¾‘æ¨ç†æ¥åˆ¤æ–­å‘è¨€çš„çœŸå®æ€§ã€‚åŒæ—¶ï¼Œå°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€ï¼Œé¿å…ç›´æ¥å†²çªï¼Œæ˜¯åº”å¯¹è¿™ç§è¡Œä¸ºçš„åŸºæœ¬ç­–ç•¥ã€‚\n",
      "ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: ç‹¼äººæ€æ¸¸æˆä¸­å¦‚ä½•åº”å¯¹ç©å®¶ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºï¼Ÿ åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œç©å®¶ç¯¡æ”¹å‘è¨€æ˜¯ä¸€ç§å¸¸è§çš„è¡Œä¸ºï¼Œå®ƒå¯èƒ½æºäºå„ç§åŸå› ï¼Œå¦‚ç´§å¼ ã€æƒ…ç»ªæ¿€åŠ¨æˆ–è¯•å›¾æ©ç›–è‡ªå·±çš„èº«ä»½ã€‚é¢å¯¹è¿™ç§æƒ…å†µï¼Œä½œä¸ºç©å®¶ï¼Œåº”è¯¥é‡‡å–ä»¥ä¸‹ç­–ç•¥æ¥åº”å¯¹ï¼š\n",
      "\n",
      "1. **ä¿æŒå†·é™**ï¼šé¦–å…ˆï¼Œä¸è¦è¢«å…¶ä»–ç©å®¶çš„å‘è¨€æ‰€å½±å“ï¼Œä¿æŒè‡ªå·±çš„å†·é™ã€‚æƒ…ç»ªæ¿€åŠ¨å¯èƒ½ä¼šå¯¼è‡´ä½ åšå‡ºé”™è¯¯çš„åˆ¤æ–­ã€‚\n",
      "\n",
      "2. **è§‚å¯Ÿå…¶ä»–ç©å®¶çš„è¡Œä¸º**ï¼šæ³¨æ„å…¶ä»–ç©å®¶çš„å‘è¨€å’Œè¡Œä¸ºï¼Œçœ‹çœ‹ä»–ä»¬æ˜¯å¦çœŸçš„æœ‰ç¯¡æ”¹å‘è¨€çš„åŠ¨æœºã€‚æœ‰æ—¶å€™ï¼Œç©å®¶å¯èƒ½ä¼šå› ä¸ºç´§å¼ è€Œè¯¯æŠ¥èº«ä»½ã€‚\n",
      "\n",
      "3. **åˆ†æå‘è¨€å†…å®¹**ï¼šä»”ç»†åˆ†æå…¶ä»–ç©å®¶çš„å‘è¨€å†…å®¹ï¼Œçœ‹æ˜¯å¦æœ‰æ˜æ˜¾çš„é€»è¾‘æ¼æ´æˆ–ä¸ä¸€è‡´çš„åœ°æ–¹ã€‚å¦‚æœå‘ç°æœ‰ç¯¡æ”¹çš„è¿¹è±¡ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚\n",
      "\n",
      "4. **ä½¿ç”¨é€»è¾‘æ¨ç†**ï¼šåˆ©ç”¨é€»è¾‘æ¨ç†æ¥åˆ¤æ–­å‘è¨€çš„çœŸå®æ€§ã€‚å¦‚æœå‘è¨€å†…å®¹ä¸æ¸¸æˆè§„åˆ™ç›¸æ‚–ï¼Œæˆ–è€…ç¼ºä¹è¶³å¤Ÿçš„è¯æ®æ”¯æŒï¼Œé‚£ä¹ˆè¿™ä¸ªå‘è¨€å¯èƒ½æ˜¯ç¯¡æ”¹çš„ã€‚\n",
      "\n",
      "5. **å°Šé‡å…¶ä»–ç©å®¶**ï¼šå³ä½¿ä½ æ€€ç–‘æŸä¸ªç©å®¶çš„å‘è¨€ï¼Œä¹Ÿåº”è¯¥å°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€ï¼Œä¸è¦è½»æ˜“åœ°å¦å®šä»–ä»¬çš„å‘è¨€ã€‚å°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€æ˜¯æ¸¸æˆçš„æ­£å¸¸ç§©åºã€‚\n",
      "\n",
      "6. **å¯»æ±‚å›¢é˜Ÿæ”¯æŒ**ï¼šå¦‚æœæ€€ç–‘æŸä¸ªç©å®¶çš„å‘è¨€ï¼Œå¯ä»¥å‘å›¢é˜Ÿæˆå‘˜å¯»æ±‚å¸®åŠ©ï¼Œå…±åŒåˆ†æå’Œåˆ¤æ–­ã€‚å›¢é˜Ÿçš„åŠ›é‡å¾€å¾€æ¯”ä¸ªäººæ›´å¼ºå¤§ã€‚\n",
      "\n",
      "7. **é¿å…ç›´æ¥å†²çª**ï¼šå°½é‡é¿å…ç›´æ¥å†²çªï¼Œè€Œæ˜¯é€šè¿‡é—´æ¥çš„æ–¹å¼è¡¨è¾¾ä½ çš„è§‚ç‚¹ã€‚è¿™æ ·å¯ä»¥é¿å…å¼•èµ·ä¸å¿…è¦çš„äº‰è®®ï¼ŒåŒæ—¶ä¹Ÿèƒ½æ›´å¥½åœ°ä¿æŠ¤å…¶ä»–ç©å®¶çš„å‘è¨€æƒã€‚\n",
      "\n",
      "8. **å­¦ä¹ å’Œé€‚åº”**ï¼šåœ¨æ¸¸æˆä¸­ï¼Œç©å®¶ä¼šé€æ¸å­¦ä¼šå¦‚ä½•åº”å¯¹ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºã€‚é€šè¿‡ä¸æ–­çš„å®è·µå’Œå­¦ä¹ ï¼Œä½ å¯ä»¥æé«˜è‡ªå·±çš„åˆ¤æ–­åŠ›å’Œåº”å¯¹èƒ½åŠ›ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œé¢å¯¹ç©å®¶ç¯¡æ”¹å‘è¨€çš„è¡Œä¸ºï¼Œå…³é”®æ˜¯è¦ä¿æŒå†·é™ï¼Œé€šè¿‡è§‚å¯Ÿã€åˆ†æå’Œé€»è¾‘æ¨ç†æ¥åˆ¤æ–­å‘è¨€çš„çœŸå®æ€§ã€‚åŒæ—¶ï¼Œå°Šé‡å…¶ä»–ç©å®¶çš„å‘è¨€ï¼Œé¿å…ç›´æ¥å†²çªï¼Œæ˜¯åº”å¯¹è¿™ç§è¡Œä¸ºçš„åŸºæœ¬ç­–ç•¥ã€‚\n",
      "\n",
      "==================== æ ·æœ¬2 è¯„ä¼°ç»“æœ ====================\n",
      "BLEU-2 - åŸå§‹: 0.2893, å¾®è°ƒ: 0.2893\n",
      "ROUGE-2 F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "ROUGE-L F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - åŸå§‹: 5.59, å¾®è°ƒ: 5.59\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# å…³é—­jiebaè°ƒè¯•æ—¥å¿—\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# ä¸­æ–‡åˆ†è¯å·¥å…·ï¼ˆæ”¹ç”¨æœç´¢å¼•æ“æ¨¡å¼æå‡å¬å›ç‡ï¼‰\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼[[2]]\n",
    "\n",
    "# # ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "def generate_response(model, prompt):\n",
    "    input_text = f\"{prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)  # å¢åŠ ç”Ÿæˆé•¿åº¦\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®pad_token_idï¼ˆé¿å…è­¦å‘Šï¼‰\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è®¡ç®—BLEU/ROUGEï¼ˆå¢åŠ å¹³æ»‘å‡½æ•°å’ŒROUGE-Lï¼‰\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¨¡å‹å¯¹ç»™å®špromptå’Œresponseçš„å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # æ‹¼æ¥è¾“å…¥ä¸å“åº”\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # æ„é€ æ ‡ç­¾ï¼šä»…è®¡ç®—responseéƒ¨åˆ†çš„loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„lossè®¡ç®—\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # è¿”å›å›°æƒ‘åº¦\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    # instruction = example[\"instruction\"]#######################################for sft prompt\n",
    "    # print(f\"æŒ‡ä»¤: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"è¾“å…¥: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"å‚è€ƒç­”æ¡ˆ: {reference}\")\n",
    "    \n",
    "    # ç”Ÿæˆé¢„æµ‹\n",
    "    # base_output = generate_response(base_model, instruction, prompt)#######################################for sft prompt\n",
    "    # ft_output = generate_response(finetuned_model, instruction, prompt)#######################################for sft prompt\n",
    "    base_output = generate_response(base_model, prompt)\n",
    "    ft_output = generate_response(finetuned_model, prompt)\n",
    "    # æ–°å¢ï¼šè®¡ç®—å›°æƒ‘åº¦\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # åˆ†è¯å¤„ç†\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEUä¼˜åŒ–ï¼šä½¿ç”¨BLEU-2+å¹³æ»‘å‡½æ•°\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # ä½¿ç”¨BLEU-2\n",
    "        smoothing_function=smoother.method1  # æ·»åŠ å¹³æ»‘\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGEä¼˜åŒ–ï¼šä½¿ç”¨åˆ†è¯åçš„æ–‡æœ¬å¹¶å¢åŠ ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # æ–°å¢æ‰“å°å†…å®¹ï¼šå¯¹æ¯”ç”Ÿæˆç»“æœä¸å‚è€ƒç­”æ¡ˆ\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} åŸå§‹å†…å®¹ {'='*20}\")\n",
    "    # print(f\"ã€æŒ‡ä»¤ã€‘: {instruction}\")\n",
    "    # print(f\"ã€è¾“å…¥ã€‘: {prompt}\")\n",
    "    print(f\"ã€å‚è€ƒå›ç­”ã€‘: {reference}\")\n",
    "    print(f\"\\nã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: {base_output}\")\n",
    "    print(f\"ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: {ft_output}\")\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡æ‰“å°ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} è¯„ä¼°ç»“æœ {'='*20}\")\n",
    "    print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # æ‰“å°å›°æƒ‘åº¦ç»“æœ\n",
    "    print(f\"Perplexity - åŸå§‹: {base_ppl:.2f}, å¾®è°ƒ: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"æ ·æœ¬{i+1}è¯„ä¼°ç»“æœ:\")\n",
    "    # print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
