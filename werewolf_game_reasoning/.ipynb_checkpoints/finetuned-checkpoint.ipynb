{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9fd2-5eba-41ea-bc20-b92db066e42a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #æ¨¡å‹ä¸‹è½½\n",
    "# from modelscope import snapshot_download\n",
    "# # model_dir = snapshot_download('Qwen/Qwen2.5-1.5B')\n",
    "# model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f73fc8-3aa8-4853-b0fa-7b16c52b1709",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:02:55.884951Z",
     "iopub.status.busy": "2025-05-04T03:02:55.884548Z",
     "iopub.status.idle": "2025-05-04T03:02:58.647290Z",
     "shell.execute_reply": "2025-05-04T03:02:58.646837Z",
     "shell.execute_reply.started": "2025-05-04T03:02:55.884912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"train_zh.csv\", encoding='utf-8')\n",
    "# df_split = np.array_split(df, 3)  # æ‹†åˆ†æˆ 3 ä»½\n",
    "\n",
    "# # ä¿å­˜æ‹†åˆ†åçš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "# for i, split_df in enumerate(df_split):\n",
    "#     split_df.to_csv(f\"train_zh_part_{i+1}.csv\", index=False, encoding='utf-8')\n",
    "# # train_zh_part_1.csv (4045 rows)\n",
    "# # train_zh_part_2.csv (4045 rows)\n",
    "# # train_zh_part_3.csv (4045 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03c8411-9f04-48f1-9596-a8bd17547602",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T03:05:18.053257Z",
     "iopub.status.busy": "2025-05-04T03:05:18.052959Z",
     "iopub.status.idle": "2025-05-04T03:05:18.805293Z",
     "shell.execute_reply": "2025-05-04T03:05:18.804863Z",
     "shell.execute_reply.started": "2025-05-04T03:05:18.053241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5595\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š7482\n",
      "æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š5385\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# # df=pd.read_csv(\"train_zh_part_1.csv\", encoding='utf-8')\n",
    "# # df\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_1.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_2.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")\n",
    "\n",
    "# max_length = 0\n",
    "# with open('train_zh_part_3.csv', 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         # å°†å½“å‰è¡Œçš„æ‰€æœ‰å­—æ®µæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²\n",
    "#         line = ''.join(row)\n",
    "#         current_length = len(line)\n",
    "#         if current_length > max_length:\n",
    "#             max_length = current_length\n",
    "# print(f\"æœ€é•¿çš„è¡Œæ•°æ®å­—æ•°ä¸ºï¼š{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306333f9-be29-4f03-b1df-a64fdf970b68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:49:56.505151Z",
     "iopub.status.busy": "2025-05-04T09:49:56.504828Z",
     "iopub.status.idle": "2025-05-04T09:50:06.310620Z",
     "shell.execute_reply": "2025-05-04T09:50:06.310110Z",
     "shell.execute_reply.started": "2025-05-04T09:49:56.505134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Collecting swanlab\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/43/95/52d0c012eb941bb6df31f31a4b3ee27536eb73191dce4adec2ded7d27c23/swanlab-0.5.7-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.3/200.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.35.49 (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/84/d9/1bd6c2a6c3d3bf1d8b0be52c39230bd1e14bb55b7ecc04f42fcb68b27343/boto3-1.38.8-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/b4/66/e5a314d1e868cd35ec5c5d11360387c2a85e8d408f084616337f1a282c61/botocore-1.38.8-py3-none-any.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (69.5.1)\n",
      "Collecting swankit==0.1.7 (from swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/74/75/8152b1f1a3034c3ba2369133264ddf9850df62b3ba2b75fd7c104d80090f/swankit-0.1.7-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3>=1.35.49->swanlab)\n",
      "  Downloading https://repo.huaweicloud.com/repository/pypi/packages/89/64/d2b49620039b82688aeebd510bd62ff4cdcdb86cbf650cc72ae42c5254a3/s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.570.86)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: swankit, botocore, s3transfer, boto3, swanlab\n",
      "Successfully installed boto3-1.38.8 botocore-1.38.8 s3transfer-0.12.0 swankit-0.1.7 swanlab-0.5.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install swanlab -i https://pypi.org/simple  # ä½¿ç”¨å®˜æ–¹PyPIæº\n",
    "!pip install swanlab -i https://repo.huaweicloud.com/repository/pypi/simple/\n",
    "from swanlab.integration.transformers import SwanLabCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000bed-d44b-4add-be16-882cbb3ccb29",
   "metadata": {},
   "source": [
    "# config(need to run before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac912852",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:40:16.730493Z",
     "iopub.status.busy": "2025-05-04T09:40:16.730186Z",
     "iopub.status.idle": "2025-05-04T09:40:16.738683Z",
     "shell.execute_reply": "2025-05-04T09:40:16.738253Z",
     "shell.execute_reply.started": "2025-05-04T09:40:16.730459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset  # æ­£ç¡®çš„å°å†™å¯¼å…¥\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "# åŸºç¡€é…ç½®\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\" #å‚æ•°é‡å’Œ gpt2 ä¸€æ ·.\n",
    "# basic_model_path = \"./lora_basic_with_all_loss\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "basic_model_path = \"./lora_basic_deepseek_r1\"     # åŸºç¡€å¾®è°ƒç»“æœ\n",
    "sft_model_path = \"./lora_sft\"        # SFTå¾®è°ƒç»“æœ\n",
    "# merged_model_path = \"./lora_basic_sft\"  # åˆå¹¶æ¨¡å‹\n",
    "device_map = \"auto\"\n",
    "\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# moda local model\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/models/Qwen/Qwen2.5-1.5B\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        truncation=False,   # ç¦ç”¨æˆªæ–­\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # å¢å¤§ LoRA çŸ©é˜µç§© -------------------> next time to 8 #é™ä½ç§©å€¼\n",
    "    lora_alpha=32,  # è°ƒæ•´ alpha å€¼ ------------------> next time to 32 #ä¿æŒalpha/r=4çš„æ¯”ä¾‹\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],,  # æ‰©å±•ç›®æ ‡æ¨¡å—\n",
    "    target_modules = [\"c_attn\", \"mlp.down_proj\", \"mlp.up_proj\"],  # qwen, å¢å¼ºç‰¹å¾æå–èƒ½åŠ›\n",
    "    lora_dropout=0.2,  # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ ---------------> next time to 0.2 #å¢åŠ Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode=False,  # ç¡®ä¿å¤„äºè®­ç»ƒæ¨¡å¼[5](@ref)\n",
    "    # modules_to_save=[\"lm_head\"]  # å…è®¸åç»­å¾®è°ƒæ—¶æ›´æ–°å¤´éƒ¨[10](@ref)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc816-2b9d-4856-8da6-ac963cf410a5",
   "metadata": {},
   "source": [
    "# basic knowledge(752 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb65dcf5-f8bd-4d59-8c02-3494327113b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:43:34.893508Z",
     "iopub.status.busy": "2025-05-04T09:43:34.893191Z",
     "iopub.status.idle": "2025-05-04T09:43:35.784740Z",
     "shell.execute_reply": "2025-05-04T09:43:35.784245Z",
     "shell.execute_reply.started": "2025-05-04T09:43:34.893486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 752/752 [00:00<00:00, 24254.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== åŸºç¡€çŸ¥è¯†å¾®è°ƒéƒ¨åˆ† ====================\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "def load_basic_knowledge_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ç¡®ä¿åªæœ‰promptå’Œresponseåˆ—\n",
    "    df = df[['prompt', 'response']]\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# åŸºç¡€çŸ¥è¯†æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_basic_data(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Prompt>{prompt}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt.replace(\"\\n\", \"\")}\n",
    "\n",
    "# åŠ è½½åŸºç¡€çŸ¥è¯†æ•°æ®é›†\n",
    "# basic_dataset = load_basic_knowledge_dataset(\"game_strategy_and_term.csv\")\n",
    "basic_dataset = load_dataset(\"csv\", data_files=\"game_strategy_and_term.csv\")[\"train\"]\n",
    "# print(basic_dataset[\"train\"])\n",
    "# basic_dataset = basic_dataset.select(range(50))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "basic_dataset = basic_dataset.map(format_basic_data, remove_columns=[\"prompt\", \"response\"])\n",
    "basic_dataset = basic_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# # # åŠ è½½å‰10è¡Œæ•°æ®\n",
    "# # def load_mini_dataset(csv_path, n_rows=2):\n",
    "# #     df = pd.read_csv(csv_path, nrows=n_rows)\n",
    "# #     # print(df)\n",
    "# #     return Dataset.from_pandas(df)\n",
    "\n",
    "# # åŠ è½½è¿·ä½ æ•°æ®é›†\n",
    "# # dataset = load_mini_dataset(\"train_zh.csv\")\n",
    "# # dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "# # # åŠ è½½æ•°æ®é›†\n",
    "# dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# dataset = dataset.select(range(5))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬ [[7]] for test.\n",
    "# dataset = dataset.map(format_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2e3cc5-f51c-4afe-b1d4-d800e2cd3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T09:43:37.361741Z",
     "iopub.status.busy": "2025-05-04T09:43:37.361433Z",
     "iopub.status.idle": "2025-05-04T09:43:37.370947Z",
     "shell.execute_reply": "2025-05-04T09:43:37.370542Z",
     "shell.execute_reply.started": "2025-05-04T09:43:37.361724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>ä¸ºä»€ä¹ˆåœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œå¹³æ°‘è§’è‰²ä¸å»ºè®®å†’é™©æ‰®æ¼”é¢„è¨€å®¶è§’è‰²ï¼Ÿ</Prompt><Response>ç‹¼äººæ€æ˜¯ä¸€æ¬¾å¯Œæœ‰ç­–ç•¥æ€§çš„æ¡Œæ¸¸ï¼Œè€Œå…¶ä¸­é¢„è¨€å®¶ä½œä¸ºä¸€ä½å…³é”®çš„è§’è‰²ï¼Œå´åœ¨æ¸¸æˆä¸­é¢ä¸´ä¸€ç³»åˆ—çš„å›°å¢ƒã€‚ä¸ºä»€ä¹ˆç‹¼äººæ€å¹³æ°‘å°½é‡ä¸è¦è·³é¢„è¨€å®¶å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥æ¢è®¨ã€‚é¦–å…ˆï¼Œé¢„è¨€å®¶æ˜¯ä¸€ä½ç›¸å¯¹å¼±åŠ¿çš„ç¥èŒè§’è‰²ã€‚è¿™ä¸€ç‚¹ä¸»è¦ä½“ç°åœ¨é¢„è¨€å®¶æ— æ³•è‡ªè¯èº«ä»½ï¼Œä¹Ÿæ— æ³•è‡ªä¿çš„ç°å®ä¸­ã€‚ç®€è€Œè¨€ä¹‹ï¼Œé¢„è¨€å®¶çš„çœŸå®èº«ä»½ä»…æœ‰å¥½äººç›¸ä¿¡ï¼Œè€Œä¸€æ—¦å¥½äººå¯¹å…¶äº§ç”Ÿæ€€ç–‘ï¼Œä»–å°±æœ‰å¯èƒ½è¢«è¯¯è®¤ä¸ºæ˜¯ç‹¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„è¨€å®¶é™·å…¥äº†èº«ä»½çš„æ— æ³•è‡ªåœ†çš„å›°å¢ƒä¸­ã€‚å¥½äººä¿¡ä½ ï¼Œä½ æ‰æ˜¯é¢„è¨€å®¶ï¼›å¥½äººä¸ä¿¡ä½ ï¼Œä½ å°±æˆäº†ç‹¼ã€‚è¿™ä½¿å¾—é¢„è¨€å®¶åœ¨æ¸¸æˆä¸­é¢ä¸´ç€æ›´å¤§çš„é£é™©ã€‚å…¶æ¬¡ï¼Œå¦‚æœä½ æ˜¯ä¸€ä¸ªå¹³æ°‘ï¼Œè·³é¢„è¨€å®¶å°±ç›¸å½“äºåœ¨æ²¡æœ‰æŸ¥éªŒä¿¡æ¯çš„æƒ…å†µä¸‹å†’é™©å½’ç¥¨ã€‚åœ¨æ•´ä¸ªæ¸¸æˆä¸­ï¼Œå¹³æ°‘æ˜¯æ•°é‡æœ€åºå¤§çš„é˜µè¥ï¼Œè€Œä½œä¸ºå¹³æ°‘çš„ä½ åœ¨è·³é¢„è¨€å®¶æ—¶ï¼Œéœ€è¦åœ¨å½’ç¥¨çš„æ—¶å€™å‡ºç¤ºä¿¡æ¯æ¥è¯æ˜è‡ªå·±çš„å–„æ„ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ²¡æœ‰æŸ¥éªŒä¿¡æ¯ï¼Œä½ å°†é¢ä¸´ç€ä¸€ä¸ªæ£˜æ‰‹çš„é—®é¢˜ï¼šå¦‚ä½•åœ¨æ²¡æœ‰å®è´¨æ€§è¯æ®çš„æƒ…å†µä¸‹å‡ºç¤ºä¿¡æ¯ï¼Œè®©å…¶ä»–ç©å®¶ç›¸ä¿¡ä½ æ˜¯å¥½äººï¼Ÿæ›´å…·ä½“åœ°è¯´ï¼Œå½“ä½ é¢å¯¹ä¸€ä¸ªè‡ªç§°é¢„è¨€å®¶çš„ç©å®¶å’Œä¸€ä¸ªè‡ªç§°ç‹¼äººçš„ç©å®¶æ—¶ï¼Œä½ æ˜¯å¦èƒ½å¤Ÿåˆ†è¾¨è°æ˜¯çœŸå®çš„é¢„è¨€å®¶ï¼Ÿåœ¨æ²¡æœ‰å®é™…æŸ¥éªŒçš„æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¼šåœ¨ä¸çŸ¥ä¸è§‰ä¸­ä¸ºç‹¼äººç«™å°ï¼Œæœ€ç»ˆå¸®åŠ©ç‹¼äººæ··æ·†è§†å¬ï¼Œå°†çœŸæ­£çš„é¢„è¨€å®¶è¯¯å¯¼å‡ºå±€ã€‚ç¬¬ä¸‰ç‚¹åœ¨äºç‹¼äººçš„å¼€çœ¼è§†è§’ã€‚åœ¨æ¸¸æˆä¸­ï¼Œç‹¼äººæ˜¯æ‹¥æœ‰å¼€çœ¼é˜¶æ®µçš„ç‰¹æ®Šè§’è‰²ï¼Œä»–ä»¬èƒ½å¤Ÿé€šè¿‡è§†è§’è§‚å¯Ÿå…¶ä»–ç©å®¶çš„åŠ¨ä½œã€‚å¦‚æœä½ å†³å®šè·³é¢„è¨€å®¶ï¼Œæ„å‘³ç€ä½ éœ€è¦åœ¨ç‹¼äººçœ¼çš®åº•ä¸‹åšå‡ºä¸€ç³»åˆ—è¡ŒåŠ¨ï¼Œè¯•å›¾æ¬ºéª—ä»–ä»¬ã€‚ç„¶è€Œï¼Œå®é™…ä¸Šä½ å¯èƒ½åªæ˜¯åœ¨æ„šå¼„å¥½äººï¼Œè€Œåœ¨ç‹¼äººçš„è§†è§’ä¸­ï¼Œä½ å°±å¦‚åŒä¸€ä¸ªå°ä¸‘ã€‚è¿™æ ·çš„æ¬ºéª—ç­–ç•¥å¾ˆéš¾å¥æ•ˆï¼Œå› ä¸ºç‹¼äººæ‹¥æœ‰ç›¸å¯¹å…¨é¢çš„ä¿¡æ¯ã€‚è€Œå¦‚æœä½ è¯•å›¾å’Œç‹¼äººåˆä½œè¿›è¡Œä»£è·³ï¼Œé‚£å¿…é¡»åœ¨é«˜åº¦é»˜å¥‘çš„å‰æä¸‹ï¼Œå¦åˆ™å¾ˆå®¹æ˜“è¢«ç‹¼äººå¯Ÿè§‰ã€‚æ€»çš„æ¥è¯´ï¼Œç‹¼äººæ€ä¸­å¹³æ°‘è·³é¢„è¨€å®¶éœ€è¦è°¨æ…è¡Œäº‹ã€‚ç¼ºä¹å®è´¨æ€§çš„ä¿¡æ¯æŸ¥éªŒï¼Œå®¹æ˜“å¯¼è‡´å¹³æ°‘è‡ªèº«é™·å…¥å›°å¢ƒï¼Œç”šè‡³ä¸ºç‹¼äººåšå‡ºè´¡çŒ®ã€‚åœ¨æ¸¸æˆä¸­ï¼Œä¿æŠ¤è‡ªå·±çš„èº«ä»½ï¼Œé€šè¿‡åˆç†çš„è¡ŒåŠ¨å±•ç°å–„æ„ï¼Œæ˜¯æ›´ä¸ºå¯è¡Œçš„ç­–ç•¥ã€‚é¿å…è¿‡äºå†’é™©çš„èº«ä»½å®£ç§°ï¼Œä¿ç•™è¶³å¤Ÿçš„æ“ä½œç©ºé—´ï¼Œå°†æœ‰åŠ©äºåœ¨å¤æ‚çš„ç‹¼äººæ€æ¸¸æˆä¸­æ›´å¥½åœ°ç”Ÿå­˜ä¸‹å»ã€‚</Response>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50979269-197c-4118-8a2f-9189dbcba7ec",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:48:07.979975Z",
     "iopub.status.busy": "2025-05-04T09:48:07.979689Z",
     "iopub.status.idle": "2025-05-04T09:48:10.904614Z",
     "shell.execute_reply": "2025-05-04T09:48:10.904161Z",
     "shell.execute_reply.started": "2025-05-04T09:48:07.979960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,702,208 || all params: 1,548,416,512 || trainable%: 0.3037\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# # åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "# print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "# basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8dbca1f-2da3-473c-b0f7-188f7e38df99",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:55:40.202425Z",
     "iopub.status.busy": "2025-05-03T12:55:40.202120Z",
     "iopub.status.idle": "2025-05-03T12:55:40.205091Z",
     "shell.execute_reply": "2025-05-03T12:55:40.204511Z",
     "shell.execute_reply.started": "2025-05-03T12:55:40.202408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[\"train\"])  # First 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f597930a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:48:10.905570Z",
     "iopub.status.busy": "2025-05-04T09:48:10.905329Z",
     "iopub.status.idle": "2025-05-04T09:48:11.796639Z",
     "shell.execute_reply": "2025-05-04T09:48:11.796099Z",
     "shell.execute_reply.started": "2025-05-04T09:48:10.905556Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676/676 [00:00<00:00, 1032.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:00<00:00, 2305.53 examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SwanLabCallback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      3\u001b[39m basic_tokenized = basic_dataset.map(tokenize_function, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m basic_training_args = TrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=basic_model_path,\n\u001b[32m      7\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„epoch\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     save_steps=\u001b[32m50\u001b[39m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m swanlab_callback = \u001b[43mSwanLabCallback\u001b[49m()\n\u001b[32m     21\u001b[39m basic_trainer = Trainer(\n\u001b[32m     22\u001b[39m     model=model,\n\u001b[32m     23\u001b[39m     args=basic_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     callbacks=[swanlab_callback]\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m basic_trainer.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'SwanLabCallback' is not defined"
     ]
    }
   ],
   "source": [
    "# åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ\n",
    "print(\"å¼€å§‹åŸºç¡€çŸ¥è¯†å¾®è°ƒè®­ç»ƒ...\")\n",
    "basic_tokenized = basic_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "basic_training_args = TrainingArguments(\n",
    "    output_dir=basic_model_path,\n",
    "    num_train_epochs=3,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„epoch\n",
    "    per_device_train_batch_size=4,     # -> next time to 4\n",
    "    gradient_accumulation_steps=8,      # -> next time to 8   # ç­‰æ•ˆæ‰¹é‡=32\n",
    "    learning_rate=2e-4,  # åŸºç¡€çŸ¥è¯†å¾®è°ƒå¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    eval_steps=10,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "swanlab_callback = SwanLabCallback()\n",
    "\n",
    "basic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=basic_training_args,\n",
    "    train_dataset=basic_tokenized[\"train\"],\n",
    "    eval_dataset=basic_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "basic_trainer.train()\n",
    "print(\"åŸºç¡€çŸ¥è¯†å¾®è°ƒå®Œæˆ!\")\n",
    "\n",
    "# æ–°å¢ä¿å­˜é€»è¾‘\n",
    "# basic_model_path = \"./lora_basic\"  # æŒ‡å®šåŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„\n",
    "model.save_pretrained(basic_model_path)  # ä¿å­˜LoRAé€‚é…å™¨å‚æ•°[8](@ref)\n",
    "tokenizer.save_pretrained(basic_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614049ff-4b8c-4046-b031-976e750edf53",
   "metadata": {},
   "source": [
    "# SFT (12134 rows, train 10921, test 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f406ff5-6068-43e5-bec1-9a9292ea4aef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T08:44:24.508360Z",
     "iopub.status.busy": "2025-05-04T08:44:24.507931Z",
     "iopub.status.idle": "2025-05-04T08:44:27.435626Z",
     "shell.execute_reply": "2025-05-04T08:44:27.435186Z",
     "shell.execute_reply.started": "2025-05-04T08:44:24.508341Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1,548,416,512 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# åœ¨SFTè®­ç»ƒä»£ç å‰é‡æ–°åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½åŸºç¡€å¾®è°ƒç»“æœï¼š\n",
    "# é‡æ–°åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆé‡è¦ï¼ï¼‰\n",
    "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # bnb_4bit_compute_dtype=torch.float16  # å¼ºåˆ¶ä½¿ç”¨ FP16 è®¡ç®—\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # ä»åŸºç¡€å¾®è°ƒä¿å­˜è·¯å¾„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # gradient_checkpointing=True,#å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µLoRAå‚æ•°\n",
    "model = get_peft_model(base_model, LoraConfig.from_pretrained(basic_model_path))\n",
    "model.print_trainable_parameters()  # éªŒè¯å‚æ•°åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed4908d-b6fd-4554-b9a0-c57b4a922cc0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T08:46:19.494270Z",
     "iopub.status.busy": "2025-05-04T08:46:19.493992Z",
     "iopub.status.idle": "2025-05-04T08:46:20.275466Z",
     "shell.execute_reply": "2025-05-04T08:46:20.274969Z",
     "shell.execute_reply.started": "2025-05-04T08:46:19.494255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 765.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==================== SFTå¾®è°ƒéƒ¨åˆ† ====================\n",
    "# SFTæ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def format_sft_data(example):\n",
    "    system_prompt = example[\"instruction\"]\n",
    "    user_input = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = f\"<Instruction>{system_prompt}</Instruction>\\n<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    # full_prompt = f\"<Prompt>{user_input}</Prompt>\\n<Response>{response}</Response>\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# åŠ è½½SFTæ•°æ®é›†\n",
    "sft_dataset = load_dataset(\"csv\", data_files=\"train_zh.csv\")[\"train\"]\n",
    "# print(sft_dataset)\n",
    "sft_dataset = sft_dataset.select(range(2))  # é€‰æ‹©å‰5ä¸ªæ ·æœ¬\n",
    "sft_dataset = sft_dataset.map(format_sft_data, remove_columns=[\"instruction\", \"prompt\", \"response\", \"meta\"])\n",
    "sft_dataset = sft_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f97b54-1663-4026-b9b4-99199ea124a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T08:46:21.591851Z",
     "iopub.status.busy": "2025-05-04T08:46:21.591528Z",
     "iopub.status.idle": "2025-05-04T08:46:21.597074Z",
     "shell.execute_reply": "2025-05-04T08:46:21.596642Z",
     "shell.execute_reply.started": "2025-05-04T08:46:21.591830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Prompt>åœ¨æœ¬åœºæ¸¸æˆä¸­ï¼Œä½ ç›®å‰å·²çŸ¥ä»¥ä¸‹ä¿¡æ¯ï¼š\\n1. è§’è‰²è®¾å®šï¼š\\nä½ æ˜¯1å·ç©å®¶ã€‚\\nä½ çš„èº«ä»½æ˜¯ï¼šç‹¼äººã€‚\\nä½ éœ€è¦å…¶ä»–ç‹¼äººåˆä½œï¼Œæ¯æ™šé€‰æ‹©ä¸€ä¸ªæ‘æ°‘çŒæ€ï¼Œä½ çš„ç›®æ ‡æ˜¯éšè—ä½ çš„èº«ä»½ï¼Œå¹¶è¯¯å¯¼å…¶ä»–ç©å®¶ï¼Œç›´è‡³ç‹¼äººè·å¾—æ¸¸æˆçš„èƒœåˆ©ã€‚\\n2. å®¢è§‚ä¿¡æ¯ï¼š\\n- æ¸¸æˆè¿›ç¨‹ï¼šç›®å‰æ¸¸æˆè¿›è¡Œåˆ°ç¬¬2è½®ã€‚\\n- å½“å‰å­˜æ´»çš„ç©å®¶æœ‰ï¼š1å·ï¼Œ3å·ï¼Œ4å·ï¼Œ5å·ï¼Œ6å·ï¼Œåªèƒ½åœ¨ä»¥ä¸Šç©å®¶ä¸­é€‰æ‹©è¿›è¡Œæ€å®³\\n-ç‹¼äººä¸ºï¼š1,2å·ç©å®¶ã€‚\\nä½ æ˜¯ç¬¬ä¸€ä¸ªè¡ŒåŠ¨çš„ç‹¼äººï¼Œè¯·é€‰æ‹©ä½ çš„æ€å®³ç›®æ ‡ã€‚\\n- æŠ•ç¥¨æƒ…å†µï¼šç¬¬1è½®æŠ•ç¥¨è®°å½•ï¼š1å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n2å·ç©å®¶æŠ•ç»™ï¼š3å·ç©å®¶ï¼›\\n3å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n4å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n5å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\n6å·ç©å®¶æŠ•ç»™ï¼š2å·ç©å®¶ï¼›\\nç»“æœï¼š2å·ç©å®¶è¢«æŠ•ç¥¨å‡ºå±€ã€‚\\n\\n3. ä¸»è§‚ä¿¡æ¯ï¼š\\n\\n- ç¬¬1è½®æ‰€æœ‰ç©å®¶å‘è¨€ï¼š\\n**5å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œæ²¡æœ‰ä¿¡æ¯ï¼Œå¬åç½®ä½é¢„è¨€å®¶æŠ¥æŸ¥éªŒã€‚\\n**6å·ç©å®¶**ï¼š6å·æ‘æ°‘ç‰Œã€‚æ˜¨æ™š7å·æ­»äº¡ã€‚1å·æœ‰å¯èƒ½æ˜¯ç‹¼äººã€‚å¬1å·å‘è¨€ã€‚\\n**1å·ç©å®¶**ï¼š5å·ç¬¬ä¸€ä¸ªå‘è¨€ï¼Œæˆ‘è®¤ä¸ºç®€çŸ­ä¸€ç‚¹ä¹Ÿåˆç†ï¼Œä½†æ˜¯æˆ‘è®¤ä¸º6å·æ˜¯ç‹¼äººï¼Œè¸©æˆ‘1å·æ˜¯ä¸ºäº†æ‰¾äººæŠ—æ¨ã€‚å¦‚æœåé¢é¢„è¨€å®¶æ²¡å½’ç¥¨ï¼Œæˆ‘ä¼šå»æŠ•6å·ã€‚\\n**2å·ç©å®¶**ï¼šæˆ‘æ˜¯ä¸€å¼ å¥½äººç‰Œï¼Œå‰ç½®ä½6å·è·Ÿ1å·é€‰æ‹©äº’ç›¸æ¶æ‰“ï¼Œæˆ‘è‚¯å®šæ˜¯äº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·ã€‚ä¸¤å¼ ç‰Œéƒ½æ˜¯ç›¸å½“äºåˆ’æ°´çš„çŠ¶æ€ã€‚å¬åé¢ç©å®¶çš„å‘è¨€å§ï¼\\n**3å·ç©å®¶**ï¼š2å·æŸ¥æ€ã€‚2å·æœ¬è½®çš„â€œäº‹ä¸å…³å·±é«˜é«˜æŒ‚èµ·â€è¿™æ®µå‘è¨€è·Ÿè®¤ç‹¼æ²¡æœ‰ä»»ä½•åŒºåˆ«ï¼Œæˆ‘ç›¸ä¿¡ä»Šå¤©2å·è‚¯å®šä¼šå‡ºå±€çš„ã€‚ä»Šå¤©å®ˆå«å®ˆæˆ‘ä¸€è½®ï¼Œç¡®ä¿æˆ‘æ˜å¤©çš„æŸ¥éªŒä¿¡æ¯èƒ½å¦‚å®æŠ¥å‡ºæ¥ã€‚å…¶æ¬¡ï¼Œä»2å·çš„å‘è¨€ä¸­å¯ä»¥æ¨ç†å‡º1å·å’Œ6å·é‡Œé¢åº”è¯¥æ²¡æœ‰2å·çš„ç‹¼åŒä¼´ã€‚å¦‚æœ2å·çš„ç‹¼åŒä¼´è¢«æ”»å‡»ï¼Œé‚£ä¹ˆ2å·ä¸å¤ªä¼šåªæ˜¯ä¸€ä¸ªçœ‹æˆçš„å§¿æ€ï¼Œè¯´è·Ÿæˆ‘æ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¦‚æœ1å·å’Œ6å·é‡Œé¢æœ‰ç‹¼äººæˆ‘è®¤ä¸º2å·ç‹¼äººåº”è¯¥å»åšç‚¹äº‹æƒ…ä»è€Œä¸ºè‡ªå·±çš„ç‹¼é˜Ÿåšè´¡çŒ®ï¼Œä½†æ˜¯2å·å¹¶æ²¡æœ‰è¿™ä¹ˆåšï¼Œæ‰€ä»¥ä»2å·çš„è§†è§’ï¼Œæˆ‘å¯ä»¥åˆ¤æ–­å‡º1å·å’Œ6å·æ˜¯ä¸¤ä¸ªå¥½äººã€‚ä»Šå¤©å…¨ç¥¨æ‰“é£2å·ã€‚\\n**4å·ç©å®¶**ï¼šæˆ‘æ˜¯æ‘æ°‘ï¼Œ3å·æ˜¯å•è¾¹é¢„è¨€å®¶ï¼Œä»Šå¤©æŠ•2å·æŸ¥æ€ã€‚å¦‚æœçœŸçš„åƒ3å·ç©å®¶æ‰€è¯´ï¼Œèƒ½ä»2å·çš„å‘è¨€ä¸­ï¼Œæ¨å‡º1å·å’Œ6å·ç©å®¶é‡Œé¢åº”è¯¥æ²¡æœ‰2å·ç©å®¶çš„ç‹¼åŒä¼´ã€‚æˆ‘è‡ªå·±æ¸…æ¥šæˆ‘çš„åº•ç‰Œèº«ä»½æ˜¯ä¸€å¼ æ°‘ç‰Œï¼Œé‚£ä¹ˆæœ€åä¸€å¼ ç‹¼åªèƒ½æ˜¯5å·ï¼Œä½†æ˜¯5å·åœ¨å‰ç½®ä½å‘è¨€è·³æ‘æ°‘åˆ’æ°´ï¼Œæ— ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å®šä¹‰ä»–æ˜¯å¦æ˜¯çœŸç‹¼äººã€‚å®ˆå«ä»Šå¤©å¯ä»¥å®ˆæŠ¤3å·å•è¾¹é¢„è¨€å®¶ç¡®ä¿å¹³å®‰å¤œã€‚ä»Šå¤©å¥½äººå…¨ç¥¨å‡º2å·ã€‚\\n\\nä½ ç›®å‰æ˜¯1å·ç‹¼äººã€‚è¯·ç»¼åˆè§’è‰²è®¾å®šã€å®¢è§‚ä¿¡æ¯å’Œä¸»è§‚ä¿¡æ¯ï¼ˆå®¢è§‚ä¿¡æ¯ä¸€å®šä¸ºçœŸï¼Œä¸»è§‚ä¿¡æ¯ä¸ä¸€å®šçœŸå®ï¼‰ï¼Œæ€è€ƒåœ¨åœºå¥½äººçš„çœŸå®åº•ç‰Œèº«ä»½ï¼Œé€‰æ‹©ä½ è¦æ€å®³çš„ç©å®¶ï¼Œè¯·ç”¨å…³é”®å­—ä¸º\\'æ€å®³\\'ã€â€˜åŸå› â€˜çš„jsonæ ¼å¼è¾“å‡ºï¼Œç›´æ¥è¾“å‡ºç©å®¶ç¼–å·ï¼Œä¸é€‰æ‹©æ€å®³ä»»ä½•äººè¾“å‡ºå¦ã€‚\\n\\n</Prompt>\\n<Response>{\"æ€å®³\": \"3\", \"åŸå› \": \"æˆ‘æ‰“ç®—æ€æ‰3å·ç©å®¶ï¼Œå› ä¸ºä»–å£°ç§°è‡ªå·±æ˜¯é¢„è¨€å®¶ï¼Œå¦‚æœä»–çœŸçš„æ˜¯é¢„è¨€å®¶ï¼Œé‚£ä¹ˆä»–çš„èƒ½åŠ›å¯èƒ½ä¼šå¨èƒåˆ°æˆ‘ä»¬ç‹¼äººçš„èº«ä»½ã€‚\"}</Response>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_dataset['train']['text'][0]\n",
    "# print(tokenizer.model_max_length)  # æŸ¥çœ‹æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527364c-71d8-4c15-8095-a5458501ea8c",
   "metadata": {},
   "source": [
    "# çˆ†æ˜¾å­˜, éœ€è¦å°†æ•°æ®é›† cut ä¸€åŠè¯•è¯•, ä¸»è¦ response ä¸èƒ½å¤Ÿè¢«æˆªæ–­, æ‰€ä»¥ä¸èƒ½ truncation. æœ€é•¿çš„è¾“å…¥åºåˆ—æ˜¯ 7482."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d65a307-2f1f-4d04-8ff4-40f0322ff144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T08:49:55.414814Z",
     "iopub.status.busy": "2025-05-04T08:49:55.414537Z",
     "iopub.status.idle": "2025-05-04T08:50:07.661365Z",
     "shell.execute_reply": "2025-05-04T08:50:07.660747Z",
     "shell.execute_reply.started": "2025-05-04T08:49:55.414798Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32365/2090994093.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 98.25 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 145.42 examples/s]\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/mnt/workspace/llm-project/werewolf_game_reasoning/swanlog/run-20250504_164958-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ‘‹ Hi \u001b[1m\u001b[39mckkai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m./lora_sft\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸ  View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/rqrh5kiyn0eujfd19rco7\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@ckkai/werewolf_game_reasoning/runs/rqrh5kiyn0eujfd19rco7\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     59\u001b[39m sft_trainer = Trainer(\n\u001b[32m     60\u001b[39m     model=model,\n\u001b[32m     61\u001b[39m     args=sft_training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# callbacks=[swanlab_callback]\u001b[39;00m\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m torch.autograd.set_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43msft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# ä¿å­˜æœ€ç»ˆæ¨¡å‹\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# model.save_pretrained(\"./lora_final\")\u001b[39;00m\n\u001b[32m     72\u001b[39m tokenizer.save_pretrained(sft_model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/trainer.py:2611\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2607\u001b[39m         grad_norm = _grad_norm\n\u001b[32m   2609\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2613\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2615\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/accelerate/optimizer.py:165\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step = \u001b[38;5;28mself\u001b[39m._optimizer_patched_step_method\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerate_step_called:\n\u001b[32m    169\u001b[39m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/amp/grad_scaler.py:454\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState.READY:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m retval = \u001b[38;5;28mself\u001b[39m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n",
      "\u001b[31mAssertionError\u001b[39m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "# # åœ¨åŸºç¡€çŸ¥è¯†è®­ç»ƒç»“æŸåä¿å­˜é€‚é…å™¨å‚æ•°\n",
    "# basic_lora_weights = model.lora_A.weight.detach().clone()\n",
    "\n",
    "# # åœ¨SFTè®­ç»ƒå¼€å§‹å‰åŠ è½½å¯¹æ¯”\n",
    "# assert torch.allclose(model.lora_A.weight, basic_lora_weights), \"å‚æ•°æœªç»§æ‰¿ï¼\"\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()  # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨\n",
    "\n",
    "# SFTå¾®è°ƒè®­ç»ƒ\n",
    "def tokenize_function(examples):  # æ–°å¢å‚æ•°\n",
    "    # å‡è®¾ response ä»¥ \"<Response>\" å¼€å§‹\n",
    "    response_start_token_id = tokenizer.encode(\"<Response>\", add_special_tokens=False)[0]\n",
    "    # print(\"Response token ID:\", response_start_token_id)  # åº”ä¸ºæœ‰æ•ˆæ•°å€¼\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # padding=\"longest\",  # åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦\n",
    "        padding=\"max_length\",\n",
    "        truncation='only_first',      # ç¦ç”¨æˆªæ–­ï¼ˆéœ€ç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ â‰¤ max_lengthï¼‰\n",
    "        max_length=7500,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # labels = tokenized[\"input_ids\"].clone()\n",
    "    # for i in range(len(labels)):\n",
    "    #     # æ‰¾åˆ° <Response> çš„èµ·å§‹ä½ç½®\n",
    "    #     start_pos = (tokenized[\"input_ids\"][i] == response_start_token_id).nonzero(as_tuple=True)[0]\n",
    "    #     if len(start_pos) > 0:\n",
    "    #         labels[i, :start_pos.item()] = -100  # å¿½ç•¥ instruction + prompt çš„ loss\n",
    "    # tokenized[\"labels\"] = labels\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ç›´æ¥å¤åˆ¶è¾“å…¥ä½œä¸ºæ ‡ç­¾\n",
    "    return tokenized\n",
    "\n",
    "print(\"å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...\")\n",
    "sft_tokenized = sft_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=sft_model_path,  # æ–°ä¿å­˜è·¯å¾„\n",
    "    num_train_epochs=5,  # å¢åŠ è®­ç»ƒè½®æ¬¡\n",
    "    per_device_train_batch_size=2,  # å¢å¤§æ‰¹é‡å¤§å°\n",
    "    gradient_accumulation_steps=2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    learning_rate=2e-4,  # è°ƒæ•´å­¦ä¹ ç‡\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16_full_eval=False,  # ç¦ç”¨è¯„ä¼°é˜¶æ®µçš„æ··åˆç²¾åº¦\n",
    "    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–æ˜¾å­˜[4](@ref)\n",
    "    eval_steps=10,  # é™ä½è¯„ä¼°æ­¥æ•°ä»¥é€‚é…å°æ•°æ®é‡\n",
    "    logging_steps=1,      # æ¯ä¸ªè®­ç»ƒæ­¥éª¤è®°å½•æ—¥å¿—\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # logging_dir=\"./logs\",  # æ–°å¢TensorBoardæ—¥å¿—ç›®å½•\n",
    "    # report_to=[\"tensorboard\"],  # å¯ç”¨TensorBoardæŠ¥å‘Š\n",
    "    # load_best_model_at_end=True  # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹\n",
    ")\n",
    "\n",
    "# swanlab_callback = SwanLabCallback()\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_tokenized[\"train\"],\n",
    "    eval_dataset=sft_tokenized[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    # callbacks=[swanlab_callback]\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(True)  # å¼€å¯æ¢¯åº¦å¼‚å¸¸æ£€æµ‹\n",
    "sft_trainer.train()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# model.save_pretrained(\"./lora_final\")\n",
    "tokenizer.save_pretrained(sft_model_path)\n",
    "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {sft_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c2442f-dcc4-4855-91b6-8eef982c26c4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-03T12:27:31.699833Z",
     "iopub.status.busy": "2025-05-03T12:27:31.699539Z",
     "iopub.status.idle": "2025-05-03T12:27:31.702763Z",
     "shell.execute_reply": "2025-05-03T12:27:31.702330Z",
     "shell.execute_reply.started": "2025-05-03T12:27:31.699818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for log in trainer.state.log_history:\n",
    "#     print(log)\n",
    "# # æå–è®­ç»ƒæ—¥å¿—ï¼ˆåŒ…å« loss æˆ– train_loss å’Œ stepï¼‰\n",
    "# train_logs = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"step\" in log:\n",
    "#         if \"loss\" in log:\n",
    "#             log[\"train_loss\"] = log[\"loss\"]  # ç»Ÿä¸€å­—æ®µåä¸º train_loss\n",
    "#         train_logs.append(log)\n",
    "\n",
    "# # è½¬æ¢ä¸º DataFrame\n",
    "# train_df = pd.DataFrame(train_logs)[[\"step\", \"train_loss\"]]\n",
    "# # æå–è¯„ä¼°æ—¥å¿—ï¼ˆåŒ…å« eval_loss å’Œ stepï¼‰\n",
    "# eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log and \"step\" in log]\n",
    "# eval_df = pd.DataFrame(eval_logs)[[\"step\", \"eval_loss\"]] if eval_logs else pd.DataFrame()\n",
    "# # åˆå¹¶è®­ç»ƒå’Œè¯„ä¼°æ—¥å¿—\n",
    "# merged = pd.merge(train_df, eval_df, on=\"step\", how=\"outer\").sort_values(\"step\")\n",
    "# merged.ffill(inplace=True)  # ä½¿ç”¨ ffill() æ›¿ä»£ fillna(method=\"ffill\")\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(merged[\"step\"], merged[\"train_loss\"], 'b-', label='Training Loss')\n",
    "# if not eval_df.empty:\n",
    "#     plt.plot(merged[\"step\"], merged[\"eval_loss\"], 'r--', label='Validation Loss')\n",
    "# plt.title(\"Training Progress Analysis\")\n",
    "# plt.xlabel(\"Training Steps\")\n",
    "# plt.ylabel(\"Loss Value\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012f81d",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆä»»åŠ¡è¯„ä¼°ï¼ˆBLEU/ROUGE/METEORï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e34c76",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:31:57.834231Z",
     "iopub.status.busy": "2025-05-04T09:31:57.833932Z",
     "iopub.status.idle": "2025-05-04T09:32:05.108804Z",
     "shell.execute_reply": "2025-05-04T09:32:05.108223Z",
     "shell.execute_reply.started": "2025-05-04T09:31:57.834215Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba  # ä¸­æ–‡åˆ†è¯æ”¯æŒ\n",
    "\n",
    "# åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, basic_model_path)\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# df = pd.read_csv(\"train_zh.csv\").tail(2)\n",
    "df = pd.read_csv(\"game_strategy_and_term.csv\").tail(2)\n",
    "test_dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9c6613",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T09:33:19.456528Z",
     "iopub.status.busy": "2025-05-04T09:33:19.456065Z",
     "iopub.status.idle": "2025-05-04T09:34:38.506619Z",
     "shell.execute_reply": "2025-05-04T09:34:38.506155Z",
     "shell.execute_reply.started": "2025-05-04T09:33:19.456511Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== æ ·æœ¬1 åŸå§‹å†…å®¹ ====================\n",
      "ã€å‚è€ƒå›ç­”ã€‘: å½“å¥½äººä¸€å®šç«™å¾—å¯¹è¾¹ã€å½“ç‹¼ä¸€å®šå€’é’©çš„ç©å®¶ï¼Œæˆ–è€…å½“å¥½äººä¸€å®šä¼šç«™é”™è¾¹ã€å½“ç‹¼ä¸€å®šå†²é”‹çš„ç©å®¶\n",
      "\n",
      "ã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œæ€æ ·ç†è§£â€œç«™è¾¹æ˜ç¯â€çš„å«ä¹‰ï¼Ÿ ç­”\n",
      "ç«™è¾¹æ˜ç¯æ˜¯æŒ‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œå½“ç‹¼äººæ€å®Œä¸€ä¸ªå¥½äººåï¼Œå¥½äººé˜µè¥ä¼šç«™è¾¹ä¸€ä¸ªå¥½äººï¼Œè¿™ä¸ªå¥½äººå°±æ˜¯ç«™è¾¹æ˜ç¯ã€‚\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œæ€æ ·ç†è§£â€œç«™è¾¹æ˜ç¯â€çš„å«ä¹‰ï¼Ÿ ç­”\n",
      "ç«™è¾¹æ˜ç¯æ˜¯æŒ‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œå½“ç‹¼äººæ€å®Œä¸€ä¸ªå¥½äººåï¼Œå¥½äººé˜µè¥ä¼šç«™è¾¹ä¸€ä¸ªå¥½äººï¼Œè¿™ä¸ªå¥½äººå°±æ˜¯ç«™è¾¹æ˜\n",
      "ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œæ€æ ·ç†è§£â€œç«™è¾¹æ˜ç¯â€çš„å«ä¹‰ï¼Ÿ ç­”\n",
      "ç«™è¾¹æ˜ç¯æ˜¯æŒ‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œå½“ç‹¼äººæ€å®Œä¸€ä¸ªå¥½äººåï¼Œå¥½äººé˜µè¥ä¼šç«™è¾¹ä¸€ä¸ªå¥½äººï¼Œè¿™ä¸ªå¥½äººå°±æ˜¯ç«™è¾¹æ˜ç¯ã€‚\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œæ€æ ·ç†è§£â€œç«™è¾¹æ˜ç¯â€çš„å«ä¹‰ï¼Ÿ ç­”\n",
      "ç«™è¾¹æ˜ç¯æ˜¯æŒ‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼Œå½“ç‹¼äººæ€å®Œä¸€ä¸ªå¥½äººåï¼Œå¥½äººé˜µè¥ä¼šç«™è¾¹ä¸€ä¸ªå¥½äººï¼Œè¿™ä¸ªå¥½äººå°±æ˜¯ç«™è¾¹æ˜\n",
      "\n",
      "==================== æ ·æœ¬1 è¯„ä¼°ç»“æœ ====================\n",
      "BLEU-2 - åŸå§‹: 0.0077, å¾®è°ƒ: 0.0077\n",
      "ROUGE-2 F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "ROUGE-L F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - åŸå§‹: 22.04, å¾®è°ƒ: 22.04\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== æ ·æœ¬2 åŸå§‹å†…å®¹ ====================\n",
      "ã€å‚è€ƒå›ç­”ã€‘: è§‚æˆ˜ä¸­\n",
      "\n",
      "ã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œâ€æ ‘ä¸Šâ€œçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ____\n",
      "A. æœ‰ç‹¼äºº\n",
      "B. æœ‰å¥³å·«\n",
      "C. æœ‰é¢„è¨€å®¶\n",
      "D. æœ‰å¥³å·«å’Œé¢„è¨€å®¶\n",
      "ç­”æ¡ˆ:\n",
      "D\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œâ€ç‹¼äººâ€œçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ____\n",
      "A. æœ‰ç‹¼äºº\n",
      "B. æœ‰å¥³å·«\n",
      "C. æœ‰é¢„è¨€å®¶\n",
      "D. æœ‰å¥³å·«å’Œé¢„è¨€å®¶\n",
      "ç­”æ¡ˆ:\n",
      "A\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€\n",
      "ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œâ€æ ‘ä¸Šâ€œçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ____\n",
      "A. æœ‰ç‹¼äºº\n",
      "B. æœ‰å¥³å·«\n",
      "C. æœ‰é¢„è¨€å®¶\n",
      "D. æœ‰å¥³å·«å’Œé¢„è¨€å®¶\n",
      "ç­”æ¡ˆ:\n",
      "D\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€æ¸¸æˆçš„è¯­å¢ƒä¸­ï¼Œâ€ç‹¼äººâ€œçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ____\n",
      "A. æœ‰ç‹¼äºº\n",
      "B. æœ‰å¥³å·«\n",
      "C. æœ‰é¢„è¨€å®¶\n",
      "D. æœ‰å¥³å·«å’Œé¢„è¨€å®¶\n",
      "ç­”æ¡ˆ:\n",
      "A\n",
      "\n",
      "åœ¨â€œç‹¼äººæ€â€\n",
      "\n",
      "==================== æ ·æœ¬2 è¯„ä¼°ç»“æœ ====================\n",
      "BLEU-2 - åŸå§‹: 0.0028, å¾®è°ƒ: 0.0028\n",
      "ROUGE-2 F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "ROUGE-L F1 - åŸå§‹: 0.0000, å¾®è°ƒ: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Perplexity - åŸå§‹: 176.60, å¾®è°ƒ: 176.60\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# å…³é—­jiebaè°ƒè¯•æ—¥å¿—\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)\n",
    "\n",
    "# ä¸­æ–‡åˆ†è¯å·¥å…·ï¼ˆæ”¹ç”¨æœç´¢å¼•æ“æ¨¡å¼æå‡å¬å›ç‡ï¼‰\n",
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut_for_search(text))  # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼[[2]]\n",
    "\n",
    "# # ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "# def generate_response(model, instruction, prompt):\n",
    "#     input_text = f\"[INST] <<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ç”Ÿæˆé¢„æµ‹æ–‡æœ¬ï¼ˆå¢åŠ ç”Ÿæˆé•¿åº¦ï¼‰\n",
    "def generate_response(model, prompt):\n",
    "    input_text = f\"{prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)  # å¢åŠ ç”Ÿæˆé•¿åº¦[[1]]\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®pad_token_idï¼ˆé¿å…è­¦å‘Šï¼‰\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è®¡ç®—BLEU/ROUGEï¼ˆå¢åŠ å¹³æ»‘å‡½æ•°å’ŒROUGE-Lï¼‰\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ¨¡å‹å¯¹ç»™å®špromptå’Œresponseçš„å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # æ‹¼æ¥è¾“å…¥ä¸å“åº”\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # æ„é€ æ ‡ç­¾ï¼šä»…è®¡ç®—responseéƒ¨åˆ†çš„loss\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[:, :prompt_len] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„lossè®¡ç®—\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return torch.exp(loss).item()  # è¿”å›å›°æƒ‘åº¦\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    # instruction = example[\"instruction\"]#######################################for sft prompt\n",
    "    # print(f\"æŒ‡ä»¤: {instruction}\")\n",
    "    prompt = example[\"prompt\"]\n",
    "    # print(f\"è¾“å…¥: {prompt}\")\n",
    "    reference = example[\"response\"]\n",
    "    # print(f\"å‚è€ƒç­”æ¡ˆ: {reference}\")\n",
    "    \n",
    "    # ç”Ÿæˆé¢„æµ‹\n",
    "    # base_output = generate_response(base_model, instruction, prompt)#######################################for sft prompt\n",
    "    # ft_output = generate_response(finetuned_model, instruction, prompt)#######################################for sft prompt\n",
    "    base_output = generate_response(base_model, prompt)\n",
    "    ft_output = generate_response(finetuned_model, prompt)\n",
    "    # æ–°å¢ï¼šè®¡ç®—å›°æƒ‘åº¦\n",
    "    base_ppl = calculate_perplexity(base_model, tokenizer, prompt, reference)\n",
    "    ft_ppl = calculate_perplexity(finetuned_model, tokenizer, prompt, reference)\n",
    "    \n",
    "    # åˆ†è¯å¤„ç†\n",
    "    ref_tokens = chinese_tokenize(reference)\n",
    "    base_tokens = chinese_tokenize(base_output)\n",
    "    ft_tokens = chinese_tokenize(ft_output)\n",
    "    \n",
    "    # BLEUä¼˜åŒ–ï¼šä½¿ç”¨BLEU-2+å¹³æ»‘å‡½æ•°\n",
    "    bleu_base = sentence_bleu(\n",
    "        [ref_tokens], base_tokens, \n",
    "        weights=(0.5, 0.5),  # ä½¿ç”¨BLEU-2\n",
    "        smoothing_function=smoother.method1  # æ·»åŠ å¹³æ»‘\n",
    "    )\n",
    "    bleu_ft = sentence_bleu(\n",
    "        [ref_tokens], ft_tokens,\n",
    "        weights=(0.5, 0.5),\n",
    "        smoothing_function=smoother.method1\n",
    "    )\n",
    "    \n",
    "    # ROUGEä¼˜åŒ–ï¼šä½¿ç”¨åˆ†è¯åçš„æ–‡æœ¬å¹¶å¢åŠ ROUGE-L\n",
    "    rouge_base = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(base_tokens)\n",
    "    )\n",
    "    rouge_ft = scorer.score(\n",
    "        \" \".join(ref_tokens), \n",
    "        \" \".join(ft_tokens)\n",
    "    )\n",
    "    # æ–°å¢æ‰“å°å†…å®¹ï¼šå¯¹æ¯”ç”Ÿæˆç»“æœä¸å‚è€ƒç­”æ¡ˆ\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} åŸå§‹å†…å®¹ {'='*20}\")\n",
    "    # print(f\"ã€æŒ‡ä»¤ã€‘: {instruction}\")\n",
    "    # print(f\"ã€è¾“å…¥ã€‘: {prompt}\")\n",
    "    print(f\"ã€å‚è€ƒå›ç­”ã€‘: {reference}\")\n",
    "    print(f\"\\nã€åŸºç¡€æ¨¡å‹ç”Ÿæˆã€‘: {base_output}\")\n",
    "    print(f\"ã€å¾®è°ƒæ¨¡å‹ç”Ÿæˆã€‘: {ft_output}\")\n",
    "    \n",
    "    # è¯„ä¼°æŒ‡æ ‡æ‰“å°ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰\n",
    "    print(f\"\\n{'='*20} æ ·æœ¬{i+1} è¯„ä¼°ç»“æœ {'='*20}\")\n",
    "    print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # æ‰“å°å›°æƒ‘åº¦ç»“æœ\n",
    "    print(f\"Perplexity - åŸå§‹: {base_ppl:.2f}, å¾®è°ƒ: {ft_ppl:.2f}\")\n",
    "    print(\"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # print(f\"æ ·æœ¬{i+1}è¯„ä¼°ç»“æœ:\")\n",
    "    # print(f\"BLEU-2 - åŸå§‹: {bleu_base:.4f}, å¾®è°ƒ: {bleu_ft:.4f}\")\n",
    "    # print(f\"ROUGE-2 F1 - åŸå§‹: {rouge_base['rouge2'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rouge2'].fmeasure:.4f}\")\n",
    "    # print(f\"ROUGE-L F1 - åŸå§‹: {rouge_base['rougeL'].fmeasure:.4f}, å¾®è°ƒ: {rouge_ft['rougeL'].fmeasure:.4f}\")\n",
    "    # print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
