# 狼人杀

数据集

- 总共收集了 **331 场狼人杀游戏** 的标注数据
  - 278局9人制游戏（包括==预言家、女巫、守卫或猎人==等角色）
  - 53局7人制游戏（包括预言家、守卫或女巫）
- 测试集划分
  - 从专家标注的数据中分离出51场比赛作为测试集，用于评估投票准确性和身份预测准确性![Screenshot 2025-04-30 at 2.19.34 PM](../../Library/Application Support/typora-user-images/Screenshot 2025-04-30 at 2.19.34 PM.png)

数据结构:

- 每场游戏都包含玩家的身份分配（如狼人、村民、预言家等）、对话记录、以及玩家的决策思维过程（即“思考链”，Chain of Thought, CoT）。
- 决策思维过程被整理为 **“先思考后回应”** （think-before-respond）格式，帮助模型理解游戏每一阶段背后的逻辑。
  - action->speech->vote
  - <u>对于行动 action，输出格式为：原因 + 目标对象。</u>(村民无action) 2,698 action events
  - <u>对于发言 speech，输出格式为：对其他玩家的预期标签 + 投票意图作为提纲，再进行生成。</u> 3,759 speech data entries (exceeding 540,000 tokens)
  - <u>对于投票 vote，输出当天讨论的总结。</u> 3,875 voting records

**SFT（监督微调）数据集**

- game_strategy_and_term.csv

  - 基础知识理解数据（共380条样本）：解释术语和基本规则。

  - 高级游戏技巧问答数据game_strategy_and_term（372条样本）：涵盖复杂策略与应对方法。

- train_zh.csv

  - 真实游戏行为数据 Action（12,000条样本）：由专家玩家的游戏记录整理而成。
    - ~~通用SFT语料库（12,000条样本）：用于增强基础语言理解能力。~~(找不到)

## 训练方法

1. **监督微调阶段 (Supervised Fine-Tuning, SFT)** ：
   - 使用Qwen2.5-14b-instruct和Qwen2.5-72b-instruct作为基座模型。
   - 训练数据规模为25,000条样本，涵盖了术语解释、技巧问答、真实游戏行为以及通用对话场景。
   - 优化器使用DeepSpeed ZeRO-3，学习率为1e−6，warm-up比率为0.05，训练3轮次。
2. ~~**多代理KTO优化阶段 (Multi-Agent KTO)** ：~~
   - ~~**目标** ：通过模拟多智能体之间的博弈交互，进一步提升模型的战略决策能力。~~
   - ~~**偏好数据生成** ：在“预言家-女巫-守卫”配置的游戏中，收集了20,000条偏好数据，其中包括12,000条可接受的行为样本和8,000条不可接受的行为样本。~~
   - ~~**模型池构建** ：使用多个强基线模型参与训练，包括GPT-4o_mini、GPT-4o、微调后的Qwen2.5-14b-instruct、Llama-3.1-8B-Instruct和Qwen2.5-72b-instruct。~~
   - ~~**损失函数设计** ：基于KTO（Kahneman-Tversky Optimization）框架，对可接受和不可接受的行为分别赋予不同的权重（λD=0.7，λU=1.0），从而优化模型在复杂社交互动中的判断能力。~~
   - ~~**训练参数设置** ：继续使用DeepSpeed ZeRO-3优化器，学习率同样设定为1e−6，每设备批量大小为2，warm-up步数为150，训练20轮次。对于不同规模的模型（14B 和 72B）分别使用8块和32块A100 GPU进行训练。~~
3. **训练过程特点** ：
   - **行为克隆（Behavior Cloning）** ：利用标注数据进行初步的模仿学习，使模型掌握基本的游戏规则和术语。
   - ~~**多代理对抗训练（Multi-Agent Gameplay）** ：通过引入多样化的对手模型，提高模型在面对不同类型玩家时的适应性。~~
   - ~~**逐步偏奋试验选择（Stepwise Preference Selection）** ：采用启发式、投票式和验证式三种方式筛选高质量偏好数据，避免简单胜负结果带来的误导。~~















































# 逆转裁判

## 数据来源

- 游戏原始数据（必要）：
  - 提取游戏脚本（证词、法庭记录、证据描述）
  - 收集游戏Wiki中的案件解析（如：https://aceattorney.fandom.com/）
  - 使用游戏模拟器+OCR提取关键对话（如：nds版逆转裁判）
- 人工构造数据（增强泛化）：
  - 修改原始案件细节生成新案例（如替换时间/地点/人物关系）
  - 基于经典推理谜题构造法律场景（需保持游戏风格）

```json
{
  "instruction": "请分析以下证词与证据的矛盾点",
  "input": "证人证词：我晚上10点在公园看到被告。\n证据：公园监控显示10点已关闭",
  "output": "矛盾点：证人说10点在公园，但监控显示此时公园已关闭。建议出示监控记录反驳。"
}
```

数据增强技巧

- 添加干扰信息（无关证据/冗余对话）
- 多轮对话格式模拟法庭辩论
- 标注逻辑链标签（如：证据A → 矛盾点B → 结论C）

## 微调框架

lora

```py
# 使用Huggingface生态
from transformers import AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model

# 初始化模型
model = AutoModelForCausalLM.from_pretrained("DeepSeek-R1-Distill-Qwen-1.5B")

# 添加LoRA适配器
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)
model = get_peft_model(model, peft_config)

# 训练参数
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    fp16=True  # 开启混合精度
)
```

- **动态掩码**：对证据文本做随机遮盖，强制模型关注关键信息
- **渐进训练**：先训练简单案件，逐步增加复杂度
- **损失函数**：对矛盾点关键词（如时间/地点）添加权重

## 验证

```py
# 构建测试用例库
test_cases = [
    {
        "input": "证人：被告用右手持刀\n证据：被告是左撇子",
        "expected": "左右手使用矛盾"
    },
    # 更多案例...
]

# 评估函数
def evaluate(model, test_case):
    output = generate(model, test_case["input"])
    return check_keywords(output, test_case["expected"])

# 使用BLEU+Rouge+关键词命中率综合评分
```

#### 人工评估维度

|    评估维度    |        说明        |            示例            |
| :------------: | :----------------: | :------------------------: |
| 矛盾定位准确率 | 能否精确到具体语句 |    证词第3句与证据2矛盾    |
| 推理逻辑完整度 |  是否展示推理过程  |       A→B→C的因果链        |
| 游戏策略匹配度 |  是否符合游戏机制  | 正确选择"威慑"或"举证"时机 |

## 迭代优化

1. **Bad Case分析**：定期收集模型错误案例，针对性补充训练数据
2. **对抗训练**：让两个模型互相生成案件和反驳，提升推理深度
3. **记忆库增强**：为模型添加《逆转裁判》知识图谱（人物关系/法律条款）