The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loaded dataset with 87999 training examples
Map:   0%|          | 0/87999 [00:00<?, ? examples/s]Map:   1%|          | 1000/87999 [00:00<00:27, 3214.51 examples/s]Map:   2%|▏         | 2000/87999 [00:00<00:25, 3321.17 examples/s]Map:   3%|▎         | 3000/87999 [00:00<00:25, 3393.26 examples/s]Map:   5%|▍         | 4000/87999 [00:01<00:24, 3415.13 examples/s]Map:   6%|▌         | 5000/87999 [00:01<00:24, 3415.13 examples/s]Map:   7%|▋         | 6000/87999 [00:01<00:23, 3444.54 examples/s]Map:   8%|▊         | 7000/87999 [00:02<00:23, 3451.06 examples/s]Map:   9%|▉         | 8000/87999 [00:02<00:23, 3449.65 examples/s]Map:  10%|█         | 9000/87999 [00:02<00:22, 3467.12 examples/s]Map:  11%|█▏        | 10000/87999 [00:02<00:22, 3470.53 examples/s]Map:  13%|█▎        | 11000/87999 [00:03<00:22, 3465.66 examples/s]Map:  14%|█▎        | 12000/87999 [00:03<00:21, 3475.18 examples/s]Map:  15%|█▍        | 13000/87999 [00:03<00:21, 3484.28 examples/s]Map:  16%|█▌        | 14000/87999 [00:04<00:21, 3479.05 examples/s]Map:  17%|█▋        | 15000/87999 [00:04<00:23, 3097.27 examples/s]Map:  18%|█▊        | 16000/87999 [00:04<00:23, 3071.51 examples/s]Map:  19%|█▉        | 17000/87999 [00:05<00:23, 3035.80 examples/s]Map:  20%|██        | 18000/87999 [00:05<00:22, 3102.93 examples/s]Map:  22%|██▏       | 19000/87999 [00:05<00:21, 3196.05 examples/s]Map:  23%|██▎       | 20000/87999 [00:06<00:20, 3279.76 examples/s]Map:  24%|██▍       | 21000/87999 [00:06<00:20, 3330.10 examples/s]Map:  25%|██▌       | 22000/87999 [00:06<00:19, 3375.09 examples/s]Map:  26%|██▌       | 23000/87999 [00:06<00:19, 3396.64 examples/s]Map:  27%|██▋       | 24000/87999 [00:07<00:18, 3419.19 examples/s]Map:  28%|██▊       | 25000/87999 [00:07<00:18, 3423.52 examples/s]Map:  30%|██▉       | 26000/87999 [00:07<00:18, 3442.90 examples/s]Map:  31%|███       | 27000/87999 [00:08<00:17, 3458.37 examples/s]Map:  32%|███▏      | 28000/87999 [00:08<00:17, 3474.25 examples/s]Map:  33%|███▎      | 29000/87999 [00:08<00:17, 3456.84 examples/s]Map:  34%|███▍      | 30000/87999 [00:08<00:16, 3466.46 examples/s]Map:  35%|███▌      | 31000/87999 [00:09<00:16, 3475.43 examples/s]Map:  36%|███▋      | 32000/87999 [00:09<00:16, 3464.82 examples/s]Map:  38%|███▊      | 33000/87999 [00:09<00:16, 3392.83 examples/s]Map:  39%|███▊      | 34000/87999 [00:10<00:15, 3412.81 examples/s]Map:  40%|███▉      | 35000/87999 [00:10<00:15, 3430.28 examples/s]Map:  41%|████      | 36000/87999 [00:10<00:15, 3461.75 examples/s]Map:  42%|████▏     | 37000/87999 [00:10<00:14, 3469.22 examples/s]Map:  43%|████▎     | 38000/87999 [00:11<00:14, 3463.46 examples/s]Map:  44%|████▍     | 39000/87999 [00:11<00:14, 3470.84 examples/s]Map:  45%|████▌     | 40000/87999 [00:11<00:13, 3452.95 examples/s]Map:  47%|████▋     | 41000/87999 [00:12<00:13, 3454.79 examples/s]Map:  48%|████▊     | 42000/87999 [00:12<00:13, 3476.04 examples/s]Map:  49%|████▉     | 43000/87999 [00:12<00:13, 3460.40 examples/s]Map:  50%|█████     | 44000/87999 [00:12<00:12, 3468.45 examples/s]Map:  51%|█████     | 45000/87999 [00:13<00:12, 3488.27 examples/s]Map:  52%|█████▏    | 46000/87999 [00:13<00:12, 3494.03 examples/s]Map:  53%|█████▎    | 47000/87999 [00:13<00:11, 3475.35 examples/s]Map:  55%|█████▍    | 48000/87999 [00:14<00:11, 3496.82 examples/s]Map:  56%|█████▌    | 49000/87999 [00:14<00:12, 3083.68 examples/s]Map:  57%|█████▋    | 50000/87999 [00:14<00:11, 3192.12 examples/s]Map:  58%|█████▊    | 51000/87999 [00:15<00:11, 3281.51 examples/s]Map:  59%|█████▉    | 52000/87999 [00:15<00:10, 3342.46 examples/s]Map:  60%|██████    | 53000/87999 [00:15<00:10, 3380.57 examples/s]Map:  61%|██████▏   | 54000/87999 [00:15<00:09, 3412.98 examples/s]Map:  63%|██████▎   | 55000/87999 [00:16<00:09, 3437.12 examples/s]Map:  64%|██████▎   | 56000/87999 [00:16<00:09, 3453.44 examples/s]Map:  65%|██████▍   | 57000/87999 [00:16<00:08, 3466.21 examples/s]Map:  66%|██████▌   | 58000/87999 [00:17<00:08, 3477.66 examples/s]Map:  67%|██████▋   | 59000/87999 [00:17<00:08, 3449.35 examples/s]Map:  68%|██████▊   | 60000/87999 [00:17<00:08, 3464.75 examples/s]Map:  69%|██████▉   | 61000/87999 [00:17<00:07, 3467.42 examples/s]Map:  70%|███████   | 62000/87999 [00:18<00:07, 3475.30 examples/s]Map:  72%|███████▏  | 63000/87999 [00:18<00:07, 3489.24 examples/s]Map:  73%|███████▎  | 64000/87999 [00:18<00:06, 3489.49 examples/s]Map:  74%|███████▍  | 65000/87999 [00:19<00:06, 3481.64 examples/s]Map:  75%|███████▌  | 66000/87999 [00:19<00:06, 3485.74 examples/s]Map:  76%|███████▌  | 67000/87999 [00:19<00:06, 3457.54 examples/s]Map:  77%|███████▋  | 68000/87999 [00:19<00:05, 3460.42 examples/s]Map:  78%|███████▊  | 69000/87999 [00:20<00:05, 3477.46 examples/s]Map:  80%|███████▉  | 70000/87999 [00:20<00:05, 3471.10 examples/s]Map:  81%|████████  | 71000/87999 [00:20<00:04, 3471.05 examples/s]Map:  82%|████████▏ | 72000/87999 [00:21<00:04, 3476.16 examples/s]Map:  83%|████████▎ | 73000/87999 [00:21<00:04, 3468.47 examples/s]Map:  84%|████████▍ | 74000/87999 [00:21<00:04, 3478.43 examples/s]Map:  85%|████████▌ | 75000/87999 [00:21<00:03, 3490.56 examples/s]Map:  86%|████████▋ | 76000/87999 [00:22<00:03, 3485.64 examples/s]Map:  88%|████████▊ | 77000/87999 [00:22<00:03, 3480.58 examples/s]Map:  89%|████████▊ | 78000/87999 [00:22<00:02, 3481.70 examples/s]Map:  90%|████████▉ | 79000/87999 [00:23<00:02, 3478.48 examples/s]Map:  91%|█████████ | 80000/87999 [00:23<00:02, 3468.86 examples/s]Map:  92%|█████████▏| 81000/87999 [00:23<00:02, 3483.60 examples/s]Map:  93%|█████████▎| 82000/87999 [00:23<00:01, 3478.29 examples/s]Map:  94%|█████████▍| 83000/87999 [00:24<00:01, 3474.96 examples/s]Map:  95%|█████████▌| 84000/87999 [00:24<00:01, 3099.21 examples/s]Map:  97%|█████████▋| 85000/87999 [00:24<00:00, 3211.91 examples/s]Map:  98%|█████████▊| 86000/87999 [00:25<00:00, 3279.93 examples/s]Map:  99%|█████████▉| 87000/87999 [00:25<00:00, 3320.44 examples/s]Map: 100%|██████████| 87999/87999 [00:25<00:00, 3363.12 examples/s]Map: 100%|██████████| 87999/87999 [00:26<00:00, 3375.41 examples/s]
Map:   0%|          | 0/10749 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10749 [00:00<00:02, 3336.79 examples/s]Map:  19%|█▊        | 2000/10749 [00:00<00:02, 3422.78 examples/s]Map:  28%|██▊       | 3000/10749 [00:00<00:02, 3421.66 examples/s]Map:  37%|███▋      | 4000/10749 [00:01<00:01, 3459.56 examples/s]Map:  47%|████▋     | 5000/10749 [00:01<00:01, 3476.11 examples/s]Map:  56%|█████▌    | 6000/10749 [00:01<00:01, 3469.20 examples/s]Map:  65%|██████▌   | 7000/10749 [00:02<00:01, 3473.76 examples/s]Map:  74%|███████▍  | 8000/10749 [00:02<00:00, 3487.24 examples/s]Map:  84%|████████▎ | 9000/10749 [00:02<00:00, 3469.02 examples/s]Map:  93%|█████████▎| 10000/10749 [00:02<00:00, 3479.11 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3486.02 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3375.44 examples/s]
Map:   0%|          | 0/10657 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10657 [00:00<00:02, 3394.39 examples/s]Map:  19%|█▉        | 2000/10657 [00:00<00:02, 3346.65 examples/s]Map:  28%|██▊       | 3000/10657 [00:00<00:02, 3411.24 examples/s]Map:  38%|███▊      | 4000/10657 [00:01<00:01, 3439.99 examples/s]Map:  47%|████▋     | 5000/10657 [00:01<00:01, 3443.28 examples/s]Map:  56%|█████▋    | 6000/10657 [00:01<00:01, 3461.18 examples/s]Map:  66%|██████▌   | 7000/10657 [00:02<00:01, 3446.66 examples/s]Map:  75%|███████▌  | 8000/10657 [00:02<00:00, 3144.31 examples/s]Map:  84%|████████▍ | 9000/10657 [00:02<00:00, 3183.87 examples/s]Map:  94%|█████████▍| 10000/10657 [00:03<00:00, 3254.87 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3261.55 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3246.98 examples/s]
/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Preprocessed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 87999
    })
    validation: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10749
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10657
    })
})
Using traditional fine-tuning on the final layers instead of LoRA...
Unfroze the final 12 transformer blocks out of 24 total
Unfroze the language model head
trainable params: 202,621,952 || all params: 354,827,264 || trainable%: 57.1044
Starting training...
  0%|          | 0/2750 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 243, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 236, in main
    trainer.train(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 201, in train
    trainer.train()
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2593, in _inner_training_loop
    _grad_norm = self.accelerator.clip_grad_norm_(
  File "/home/rliubk/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 2609, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/rliubk/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 2548, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 342, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 264, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
  0%|          | 0/2750 [00:00<?, ?it/s]
Running: python src/trainer.py --model gpt2-medium --epochs 1 --output_dir models

==================================================
  Training the model
==================================================
Running: python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 79, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 59, in main
    run_command(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 19, in run_command
    result = subprocess.run(command, shell=True, check=True)
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models' returned non-zero exit status 1.
