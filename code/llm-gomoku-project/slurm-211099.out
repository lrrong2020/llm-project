The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loaded dataset with 87999 training examples
Map:   0%|          | 0/87999 [00:00<?, ? examples/s]Map:   1%|          | 1000/87999 [00:00<00:26, 3228.38 examples/s]Map:   2%|▏         | 2000/87999 [00:00<00:25, 3351.64 examples/s]Map:   3%|▎         | 3000/87999 [00:00<00:24, 3401.04 examples/s]Map:   5%|▍         | 4000/87999 [00:01<00:24, 3433.65 examples/s]Map:   6%|▌         | 5000/87999 [00:01<00:24, 3424.64 examples/s]Map:   7%|▋         | 6000/87999 [00:01<00:23, 3466.66 examples/s]Map:   8%|▊         | 7000/87999 [00:02<00:23, 3481.97 examples/s]Map:   9%|▉         | 8000/87999 [00:02<00:23, 3477.46 examples/s]Map:  10%|█         | 9000/87999 [00:02<00:22, 3497.20 examples/s]Map:  11%|█▏        | 10000/87999 [00:02<00:22, 3461.27 examples/s]Map:  13%|█▎        | 11000/87999 [00:03<00:22, 3461.86 examples/s]Map:  14%|█▎        | 12000/87999 [00:03<00:22, 3431.12 examples/s]Map:  15%|█▍        | 13000/87999 [00:03<00:21, 3455.96 examples/s]Map:  16%|█▌        | 14000/87999 [00:04<00:21, 3466.63 examples/s]Map:  17%|█▋        | 15000/87999 [00:04<00:23, 3102.23 examples/s]Map:  18%|█▊        | 16000/87999 [00:04<00:22, 3219.71 examples/s]Map:  19%|█▉        | 17000/87999 [00:05<00:21, 3298.51 examples/s]Map:  20%|██        | 18000/87999 [00:05<00:20, 3350.12 examples/s]Map:  22%|██▏       | 19000/87999 [00:05<00:20, 3401.41 examples/s]Map:  23%|██▎       | 20000/87999 [00:05<00:19, 3426.10 examples/s]Map:  24%|██▍       | 21000/87999 [00:06<00:19, 3447.80 examples/s]Map:  25%|██▌       | 22000/87999 [00:06<00:19, 3461.42 examples/s]Map:  26%|██▌       | 23000/87999 [00:06<00:18, 3471.60 examples/s]Map:  27%|██▋       | 24000/87999 [00:07<00:18, 3488.58 examples/s]Map:  28%|██▊       | 25000/87999 [00:07<00:18, 3484.12 examples/s]Map:  30%|██▉       | 26000/87999 [00:07<00:18, 3351.77 examples/s]Map:  31%|███       | 27000/87999 [00:07<00:17, 3392.54 examples/s]Map:  32%|███▏      | 28000/87999 [00:08<00:17, 3428.05 examples/s]Map:  33%|███▎      | 29000/87999 [00:08<00:17, 3438.20 examples/s]Map:  34%|███▍      | 30000/87999 [00:08<00:16, 3450.21 examples/s]Map:  35%|███▌      | 31000/87999 [00:09<00:16, 3467.07 examples/s]Map:  36%|███▋      | 32000/87999 [00:09<00:16, 3468.16 examples/s]Map:  38%|███▊      | 33000/87999 [00:09<00:15, 3478.44 examples/s]Map:  39%|███▊      | 34000/87999 [00:09<00:15, 3484.62 examples/s]Map:  40%|███▉      | 35000/87999 [00:10<00:15, 3471.13 examples/s]Map:  41%|████      | 36000/87999 [00:10<00:14, 3489.90 examples/s]Map:  42%|████▏     | 37000/87999 [00:10<00:14, 3489.61 examples/s]Map:  43%|████▎     | 38000/87999 [00:11<00:14, 3489.90 examples/s]Map:  44%|████▍     | 39000/87999 [00:11<00:13, 3501.06 examples/s]Map:  45%|████▌     | 40000/87999 [00:11<00:13, 3503.63 examples/s]Map:  47%|████▋     | 41000/87999 [00:11<00:13, 3494.61 examples/s]Map:  48%|████▊     | 42000/87999 [00:12<00:13, 3482.75 examples/s]Map:  49%|████▉     | 43000/87999 [00:12<00:12, 3488.35 examples/s]Map:  50%|█████     | 44000/87999 [00:12<00:12, 3481.69 examples/s]Map:  51%|█████     | 45000/87999 [00:13<00:12, 3479.16 examples/s]Map:  52%|█████▏    | 46000/87999 [00:13<00:12, 3499.61 examples/s]Map:  53%|█████▎    | 47000/87999 [00:13<00:11, 3485.21 examples/s]Map:  55%|█████▍    | 48000/87999 [00:13<00:11, 3475.25 examples/s]Map:  56%|█████▌    | 49000/87999 [00:14<00:12, 3098.69 examples/s]Map:  57%|█████▋    | 50000/87999 [00:14<00:11, 3197.14 examples/s]Map:  58%|█████▊    | 51000/87999 [00:14<00:11, 3287.22 examples/s]Map:  59%|█████▉    | 52000/87999 [00:15<00:10, 3337.84 examples/s]Map:  60%|██████    | 53000/87999 [00:15<00:10, 3383.39 examples/s]Map:  61%|██████▏   | 54000/87999 [00:15<00:09, 3428.97 examples/s]Map:  63%|██████▎   | 55000/87999 [00:16<00:09, 3442.12 examples/s]Map:  64%|██████▎   | 56000/87999 [00:16<00:09, 3457.03 examples/s]Map:  65%|██████▍   | 57000/87999 [00:16<00:08, 3470.12 examples/s]Map:  66%|██████▌   | 58000/87999 [00:16<00:08, 3480.52 examples/s]Map:  67%|██████▋   | 59000/87999 [00:17<00:08, 3458.80 examples/s]Map:  68%|██████▊   | 60000/87999 [00:17<00:08, 3479.20 examples/s]Map:  69%|██████▉   | 61000/87999 [00:17<00:07, 3486.87 examples/s]Map:  70%|███████   | 62000/87999 [00:18<00:07, 3484.39 examples/s]Map:  72%|███████▏  | 63000/87999 [00:18<00:07, 3483.21 examples/s]Map:  73%|███████▎  | 64000/87999 [00:18<00:06, 3486.80 examples/s]Map:  74%|███████▍  | 65000/87999 [00:18<00:06, 3486.57 examples/s]Map:  75%|███████▌  | 66000/87999 [00:19<00:06, 3491.43 examples/s]Map:  76%|███████▌  | 67000/87999 [00:19<00:06, 3456.32 examples/s]Map:  77%|███████▋  | 68000/87999 [00:19<00:05, 3464.15 examples/s]Map:  78%|███████▊  | 69000/87999 [00:20<00:05, 3457.77 examples/s]Map:  80%|███████▉  | 70000/87999 [00:20<00:05, 3427.83 examples/s]Map:  81%|████████  | 71000/87999 [00:20<00:04, 3428.88 examples/s]Map:  82%|████████▏ | 72000/87999 [00:20<00:04, 3460.15 examples/s]Map:  83%|████████▎ | 73000/87999 [00:21<00:04, 3460.11 examples/s]Map:  84%|████████▍ | 74000/87999 [00:21<00:04, 3454.26 examples/s]Map:  85%|████████▌ | 75000/87999 [00:21<00:03, 3440.42 examples/s]Map:  86%|████████▋ | 76000/87999 [00:22<00:03, 3435.09 examples/s]Map:  88%|████████▊ | 77000/87999 [00:22<00:03, 3435.53 examples/s]Map:  89%|████████▊ | 78000/87999 [00:22<00:02, 3462.95 examples/s]Map:  90%|████████▉ | 79000/87999 [00:22<00:02, 3469.65 examples/s]Map:  91%|█████████ | 80000/87999 [00:23<00:02, 3475.23 examples/s]Map:  92%|█████████▏| 81000/87999 [00:23<00:02, 3487.00 examples/s]Map:  93%|█████████▎| 82000/87999 [00:23<00:01, 3484.69 examples/s]Map:  94%|█████████▍| 83000/87999 [00:24<00:01, 3473.98 examples/s]Map:  95%|█████████▌| 84000/87999 [00:24<00:01, 3116.45 examples/s]Map:  97%|█████████▋| 85000/87999 [00:24<00:00, 3213.32 examples/s]Map:  98%|█████████▊| 86000/87999 [00:25<00:00, 3283.85 examples/s]Map:  99%|█████████▉| 87000/87999 [00:25<00:00, 3356.08 examples/s]Map: 100%|██████████| 87999/87999 [00:25<00:00, 3392.69 examples/s]Map: 100%|██████████| 87999/87999 [00:25<00:00, 3387.87 examples/s]
Map:   0%|          | 0/10749 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10749 [00:00<00:02, 3333.54 examples/s]Map:  19%|█▊        | 2000/10749 [00:00<00:02, 3425.68 examples/s]Map:  28%|██▊       | 3000/10749 [00:00<00:02, 3451.10 examples/s]Map:  37%|███▋      | 4000/10749 [00:01<00:01, 3483.14 examples/s]Map:  47%|████▋     | 5000/10749 [00:01<00:01, 3479.37 examples/s]Map:  56%|█████▌    | 6000/10749 [00:01<00:01, 3486.18 examples/s]Map:  65%|██████▌   | 7000/10749 [00:02<00:01, 3491.46 examples/s]Map:  74%|███████▍  | 8000/10749 [00:02<00:00, 3503.17 examples/s]Map:  84%|████████▎ | 9000/10749 [00:02<00:00, 3359.97 examples/s]Map:  93%|█████████▎| 10000/10749 [00:02<00:00, 3395.73 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3415.29 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3332.74 examples/s]
Map:   0%|          | 0/10657 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10657 [00:00<00:02, 3367.03 examples/s]Map:  19%|█▉        | 2000/10657 [00:00<00:02, 3445.01 examples/s]Map:  28%|██▊       | 3000/10657 [00:00<00:02, 3491.98 examples/s]Map:  38%|███▊      | 4000/10657 [00:01<00:01, 3495.14 examples/s]Map:  47%|████▋     | 5000/10657 [00:01<00:01, 3478.82 examples/s]Map:  56%|█████▋    | 6000/10657 [00:01<00:01, 3477.03 examples/s]Map:  66%|██████▌   | 7000/10657 [00:02<00:01, 3490.48 examples/s]Map:  75%|███████▌  | 8000/10657 [00:02<00:00, 3482.03 examples/s]Map:  84%|████████▍ | 9000/10657 [00:02<00:00, 3386.12 examples/s]Map:  94%|█████████▍| 10000/10657 [00:02<00:00, 3368.67 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3393.21 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3331.35 examples/s]
/home/rliubk/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py:188: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Preprocessed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 87999
    })
    validation: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10749
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10657
    })
})
trainable params: 393,216 || all params: 355,613,696 || trainable%: 0.1106
Starting training...
  0%|          | 0/2750 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/rliubk/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 247, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 240, in main
    trainer.train(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 199, in train
    trainer.train()
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/rliubk/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 2450, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  0%|          | 0/2750 [00:00<?, ?it/s]
Running: python src/trainer.py --model gpt2-medium --epochs 1 --output_dir models

==================================================
  Training the model
==================================================
Running: python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 79, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 59, in main
    run_command(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 19, in run_command
    result = subprocess.run(command, shell=True, check=True)
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models' returned non-zero exit status 1.
