The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loaded dataset with 87999 training examples
Map:   0%|          | 0/87999 [00:00<?, ? examples/s]Map:   1%|          | 1000/87999 [00:00<00:26, 3223.56 examples/s]Map:   2%|▏         | 2000/87999 [00:00<00:25, 3364.42 examples/s]Map:   3%|▎         | 3000/87999 [00:00<00:24, 3419.12 examples/s]Map:   5%|▍         | 4000/87999 [00:01<00:24, 3442.34 examples/s]Map:   6%|▌         | 5000/87999 [00:01<00:24, 3436.84 examples/s]Map:   7%|▋         | 6000/87999 [00:01<00:23, 3471.46 examples/s]Map:   8%|▊         | 7000/87999 [00:02<00:23, 3474.68 examples/s]Map:   9%|▉         | 8000/87999 [00:02<00:23, 3473.34 examples/s]Map:  10%|█         | 9000/87999 [00:02<00:22, 3489.58 examples/s]Map:  11%|█▏        | 10000/87999 [00:02<00:22, 3496.95 examples/s]Map:  13%|█▎        | 11000/87999 [00:03<00:22, 3356.85 examples/s]Map:  14%|█▎        | 12000/87999 [00:03<00:22, 3401.57 examples/s]Map:  15%|█▍        | 13000/87999 [00:03<00:21, 3426.00 examples/s]Map:  16%|█▌        | 14000/87999 [00:04<00:21, 3430.76 examples/s]Map:  17%|█▋        | 15000/87999 [00:04<00:23, 3101.13 examples/s]Map:  18%|█▊        | 16000/87999 [00:04<00:22, 3214.92 examples/s]Map:  19%|█▉        | 17000/87999 [00:05<00:21, 3289.77 examples/s]Map:  20%|██        | 18000/87999 [00:05<00:21, 3330.75 examples/s]Map:  22%|██▏       | 19000/87999 [00:05<00:20, 3385.12 examples/s]Map:  23%|██▎       | 20000/87999 [00:05<00:19, 3411.09 examples/s]Map:  24%|██▍       | 21000/87999 [00:06<00:19, 3445.77 examples/s]Map:  25%|██▌       | 22000/87999 [00:06<00:19, 3452.43 examples/s]Map:  26%|██▌       | 23000/87999 [00:06<00:18, 3463.20 examples/s]Map:  27%|██▋       | 24000/87999 [00:07<00:18, 3488.35 examples/s]Map:  28%|██▊       | 25000/87999 [00:07<00:18, 3490.96 examples/s]Map:  30%|██▉       | 26000/87999 [00:07<00:17, 3482.02 examples/s]Map:  31%|███       | 27000/87999 [00:07<00:17, 3492.80 examples/s]Map:  32%|███▏      | 28000/87999 [00:08<00:17, 3501.24 examples/s]Map:  33%|███▎      | 29000/87999 [00:08<00:16, 3490.89 examples/s]Map:  34%|███▍      | 30000/87999 [00:08<00:16, 3485.59 examples/s]Map:  35%|███▌      | 31000/87999 [00:09<00:16, 3491.92 examples/s]Map:  36%|███▋      | 32000/87999 [00:09<00:16, 3494.89 examples/s]Map:  38%|███▊      | 33000/87999 [00:09<00:15, 3498.41 examples/s]Map:  39%|███▊      | 34000/87999 [00:09<00:15, 3504.31 examples/s]Map:  40%|███▉      | 35000/87999 [00:10<00:15, 3501.57 examples/s]Map:  41%|████      | 36000/87999 [00:10<00:14, 3506.98 examples/s]Map:  42%|████▏     | 37000/87999 [00:10<00:14, 3499.86 examples/s]Map:  43%|████▎     | 38000/87999 [00:11<00:14, 3495.99 examples/s]Map:  44%|████▍     | 39000/87999 [00:11<00:13, 3500.91 examples/s]Map:  45%|████▌     | 40000/87999 [00:11<00:13, 3500.52 examples/s]Map:  47%|████▋     | 41000/87999 [00:11<00:13, 3488.91 examples/s]Map:  48%|████▊     | 42000/87999 [00:12<00:13, 3501.66 examples/s]Map:  49%|████▉     | 43000/87999 [00:12<00:12, 3492.92 examples/s]Map:  50%|█████     | 44000/87999 [00:12<00:12, 3487.38 examples/s]Map:  51%|█████     | 45000/87999 [00:13<00:12, 3476.22 examples/s]Map:  52%|█████▏    | 46000/87999 [00:13<00:12, 3479.46 examples/s]Map:  53%|█████▎    | 47000/87999 [00:13<00:12, 3325.31 examples/s]Map:  55%|█████▍    | 48000/87999 [00:13<00:11, 3374.39 examples/s]Map:  56%|█████▌    | 49000/87999 [00:14<00:12, 3039.93 examples/s]Map:  57%|█████▋    | 50000/87999 [00:14<00:12, 3161.20 examples/s]Map:  58%|█████▊    | 51000/87999 [00:14<00:11, 3264.24 examples/s]Map:  59%|█████▉    | 52000/87999 [00:15<00:10, 3328.69 examples/s]Map:  60%|██████    | 53000/87999 [00:15<00:10, 3370.59 examples/s]Map:  61%|██████▏   | 54000/87999 [00:15<00:10, 3399.06 examples/s]Map:  63%|██████▎   | 55000/87999 [00:16<00:09, 3434.13 examples/s]Map:  64%|██████▎   | 56000/87999 [00:16<00:09, 3428.06 examples/s]Map:  65%|██████▍   | 57000/87999 [00:16<00:08, 3446.42 examples/s]Map:  66%|██████▌   | 58000/87999 [00:16<00:08, 3459.05 examples/s]Map:  67%|██████▋   | 59000/87999 [00:17<00:08, 3451.29 examples/s]Map:  68%|██████▊   | 60000/87999 [00:17<00:08, 3473.22 examples/s]Map:  69%|██████▉   | 61000/87999 [00:17<00:08, 3371.53 examples/s]Map:  70%|███████   | 62000/87999 [00:18<00:07, 3395.62 examples/s]Map:  72%|███████▏  | 63000/87999 [00:18<00:07, 3419.51 examples/s]Map:  73%|███████▎  | 64000/87999 [00:18<00:06, 3436.79 examples/s]Map:  74%|███████▍  | 65000/87999 [00:19<00:06, 3424.24 examples/s]Map:  75%|███████▌  | 66000/87999 [00:19<00:06, 3445.44 examples/s]Map:  76%|███████▌  | 67000/87999 [00:19<00:06, 3452.21 examples/s]Map:  77%|███████▋  | 68000/87999 [00:19<00:06, 3297.42 examples/s]Map:  78%|███████▊  | 69000/87999 [00:20<00:05, 3332.82 examples/s]Map:  80%|███████▉  | 70000/87999 [00:20<00:05, 3315.59 examples/s]Map:  81%|████████  | 71000/87999 [00:20<00:05, 3305.14 examples/s]Map:  82%|████████▏ | 72000/87999 [00:21<00:04, 3350.68 examples/s]Map:  83%|████████▎ | 73000/87999 [00:21<00:04, 3394.44 examples/s]Map:  84%|████████▍ | 74000/87999 [00:21<00:04, 3413.74 examples/s]Map:  85%|████████▌ | 75000/87999 [00:21<00:03, 3434.25 examples/s]Map:  86%|████████▋ | 76000/87999 [00:22<00:03, 3434.29 examples/s]Map:  88%|████████▊ | 77000/87999 [00:22<00:03, 3429.62 examples/s]Map:  89%|████████▊ | 78000/87999 [00:22<00:02, 3393.19 examples/s]Map:  90%|████████▉ | 79000/87999 [00:23<00:02, 3422.57 examples/s]Map:  91%|█████████ | 80000/87999 [00:23<00:02, 3431.40 examples/s]Map:  92%|█████████▏| 81000/87999 [00:23<00:02, 3457.32 examples/s]Map:  93%|█████████▎| 82000/87999 [00:23<00:01, 3465.25 examples/s]Map:  94%|█████████▍| 83000/87999 [00:24<00:01, 3464.32 examples/s]Map:  95%|█████████▌| 84000/87999 [00:24<00:01, 3073.39 examples/s]Map:  97%|█████████▋| 85000/87999 [00:24<00:00, 3185.41 examples/s]Map:  98%|█████████▊| 86000/87999 [00:25<00:00, 3261.68 examples/s]Map:  99%|█████████▉| 87000/87999 [00:25<00:00, 3328.55 examples/s]Map: 100%|██████████| 87999/87999 [00:25<00:00, 3383.18 examples/s]Map: 100%|██████████| 87999/87999 [00:26<00:00, 3375.96 examples/s]
Map:   0%|          | 0/10749 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10749 [00:00<00:02, 3361.21 examples/s]Map:  19%|█▊        | 2000/10749 [00:00<00:02, 3437.41 examples/s]Map:  28%|██▊       | 3000/10749 [00:00<00:02, 3466.62 examples/s]Map:  37%|███▋      | 4000/10749 [00:01<00:01, 3497.33 examples/s]Map:  47%|████▋     | 5000/10749 [00:01<00:01, 3504.98 examples/s]Map:  56%|█████▌    | 6000/10749 [00:01<00:01, 3489.45 examples/s]Map:  65%|██████▌   | 7000/10749 [00:02<00:01, 3494.49 examples/s]Map:  74%|███████▍  | 8000/10749 [00:02<00:00, 3480.46 examples/s]Map:  84%|████████▎ | 9000/10749 [00:02<00:00, 3439.17 examples/s]Map:  93%|█████████▎| 10000/10749 [00:02<00:00, 3460.75 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3457.62 examples/s]Map: 100%|██████████| 10749/10749 [00:03<00:00, 3368.65 examples/s]
Map:   0%|          | 0/10657 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10657 [00:00<00:02, 3420.09 examples/s]Map:  19%|█▉        | 2000/10657 [00:00<00:02, 3483.25 examples/s]Map:  28%|██▊       | 3000/10657 [00:00<00:02, 3457.22 examples/s]Map:  38%|███▊      | 4000/10657 [00:01<00:01, 3473.47 examples/s]Map:  47%|████▋     | 5000/10657 [00:01<00:01, 3479.95 examples/s]Map:  56%|█████▋    | 6000/10657 [00:01<00:01, 3490.01 examples/s]Map:  66%|██████▌   | 7000/10657 [00:02<00:01, 3499.84 examples/s]Map:  75%|███████▌  | 8000/10657 [00:02<00:00, 3509.17 examples/s]Map:  84%|████████▍ | 9000/10657 [00:02<00:00, 3490.86 examples/s]Map:  94%|█████████▍| 10000/10657 [00:02<00:00, 3497.89 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3488.86 examples/s]Map: 100%|██████████| 10657/10657 [00:03<00:00, 3414.24 examples/s]
/home/rliubk/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py:225: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Preprocessed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 87999
    })
    validation: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10749
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask', 'labels'],
        num_rows: 10657
    })
})
LoRA adaptation applied successfully
Froze the bottom 12 transformer blocks
Gradient checkpointing enabled
trainable params: 1,081,344 || all params: 356,989,952 || trainable%: 0.3029
Starting training...
  0%|          | 0/2750 [00:00<?, ?it/s]/home/rliubk/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 284, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 277, in main
    trainer.train(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/src/trainer.py", line 236, in train
    trainer.train()
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/rliubk/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/rliubk/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 2450, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/rliubk/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  0%|          | 0/2750 [00:00<?, ?it/s]
Running: python src/trainer.py --model gpt2-medium --epochs 1 --output_dir models

==================================================
  Training the model
==================================================
Running: python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models
Traceback (most recent call last):
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 79, in <module>
    main() 
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 59, in main
    run_command(
  File "/home/rliubk/llm/llm-project/code/llm-gomoku-project/run_pipeline.py", line 19, in run_command
    result = subprocess.run(command, shell=True, check=True)
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'python run_no_ds.py --model gpt2-medium --epochs 1 --output_dir models' returned non-zero exit status 1.
